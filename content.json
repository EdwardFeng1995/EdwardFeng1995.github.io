{"pages":[{"title":"xinde","text":"","link":"/xinde/index.html"}],"posts":[{"title":"I/O之同步、异步、阻塞、非阻塞","text":"今天在看到 Libevent 的核心是事件驱动、同步非阻塞 这句话时，突然产生了疑惑，libevent是同步的吗？，一直以为是异步的，然后开始搜索资料，发现众说纷纭，有人说是异步，有人说是同步。答案是同步。。 之所以产生疑惑，根本原因是自己对同步、异步以及阻塞、非阻塞理解的不够透彻。 通过查找网上的资料发现，关于同步/异步，阻塞/非阻塞的解释有各种各样的版本，有各种各样的理解，搞得自己也有点懵，所以还是以比较权威的 W.Richard Stevens的UNP 6.2 I/O模型 来理解。 书中介绍了在Unix下可用的5中I/O模型，分别是： 阻塞式I/O 非阻塞式I/O I/O复用（select和poll） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 一个输入操作通常包括两个不同的阶段： (1)等待数据准备好 (2)从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 阻塞式I/O模型默认情形下，所有套接字都是阻塞的。。以数据报套接字作为例子，如下图所示： 在上图中，进程调用recvfrom，其系统调用直到数据报到达且被复制到应用进程的缓冲区中或者发生错误才返回。最常见的错误是系统调用被信号中断。这里，进程在从调用recvfrom开始到它返回的整段时间内是被阻塞的。recvfrom成功返回后，应用进程开始处理数据报。 非阻塞I/O模型套接字的默认状态是阻塞的。这就意味着当发出一个不能立即完成的套接字调用时，其进程将被投入睡眠，等待相应操作完成。进程把一个套接字设置称非阻塞是在通知内核：当所请求的I/O操作非得把本进程投入睡眠才能完成时，而是返回一个错误。 看下图的例子： 前三次调用recvfrom时没有数据可返回，因此内核转而立即返回一个EWOULDBLOCK错误。第四次调用recvfrom时已有一个数据报准备好，它被复制到应用进程缓冲区，于是recvfrom成功返回。接着处理数据。 当一个应用进程像这样一个非阻塞描述符循环调用recvfrom时，我们称之为轮询(polling)。应用进程持续轮询内核，以查看某个操作是否就绪。这么做往往消耗大量CPU时间，不过这种模型偶尔也会遇到，通常是在专门提供某一种功能的系统中才有。 I/O复用模型有了I/O复用模型，我们就可以调用select或poll，阻塞在这两个系统调用中的某一个之上，而不是阻塞在真正的I/O系统调用上。下图概括展示了I/O复用模型。 阻塞于select调用，等待数据报套接字变为可读。当select返回套接字可读这一条件时，我们调用recvfrom把所读数据报复制到进程缓冲区。 比较图6-3和6-1，I/O复用并不显得有说明优势，事实上由于使用select需要两个而不是单个系统调用，I/O复用还稍显劣势。但是，使用select的优势在于可以等待多个描述符就绪。 信号驱动式I/O模型此外，还可以用信号，让内核在描述符就绪时发送SIGIO信号通知我们。我们称这种模型为信号驱动式I/O模型，下图是它的概要展示。 我们首先开启套接字的信号驱动式I/O工呢，并通过sigaction系统调用安装一个信号处理函数。该系统调用立即返回，我们进程继续工作，也就是说它没有被阻塞。当数据报准备好读取时，内核九尾该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它读书数据报。 无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达期间进程不被阻塞。主循环可以继续执行，只要等待来自信号处理函数的通知：既可以是数据已准备好被处理，也可以是数据已准备好被读取。 异步I/O模型工作机制是：告知内核启动某个操作，并让内核在整个操作（包括将数据从内核复制到我们自己的缓冲区）完成后通知我们。这种模型与信号驱动模型的主要区别在于：信号驱动式I/O是由内核通知我们何时可以启动一个I/O操作，而异步I/O模型是由内核通知我们I/O操作何时完成 。 看下图例子。 我们调用aio_read函数，给内核传递描述符、缓冲区指针、缓冲区大小和文件偏移，并告诉内核当整个操作完成时何时通知我们。系统调用立即返回，而且在等待I/O完成期间，我们进程不被阻塞。本例中我们假设要求内核在操作完成时产生某个信号。该信号直到数据已复制到应用进程缓冲区才产生，这一点不同于信号驱动式I/O模型。 各种I/O模型的比较下图对比了上述五种不同的I/O模型。 可以看出，前4种模型的主要区别在于第一阶段，因为它们的第二阶段是一样的：在数据从内核复制到调用者的缓冲区期间，进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理，进程不需要阻塞，从而不同于其他4种模型。 同步I/O和异步I/O对比POSIX 把这两个术语定义如下： 同步I/O操作：导致请求进程阻塞，直到I/O操作完成 异步I/O操作：不导致请求进程阻塞 所以，根据上述定义，前4种模型：阻塞式I/O模型、非阻塞I/O模型、I/O复用模型、信号驱动式I/O模型都是同步I/O模型，因为其中真正的I/O操作（recvfrom）将进程阻塞。只有异步I/O模型与POSIX定义的异步I/O相匹配。 此外，在知乎上，陈硕大佬给出了这样的解答 所以，最后我理解的是 同步/异步的区别主要在于第二阶段，即将数据从内核复制到用户空间这一阶段。同步需要主动去读写数据，在读写的系统调用过程中，是阻塞的。而异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写，在这个过程中进程不阻塞。 阻塞/非阻塞，如果是阻塞，进程在发起系统调用后，在调用返回之前，进程会被挂起，等待返回结果；如果是非阻塞，调用会立即返回一个状态值，进程不需要挂起，不需要等待，这就是非阻塞，如果进程时不时去查看是否有结果，就是轮询。 在清楚了同步、异步、阻塞、非阻塞的概念和区别后，再来看Reactor模型，其读操作的具体步骤如下： 应用程序注册读就绪事件和相关联的事件处理器 事件分离器等待事件的发生 当发生读就绪事件的时候，事件分离器调用第一步注册的事件处理器 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理 当读就绪事件发生时，事件处理器会主动执行读取操作，所以Reactor是同步的。 此外还有一种异步I/O模型，就是Proactor模型，其读取操作步骤如下： 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注读取就绪事件，而是关注读取完成事件，这是区别于Reactor的关键。 事件分离器等待读取操作完成事件 在事件分离器等待读取操作完成的时候，操作系统调用内核线程完成读取操作（异步IO都是操作系统负责将数据读写到应用传递进来的缓冲区供应用程序操作，操作系统扮演了重要角色），并将读取的内容放入用户传递过来的缓存区中。这也是区别于Reactor的一点，Proactor中，应用程序需要传递缓存区。 事件分离器捕获到读取完成事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。 可以清楚地看到两者的区别，Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的，Reactor中需要应用程序自己读取或者写入数据，而Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，操作系统会读取缓存区或者写入缓存区到真正的IO设备。 最后Libevent的核心就是Reactor模型，再看libevent的底层代码，实际仍然是一个while循环，调用epoll或者select去监听event是否准备就绪，事件准备就绪后，事件处理器还是会主动去执行需要的操作，例如读事件，是要主动去读取数据，所以，这样看libevent是同步的。","link":"/2019/05/10/I-O之同步、异步、阻塞、非阻塞/"},{"title":"epoll反应堆模型","text":"epoll反应堆模型epoll是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率，因为它会复用文件描述符集合来传递结果而不用迫使开发者每次等待事件之前都必须重新准备要被侦听的文件描述符集合，另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。epoll除了提供select/poll那种IO事件的电平触发（Level Triggered）外，还提供了边沿触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。 1、epoll函数原型（1）创建一个epoll句柄，参数size用来告诉内核监听的文件描述符的个数，跟内存大小有关。返回代表新创建的epoll实例的文件描述符，其实创建的为一颗红黑树，返回根节点。12#include &lt;sys/epoll.h&gt;int epoll_create(int size) size：监听数目 （2）控制某个epoll监控的文件描述符上的事件：注册、修改、删除。 123456789101112131415161718192021222324252627#include &lt;sys/epoll.h&gt; int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) epfd： 为epoll_creat的句柄 op： 表示动作，用3个宏来表示： EPOLL_CTL_ADD (注册新的fd到epfd，给红黑树加入新节点)， EPOLL_CTL_MOD (修改已经注册的fd的监听事件)， EPOLL_CTL_DEL (从epfd删除一个fd，删除红黑树节点)； event： 告诉内核需要监听的事件 struct epoll_event { __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; EPOLLIN ： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭） EPOLLOUT： 表示对应的文件描述符可以写 EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来） EPOLLERR： 表示对应的文件描述符发生错误 EPOLLHUP： 表示对应的文件描述符被挂断； EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)而言的 EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 （3）等待所监控文件描述符上有事件的产生，类似于select()调用。123456789#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout) events： 用来存内核得到事件的集合， maxevents： 告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size， timeout： 是超时时间 -1： 阻塞 0： 立即返回，非阻塞 &gt;0： 指定毫秒 返回值： 成功返回有多少文件描述符就绪，时间到时返回0，出错返回-1 epoll的设计： （1）epoll在Linux内核中构建了一个文件系统，该文件系统采用红黑树来构建，红黑树在增加和删除上面的效率极高，因此是epoll高效的原因之一。有兴趣可以百度红黑树了解，但在这里你只需知道其算法效率超高即可。 （2）epoll红黑树上采用事件异步唤醒，内核监听I/O，事件发生后内核搜索红黑树并将对应节点数据放入异步唤醒的事件队列中。 （3）epoll的数据从用户空间到内核空间采用mmap存储I/O映射来加速。该方法是目前Linux进程间通信中传递最快,消耗最小,传递数据过程不涉及系统调用的方法。 epoll接口相对于传统的select/poll而言，有以下优点： （1）支持单个进程打开大数量的文件描述符。受进程最大打开的文件描述符数量限制，而不是受自身实现限制。而select单个进程能够打开的文件描述符的数量存在最大限制，这个限制是select自身实现的限制。通常是1024。poll采用链表，也是远超select的。 （2）Linux的I/O效率不会随着文件描述符数量的增加而线性下降。较之于select/poll，当处于一个高并发时(例如10万，100万)。在如此庞大的socket集合中，任一时间里其实只有部分的socket是“活跃”的。select/poll的处理方式是，对用如此庞大的集合进行线性扫描并对有事件发生的socket进行处理，这将极大的浪费CPU资源。因此epoll的改进是，由于I/O事件发生，内核将活跃的socket放入队列并交给mmap加速到用户空间，程序拿到的集合是处于活跃的socket集合，而不是所有socket集合。 （3）使用mmap加速内核与用户空间的消息传递。select/poll采用的方式是，将所有要监听的文件描述符集合拷贝到内核空间（用户态到内核态切换）。接着内核对集合进行轮询检测，当有事件发生时，内核从中集合并将集合复制到用户空间。 再看看epoll怎么做的，内核与程序共用一块内存，用户与mmap加速区进行数据交互不涉及权限的切换(用户态到内核态，内核态到用户态)。内核对于处于非内核空间的内存有权限进行读取。 作者：青城山小和尚 来源：CSDN 原文：https://blog.csdn.net/qq_36359022/article/details/81355897 2、边沿触发（ET）和水平触发（LT） （1）水平触发, 此方式为默认情况。当设置了水平触发以后，以可读事件为例，当有数据到来并且数据在缓冲区待读。即使我这一次没有读取完数据，只要缓冲区里还有数据就会触发第二次，直到缓冲区里没数据。 （2）epoll边沿触发，当设置了边沿触发以后，以可读事件为例，对“有数据到来”这件事为触发。 为什么说边沿触发(ET) 的效率更高呢？ (1) 边沿触发只在数据到来的一刻才触发，很多时候服务器在接受大量数据时会先接受数据头部(水平触发在此触发第一次，边沿触发第一次)。 (2) 接着服务器通过解析头部决定要不要接这个数据。此时，如果不接受数据，水平触发需要手动清除，而边沿触发可以将清除工作交给一个定时的清除程序去做，自己立刻返回。 (3) 如果接受，两种方式都可以用while接收完整数据。 3、epoll非阻塞I/O使用的读取数据函数为read函数，在默认情况下此类函数是阻塞式的，在没有数据时会一直阻塞等待数据到来。 （1）数据到来100B，在epoll模式下调用read时，即使read()是阻塞式的也不会在这里等待，因为既然运行到read()，说明数据缓冲区已经有数据，因此这处无影响。 （2）在服务器开发中，一般不会直接用采用类似read()函数这一类系统调用(只有内核缓冲区)，会使用封装好的一些库函数(有内核缓冲区+用户缓冲区)或者自己封装的函数。 例如：使用readn()函数，设置读取200B返回，假设数据到来100B,可读事件触发，而程序要使用readn()读200B，那么此时如果是阻塞式的，将在此处形成死锁 流程是：100B ⇒ 触发可读事件 ⇒ readn()调用 ⇒ readn()都不够200B,阻塞 ⇒ cfd又到来200B ⇒ 此时程序在readn()处暂停，没有机会调用epoll_wait() ⇒ 完成死锁 解决：将该cfd在上树前设置为非阻塞式 4、epoll反应堆模型（Libevent核心思想）1) epoll — 服务器 — 监听 — cfd —- 可读 —- epoll返回 —- read – cfd从树上摘下 — 设置监听cfd写事件， 操作 — 小写转大写 – 等待epoll_wait 返回 — 回写客户端 – cfd从树上摘下 —– 设置监听cfd读事件， 操作 – epoll继续监听。关键为回调函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280/* *epoll基于非阻塞I/O事件驱动 */#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#define MAX_EVENTS 1024 //监听上限数#define BUFLEN 4096#define SERV_PORT 8080void recvdata(int fd, int events, void *arg);void senddata(int fd, int events, void *arg);/* 描述就绪文件描述符相关信息 */struct myevent_s { int fd; //要监听的文件描述符 int events; //对应的监听事件 void *arg; //泛型参数 void (*call_back)(int fd, int events, void *arg); //回调函数 int status; //是否在监听:1-&gt;在红黑树上(监听), 0-&gt;不在(不监听) char buf[BUFLEN]; int len; long last_active; //记录每次加入红黑树 g_efd 的时间值};int g_efd; //全局变量, 保存epoll_create返回的文件描述符struct myevent_s g_events[MAX_EVENTS+1]; //自定义结构体类型数组. +1--&gt;listen fd/*将结构体 myevent_s 成员变量 初始化*/void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg){ ev-&gt;fd = fd; ev-&gt;call_back = call_back; ev-&gt;events = 0; ev-&gt;arg = arg; ev-&gt;status = 0; //memset(ev-&gt;buf, 0, sizeof(ev-&gt;buf)); //ev-&gt;len = 0; ev-&gt;last_active = time(NULL); //调用eventset函数的时间 return;}/* 向 epoll监听的红黑树 添加一个 文件描述符 */void eventadd(int efd, int events, struct myevent_s *ev){ struct epoll_event epv = {0, {0}}; int op; epv.data.ptr = ev; epv.events = ev-&gt;events = events; //EPOLLIN 或 EPOLLOUT if (ev-&gt;status == 1) { //已经在红黑树 g_efd 里 op = EPOLL_CTL_MOD; //修改其属性 } else { //不在红黑树里 op = EPOLL_CTL_ADD; //将其加入红黑树 g_efd, 并将status置1 ev-&gt;status = 1; } if (epoll_ctl(efd, op, ev-&gt;fd, &amp;epv) &lt; 0) //实际添加/修改 printf(\"event add failed [fd=%d], events[%d]\\n\", ev-&gt;fd, events); else printf(\"event add OK [fd=%d], op=%d, events[%0X]\\n\", ev-&gt;fd, op, events); return ;}/* 从epoll 监听的 红黑树中删除一个 文件描述符*/void eventdel(int efd, struct myevent_s *ev){ struct epoll_event epv = {0, {0}}; if (ev-&gt;status != 1) //不在红黑树上 return ; epv.data.ptr = ev; ev-&gt;status = 0; //修改状态 epoll_ctl(efd, EPOLL_CTL_DEL, ev-&gt;fd, &amp;epv); //从红黑树 efd 上将 ev-&gt;fd 摘除 return ;}/* 当有文件描述符就绪, epoll返回, 调用该函数 与客户端建立链接 */void acceptconn(int lfd, int events, void *arg){ struct sockaddr_in cin; socklen_t len = sizeof(cin); int cfd, i; if ((cfd = accept(lfd, (struct sockaddr *)&amp;cin, &amp;len)) == -1) { if (errno != EAGAIN &amp;&amp; errno != EINTR) { /* 暂时不做出错处理 */ } printf(\"%s: accept, %s\\n\", __func__, strerror(errno)); return ; } do { for (i = 0; i &lt; MAX_EVENTS; i++) //从全局数组g_events中找一个空闲元素 if (g_events[i].status == 0) //类似于select中找值为-1的元素 break; //跳出 for if (i == MAX_EVENTS) { printf(\"%s: max connect limit[%d]\\n\", __func__, MAX_EVENTS); break; //跳出do while(0) 不执行后续代码 } int flag = 0; if ((flag = fcntl(cfd, F_SETFL, O_NONBLOCK)) &lt; 0) { //将cfd也设置为非阻塞 printf(\"%s: fcntl nonblocking failed, %s\\n\", __func__, strerror(errno)); break; } /* 给cfd设置一个 myevent_s 结构体, 回调函数 设置为 recvdata */ eventset(&amp;g_events[i], cfd, recvdata, &amp;g_events[i]); eventadd(g_efd, EPOLLIN, &amp;g_events[i]); //将cfd添加到红黑树g_efd中,监听读事件 } while(0); printf(\"new connect [%s:%d][time:%ld], pos[%d]\\n\", inet_ntoa(cin.sin_addr), ntohs(cin.sin_port), g_events[i].last_active, i); return ;}void recvdata(int fd, int events, void *arg){ struct myevent_s *ev = (struct myevent_s *)arg; int len; len = recv(fd, ev-&gt;buf, sizeof(ev-&gt;buf), 0); //读文件描述符, 数据存入myevent_s成员buf中 eventdel(g_efd, ev); //将该节点从红黑树上摘除 if (len &gt; 0) { ev-&gt;len = len; ev-&gt;buf[len] = '\\0'; //手动添加字符串结束标记 printf(\"C[%d]:%s\\n\", fd, ev-&gt;buf); eventset(ev, fd, senddata, ev); //设置该 fd 对应的回调函数为 senddata eventadd(g_efd, EPOLLOUT, ev); //将fd加入红黑树g_efd中,监听其写事件 } else if (len == 0) { close(ev-&gt;fd); /* ev-g_events 地址相减得到偏移元素位置 */ printf(\"[fd=%d] pos[%ld], closed\\n\", fd, ev-g_events); } else { close(ev-&gt;fd); printf(\"recv[fd=%d] error[%d]:%s\\n\", fd, errno, strerror(errno)); } return;}void senddata(int fd, int events, void *arg){ struct myevent_s *ev = (struct myevent_s *)arg; int len; len = send(fd, ev-&gt;buf, ev-&gt;len, 0); //直接将数据 回写给客户端。未作处理 /* printf(\"fd=%d\\tev-&gt;buf=%s\\ttev-&gt;len=%d\\n\", fd, ev-&gt;buf, ev-&gt;len); printf(\"send len = %d\\n\", len); */ if (len &gt; 0) { printf(\"send[fd=%d], [%d]%s\\n\", fd, len, ev-&gt;buf); eventdel(g_efd, ev); //从红黑树g_efd中移除 eventset(ev, fd, recvdata, ev); //将该fd的 回调函数改为 recvdata eventadd(g_efd, EPOLLIN, ev); //从新添加到红黑树上， 设为监听读事件 } else { close(ev-&gt;fd); //关闭链接 eventdel(g_efd, ev); //从红黑树g_efd中移除 printf(\"send[fd=%d] error %s\\n\", fd, strerror(errno)); } return ;}/*创建 socket, 初始化lfd */void initlistensocket(int efd, short port){ int lfd = socket(AF_INET, SOCK_STREAM, 0); fcntl(lfd, F_SETFL, O_NONBLOCK); //将socket设为非阻塞 /* void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg); */ eventset(&amp;g_events[MAX_EVENTS], lfd, acceptconn, &amp;g_events[MAX_EVENTS]); /* void eventadd(int efd, int events, struct myevent_s *ev) */ eventadd(efd, EPOLLIN, &amp;g_events[MAX_EVENTS]); struct sockaddr_in sin; memset(&amp;sin, 0, sizeof(sin)); //bzero(&amp;sin, sizeof(sin)) sin.sin_family = AF_INET; sin.sin_addr.s_addr = INADDR_ANY; sin.sin_port = htons(port); bind(lfd, (struct sockaddr *)&amp;sin, sizeof(sin)); listen(lfd, 20); return ;}int main(int argc, char *argv[]){ unsigned short port = SERV_PORT; if (argc == 2) port = atoi(argv[1]); //使用用户指定端口.如未指定,用默认端口 g_efd = epoll_create(MAX_EVENTS+1); //创建红黑树,返回给全局 g_efd if (g_efd &lt;= 0) printf(\"create efd in %s err %s\\n\", __func__, strerror(errno)); initlistensocket(g_efd, port); //初始化监听socket struct epoll_event events[MAX_EVENTS+1]; //保存已经满足就绪事件的文件描述符数组 printf(\"server running:port[%d]\\n\", port); int checkpos = 0, i; while (1) { /* 超时验证，每次测试100个链接，不测试listenfd 当客户端60秒内没有和服务器通信，则关闭此客户端链接 */ long now = time(NULL); //当前时间 for (i = 0; i &lt; 100; i++, checkpos++) { //一次循环检测100个。 使用checkpos控制检测对象 if (checkpos == MAX_EVENTS) checkpos = 0; if (g_events[checkpos].status != 1) //不在红黑树 g_efd 上 continue; long duration = now - g_events[checkpos].last_active; //客户端不活跃的世间 if (duration &gt;= 60) { close(g_events[checkpos].fd); //关闭与该客户端链接 printf(\"[fd=%d] timeout\\n\", g_events[checkpos].fd); eventdel(g_efd, &amp;g_events[checkpos]); //将该客户端 从红黑树 g_efd移除 } } /*监听红黑树g_efd, 将满足的事件的文件描述符加至events数组中, 1秒没有事件满足, 返回 0*/ int nfd = epoll_wait(g_efd, events, MAX_EVENTS+1, 1000); if (nfd &lt; 0) { printf(\"epoll_wait error, exit\\n\"); break; } for (i = 0; i &lt; nfd; i++) { /*使用自定义结构体myevent_s类型指针, 接收 联合体data的void *ptr成员*/ struct myevent_s *ev = (struct myevent_s *)events[i].data.ptr; if ((events[i].events &amp; EPOLLIN) &amp;&amp; (ev-&gt;events &amp; EPOLLIN)) { //读就绪事件 ev-&gt;call_back(ev-&gt;fd, events[i].events, ev-&gt;arg); } if ((events[i].events &amp; EPOLLOUT) &amp;&amp; (ev-&gt;events &amp; EPOLLOUT)) { //写就绪事件 ev-&gt;call_back(ev-&gt;fd, events[i].events, ev-&gt;arg); } } } /* 退出前释放所有资源 */ return 0;}","link":"/2019/04/26/epoll反应堆模型/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/03/16/hello-world/"},{"title":"libevent源码剖析(10)--让libevent支持多线程","text":"Libevent 本身不是多线程安全的，在多核的时代，如何能充分利用 CPU 的能力呢，这一节来说说如何在多线程环境中使用 libevent，跟源代码并没有太大的关系，纯粹是使用上的技巧。 1、错误使用示例在多核的 CPU 上只使用一个线程始终是对不起 CPU 的处理能力啊，那好吧，那就多创建几个线程，比如下面的简单服务器场景。 1 主线程创建工作线程1； 2 接着主线程监听在端口上,等待新的连接； 3 在线程 1 中执行 event 事件循环,等待事件到来; 4 新连接到来,主线程调用 libevent 接口 event_add 将新连接注册到 libevent 上; … … 上面的逻辑看起来没什么错误,在很多服务器设计中都可能用到主线程和工作线程的模式…. 可是就在线程 1 注册事件时,主线程很可能也在操作事件,比如删除,修改,通过 libevent的源代码也能看到,没有同步保护机制,问题麻烦了,看起来不能这样做啊,难道只能使用单线程不成!? 2、支持多线程的几种模式Libevent 并不是线程安全的,但这不代表 libevent 不支持多线程模式,其实方法在前面已经将 signal 事件处理时就接触到了,那就是消息通知机制。 一句话，“你发消息通知我,然后再由我在合适的时间来处理”； 说到这就再多说几句，再打个比方，把你自己比作一个工作线程，而你的头是主线程，你有一个消息信箱来接收别人发给你的消息，当时头有个新任务要指派给你。 2.1暴力抢占那么第一节中使用的多线程方法相当下面的流程: 1 当时你正在做事,比如在写文档; 2 你的头找到了一个任务,要指派给你,比如帮他搞个 PPT,哈; 3 头命令你马上搞 PPT,你这是不得不停止手头的工作,把 PPT 搞定了再接着写文档; … 2.2纯粹的消息通知机制那么基于纯粹的消息通知机制的多线程方式就像下面这样: 1 当时你正在写文档; 2 你的头找到了一个任务,要指派给你,帮他搞个 PPT; 3 头发个消息到你信箱,有个 PPT 要帮他搞定,这时你并不鸟他; 4 你写好文档,接着检查消息发现头有个 PPT 要你搞定,你开始搞 PPT; … 第一种的好处是消息可以立即得到处理,但是很方法很粗暴,你必须立即处理这个消息,所以你必须处理好切换问题,省得把文档上的内容不小心写到 PPT 里。在操作系统的进程通信中,消息队列(消息信箱)都是操作系统维护的,你不必关心。 第二种的优点是通过消息通知,切换问题省心了,不过消息是不能立即处理的(基于消息通知机制,这个总是难免的),而且所有的内容都通过消息发送,比如 PPT 的格式、内容等等信息,这无疑增加了通信开销。 2.3消息通知+同步有个折中机制可以减少消息通信的开销,就是提取一个同步层,还拿上面的例子来说,你把工作安排都存放在一个工作队列中,而且你能够保证“任何人把新任务扔到这个队列”,“自己取出当前第一个任务”等这些操作都能够保证不会把队列搞乱(其实就是个加锁的队列容器)。 再来看看处理过程和上面有什么不同: 1 当时你正在写文档; 2 你的头找到了一个任务,要指派给你,帮他搞个 PPT; 2 头有个 PPT 要你搞定,他把任务 push 到你的工作队列中,包括了 PPT 的格式、内容等信息; 3 头发个消息(一个字节)到你信箱,有个 PPT 要帮他搞定,这时你并不鸟他; 4 你写好文档,发现有新消息(这预示着有新任务来了),检查工作队列知道头有个 PPT要你搞定,你开始搞 PPT; … 工作队列其实就是一个加锁的容器(队列、链表等等),这个很容易实现实现;而消息通知仅需要一个字节,具体的任务都 push 到了在工作队列中,因此相比 2.2 减少了不少通信开销。 多线程编程有很多陷阱,线程间资源的同步互斥不是一两句能说得清的,而且出现 bug很难跟踪调试;这也有很多的经验和教训,因此如果让我选择,在绝大多数情况下都会选择机制 3 作为实现多线程的方法。 3、例子——memcachedMemcached 中的网络部分就是基于 libevent 完成的,其中的多线程模型就是典型的消息通知+同步层机制。下面的图足够说明其多线程模型了,其中有详细的文字说明。 注:该图的具体出处忘记了,感谢原作者。 4、小结本节更是 libevent 的使用方面的技巧,讨论了一下如何让 libevent 支持多线程,以及几种支持多线程的机制,memcached 使用 libevent 的多线程模型。","link":"/2019/05/12/libevent源码剖析-10-让libevent支持多线程/"},{"title":"libevent源码剖析(3)--核心event","text":"对事件处理流程有了高层的认识后,本节将详细介绍 libevent 的核心结构 event,以及libevent 对 event 的管理。 1、libevent的核心-eventLibevent 是基于事件驱动(event-driven)的,从名字也可以看到 event 是整个库的核心。event 就是 Reactor 框架中的事件处理程序组件;它提供了函数接口,供 Reactor 在事件发生时调用,以执行相应的事件处理,通常它会绑定一个有效的句柄。首先给出 event 结构体的声明,它位于 event.h 文件中: 1234567891011121314151617181920212223struct event { TAILQ_ENTRY (event) ev_next; TAILQ_ENTRY (event) ev_active_next; TAILQ_ENTRY (event) ev_signal_next; unsigned int min_heap_idx; /* for managing timeouts */ struct event_base *ev_base; int ev_fd; short ev_events; short ev_ncalls; short *ev_pncalls; /* Allows deletes in callback */ struct timeval ev_timeout; int ev_pri; /* smaller numbers are higher priority */ void (*ev_callback)(int, short, void *arg); void *ev_arg; int ev_res; /* result passed to event callback */ int ev_flags;}; 下面详细解释一下结构体中各字段的含义。 1)ev_events: event关注的事件类型，可以是以下3种类型： I/O事件: EV_WRITE和EV_READ 定时事件:EV_TIMEOUT 信号：EV_SIGNAL 辅助选项:EV_PERSIST,表明是一个永久事件 libevent中的定义为： 12345#define EV_TIMEOUT 0x01#define EV_READ 0x02#define EV_WRITE 0x04#define EV_SIGNAL 0x08#define EV_PERSIST 0x10 /* Persistant event */ 可以看出事件类型可以使用“|”运算符进行组合,需要说明的是,信号和I/O事件不能同时设置;还可以看出libevent使用event结构体将这3种事件的处理统一起来; 2)ev_next,ev_active_next 和 ev_signal_next 都是双向链表节点指针;它们是 libevent 对不同事件类型和在不同的时期,对事件的管理时使用到的字段。 libevent 使用双向链表保存所有注册的 I/O 和 Signal 事件,ev_next 就是该 I/O 事件在链表中的位置;称此链表为“已注册事件链表”; 同样 ev_signal_next 就是 signal 事件在 signal 事件链表中的位置; ev_active_next:libevent 将所有的激活事件放入到链表 active list 中,然后遍历 active list 执行调度,ev_active_next 就指明了 event 在 active list 中的位置; 3)min_heap_idx 和 ev_timeout 如果是 timeout 事件,它们是 event 在小根堆中的索引和超时值,libevent 使用小根堆来管理定时事件,这将在后面定时事件处理时专门讲解; 4)ev_base 该事件所属的反应堆实例,这是一个 event_base 结构体,下一节将会详细讲解; 5)ev_fd 对于 I/O 事件,是绑定的文件描述符;对于 signal 事件,是绑定的信号; 5)ev_callback event 的回调函数,被 ev_base 调用,执行事件处理程序,这是一个函数指针,原型为: 1void (*ev_callback)(int fd, short events, void *arg) 其中参数 fd 对应于 ev_fd；events 对应于 ev_events；arg 对应于 ev_arg； 6)ev_arg: void* 表明可以是任意类型的数据,在设置 event 时指定; 7)eb_flags:libevent 用于标记 event 信息的字段,表明其当前的状态,可能的值有: 123456#define EVLIST_TIMEOUT 0x01 //event在time堆中#define EVLIST_INSERTED 0x02 //event在已注册事件链表中#define EVLIST_SIGNAL 0x04 //未见使用#define EVLIST_ACTIVE 0x08 //event在激活链表中#define EVLIST_INTERNAL 0x10 //内部使用标记#define EVLIST_INIT 0x80 //event已被初始化 8)ev_ncalls:事件就绪执行时,调用 ev_callback 的次数,通常为 1; 9)ev_pncalls:指针,通常指向 ev_ncalls 或者为 NULL; 10)ev_res:记录了当前激活事件的类型; 2、libevent 对 event 的管理从event结构体中的3个链表节点指针和一个堆索引出发，大体上也能窥出 libevent 对event 的管理方法了，可以参见下面的示意图。 每次当有事件 event 转变为就绪状态时, libevent 就会把它移入到 active event list[priority]中,其中 priority 是 event 的优先级; 接着 libevent 会根据自己的调度策略选择就绪事件,调用其 cb_callback()函数执行事件处理;并根据就绪的句柄和事件类型填充 cb_callback 函数的参数。 3、事件设置的接口函数要向 libevent 添加一个事件,需要首先设置 event 对象,这通过调用 libevent 提供的函数有:event_set(), event_base_set(), event_priority_set() 来完成；下面分别进行讲解。 12345678910111213141516171819202122voidevent_set(struct event *ev, int fd, short events, void (*callback)(int, short, void *), void *arg){ /* Take the current base - caller needs to set the real base later */ ev-&gt;ev_base = current_base; ev-&gt;ev_callback = callback; ev-&gt;ev_arg = arg; ev-&gt;ev_fd = fd; ev-&gt;ev_events = events; ev-&gt;ev_res = 0; ev-&gt;ev_flags = EVLIST_INIT; ev-&gt;ev_ncalls = 0; ev-&gt;ev_pncalls = NULL; min_heap_elem_init(ev); /* by default, we put new events into the middle priority */ if(current_base) ev-&gt;ev_pri = current_base-&gt;nactivequeues/2;} 设置事件 ev 绑定的文件描述符或者信号,对于定时事件,设为-1 即可; 设置事件类型,比如 EV_READ|EV_PERSIST, EV_WRITE, EV_SIGNAL 等; 设置事件的回调函数以及参数 arg; 初始化其它字段,比如缺省的 event_base 和优先级; 123456789101112intevent_base_set(struct event_base *base, struct event *ev){ /* Only innocent events may be assigned to a different base */ if (ev-&gt;ev_flags != EVLIST_INIT) return (-1); ev-&gt;ev_base = base; ev-&gt;ev_pri = base-&gt;nactivequeues/2; return (0);} 设置 event ev 将要注册到的 event_base; libevent 有一个全局 event_base 指针 current_base,默认情况下事件 ev将被注册到 current_base 上,使用该函数可以指定不同的 event_base; 如果一个进程中存在多个 libevent 实例,则必须要调用该函数为 event 设置不同的 event_base; 1234567891011121314151617/* * Set's the priority of an event - if an event is already scheduled * changing the priority is going to fail. */intevent_priority_set(struct event *ev, int pri){ if (ev-&gt;ev_flags &amp; EVLIST_ACTIVE) return (-1); if (pri &lt; 0 || pri &gt;= ev-&gt;ev_base-&gt;nactivequeues) return (-1); ev-&gt;ev_pri = pri; return (0);} 设置event ev的优先级,没什么可说的,注意的一点就是:当ev正处于就绪状态时,不能设置,返回-1。 4、小结本节讲述了libevent的核心event结构,以及libevent支持的事件类型和libevent对event的管理模型;接下来将会描述libevent的事件处理框架,以及其中使用的重要的结构体event_base。 —-节选自《libevent 源码深度剖析》 张亮","link":"/2019/05/08/libevent源码剖析-3-核心event/"},{"title":"libevent源码剖析(4)--事件处理框架","text":"前面已经对 libevent 的事件处理框架和 event 结构体做了描述,现在是时候剖析 libevent 对事件的详细处理流程了,本节将分析 libevent 的事件处理框架 event_base 和 libevent 注册、 删除事件的具体流程,可结合前一节 libevent 对 event 的管理。 1、事件处理框架回想 Reactor 模式的几个基本组件,本节讲解的部分对应于 Reactor 框架组件。在 libevent中,这就表现为 event_base 结构体,结构体声明如下,它位于 event-internal.h 文件中: 1234567891011121314151617181920212223struct event_base { const struct eventop *evsel; void *evbase; int event_count; /* counts number of total events */ int event_count_active; /* counts number of active events */ int event_gotterm; /* Set to terminate loop */ int event_break; /* Set to terminate loop immediately */ /* active event management */ struct event_list **activequeues; int nactivequeues; /* signal handling info */ struct evsignal_info sig; struct event_list eventqueue; struct timeval event_tv; struct min_heap timeheap; struct timeval tv_cache;}; 下面详细解释一下结构体中各字段的含义。 1）evsel和evbase 这两个字段的设置可能会让人有些迷惑，这里你可以把evsel和evbase看作是类和静态函数的关系，比如添加事件时的调用行为：evsel-&gt;add(evbase, ev)，实际执行操作的是evbase；这相当于class::add(instance, ev)，instance就是class的一个对象实例。 evsel指向了全局变量static const struct eventop * eventops[]中的一个；前面也说过，libevent将系统提供的I/O demultiplex机制统一封装成了eventop结构；因此eventops[]包含了select、poll、kequeue和epoll等等其中的若干个全局实例对象。 evbase实际上是一个eventop实例对象； 先来看看eventop结构体，它的成员是一系列的函数指针, 在event-internal.h文件中： 1234567891011//一系列函数指针，对应epoll,select,poll等I/O demultiplex的五个接口函数struct eventop { const char *name; void *(*init)(struct event_base *); int (*add)(void *, struct event *); int (*del)(void *, struct event *); int (*dispatch)(struct event_base *, void *, struct timeval *); void (*dealloc)(struct event_base *, void *); /* set if we need to reinitialize the event base */ int need_reinit;}; 再来看全局变量static const struct eventop * eventops[]，在event.h文件中，该变量定义了几种可选的I/O demultiplex机制： 12345678910111213141516171819202122232425 /* In order of preference */static const struct eventop *eventops[] = {#ifdef HAVE_EVENT_PORTS &amp;evportops,#endif#ifdef HAVE_WORKING_KQUEUE &amp;kqops,#endif#ifdef HAVE_EPOLL &amp;epollops,#endif#ifdef HAVE_DEVPOLL &amp;devpollops,#endif#ifdef HAVE_POLL &amp;pollops,#endif#ifdef HAVE_SELECT &amp;selectops,#endif#ifdef WIN32 &amp;win32ops,#endif NULL}; 也就是说，在libevent中，每种I/O demultiplex机制的实现都必须提供这五个函数接口，来完成自身的初始化、销毁释放；对事件的注册、注销和分发。 比如对于epoll，libevent 实现了5个对应的接口函数，并在初始化时并将eventop的5个函数指针指向这5个函数，那么程序就可以使用epoll作为I/O demultiplex机制了，这个在后面会再次提到。 2）activequeues 是一个二级指针，前面讲过libevent支持事件优先级，因此你可以把它看作是数组，其中的元素activequeues[priority]是一个链表，链表的每个节点指向一个优先级为priority的就绪事件event。 3）eventqueue，链表，保存了所有的注册事件event的指针。 4）sig是用来管理信号的结构体，将在后面信号处理时专门讲解； 5）timeheap是管理定时事件的小根堆，将在后面定时事件处理时专门讲解； 6）event_tv和tv_cache是libevent用于时间管理的变量，将在后面讲到；其它各个变量都能因名知意，就不再啰嗦了。 2、创建和初始化event_base创建一个event_base 对象也既是创建了一个新的libevent 实例，程序需要通过调用event_init()（内部调用event_base_new函数执行具体操作）函数来创建，该函数同时还对新生成的libevent实例进行了初始化。该函数如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455struct event_base *event_init(void){ struct event_base *base = event_base_new(); if (base != NULL) current_base = base; return (base);}//event_base_new()函数实现struct event_base *event_base_new(void){ int i; struct event_base *base; //分配实例空间 if ((base = calloc(1, sizeof(struct event_base))) == NULL) event_err(1, \"%s: calloc\", __func__); event_sigcb = NULL; event_gotsig = 0; detect_monotonic(); gettime(base, &amp;base-&gt;event_tv); min_heap_ctor(&amp;base-&gt;timeheap); TAILQ_INIT(&amp;base-&gt;eventqueue); base-&gt;sig.ev_signal_pair[0] = -1; base-&gt;sig.ev_signal_pair[1] = -1; //初始化决定使用哪种I/O demultiplex机制 base-&gt;evbase = NULL; for (i = 0; eventops[i] &amp;&amp; !base-&gt;evbase; i++) { //确定使用哪种I/O demultiplex机制 base-&gt;evsel = eventops[i]; //实例化一个evsel，例如evsel为epoll，相当于调用epoll的init函数创建一个epoll实例 base-&gt;evbase = base-&gt;evsel-&gt;init(base); } if (base-&gt;evbase == NULL) event_errx(1, \"%s: no event mechanism available\", __func__); if (evutil_getenv(\"EVENT_SHOW_METHOD\")) event_msgx(\"libevent using: %s\\n\", base-&gt;evsel-&gt;name); /* allocate a single active event queue */ event_base_priority_init(base, 1); return (base);} 该函数首先为event_base实例申请空间，然后初始化timer mini-heap，选择并初始化合适的系统I/O 的demultiplexer机制，初始化各事件链表；函数还检测了系统的时间设置，为后面的时间管理打下基础。 3、接口函数前面提到Reactor框架（管理器）的作用就是提供事件的注册、注销接口；根据系统提供的事件多路分发机制执行事件循环，当有事件进入“就绪”状态时，调用注册事件的回调函数来处理事件。Libevent中对应的接口函数主要就是： int event_add(struct eventev, const struct timevaltimeout); int event_del(struct event*ev); int event_base_loop(struct event_base*base, int loops); void event_active(struct event*event, int res, short events); void event_process_active(struct event_base*base); 本节将按介绍事件注册和删除的代码流程，libevent 的事件循环框架将在下一节再具体描述。对于定时事件，这些函数将调用timer heap管理接口执行插入和删除操作；对于I/O和Signal事件将调用eventopadd和delete接口函数执行插入和删除操作（eventop会对Signal事件调用Signal处理接口执行操作）；这些组件将在后面的内容描述。 1）注册事件 函数原型： 1int event_add(struct event*ev, const struct timeval*tv) 参数：ev：指向要注册的事件； tv：超时时间； 函数将ev注册到ev-&gt;ev_base上，事件类型由ev-&gt;ev_events指明，如果注册成功，ev将被插入到已注册链表中；如果tv不是NULL，则会同时注册定时事件，将ev添加到timer堆上； 如果其中有一步操作失败，那么函数保证没有事件会被注册，可以讲这相当于一个原子操作。这个函数也体现了libevent细节之处的巧妙设计，且仔细看程序代码，部分有省略，注释直接附在代码中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889intevent_add(struct event *ev, const struct timeval *tv){ struct event_base *base = ev-&gt;ev_base; //要注册到的event_base const struct eventop *evsel = base-&gt;evsel; void *evbase = base-&gt;evbase; //base使用的系统I/O策略 int res = 0; //定义在log.h中 event_debug(( \"event_add: event: %p, %s%s%scall %p\", ev, ev-&gt;ev_events &amp; EV_READ ? \"EV_READ \" : \" \", ev-&gt;ev_events &amp; EV_WRITE ? \"EV_WRITE \" : \" \", tv ? \"EV_TIMEOUT \" : \" \", ev-&gt;ev_callback)); assert(!(ev-&gt;ev_flags &amp; ~EVLIST_ALL)); /* * prepare for timeout insertion further below, if we get a * failure on any step, we should not change any state. */ // 新的timer事件，调用timer heap接口在堆上预留一个位置 // 注：这样能保证该操作的原子性： // 向系统I/O机制注册可能会失败，而当在堆上预留成功后， // 定时事件的添加将肯定不会失败； // 而预留位置的可能结果是堆扩充，但是内部元素并不会改变 if (tv != NULL &amp;&amp; !(ev-&gt;ev_flags &amp; EVLIST_TIMEOUT)) { if (min_heap_reserve(&amp;base-&gt;timeheap, 1 + min_heap_size(&amp;base-&gt;timeheap)) == -1) return (-1); /* ENOMEM == errno */ } // 如果事件ev不在已注册或者激活链表中,则调用evbase注册事件 if ((ev-&gt;ev_events &amp; (EV_READ|EV_WRITE|EV_SIGNAL)) &amp;&amp; !(ev-&gt;ev_flags &amp; (EVLIST_INSERTED|EVLIST_ACTIVE))) { res = evsel-&gt;add(evbase, ev); if (res != -1) //注册成功，插入event到已注册链表中 event_queue_insert(base, ev, EVLIST_INSERTED); } /* * we should change the timout state only if the previous event * addition succeeded. */ //准备添加定时事件 if (res != -1 &amp;&amp; tv != NULL) { struct timeval now; /* * we already reserved memory above for the case where we * are not replacing an exisiting timeout. */ //EVLIST_TIMEOUT表明event已经在定时器堆中了，删除旧的 if (ev-&gt;ev_flags &amp; EVLIST_TIMEOUT) event_queue_remove(base, ev, EVLIST_TIMEOUT); /* Check if it is active due to a timeout. Rescheduling * this timeout before the callback can be executed * removes it from the active list. */ //如果事件已经是就绪状态则从激活链表中删除 if ((ev-&gt;ev_flags &amp; EVLIST_ACTIVE) &amp;&amp; (ev-&gt;ev_res &amp; EV_TIMEOUT)) { /* See if we are just active executing this * event in a loop */ //将ev_callback调用次数设置为0 if (ev-&gt;ev_ncalls &amp;&amp; ev-&gt;ev_pncalls) { /* Abort loop */ *ev-&gt;ev_pncalls = 0; } event_queue_remove(base, ev, EVLIST_ACTIVE); } //计算时间，并插入到timer小根堆中 gettime(base, &amp;now); evutil_timeradd(&amp;now, tv, &amp;ev-&gt;ev_timeout); event_debug(( \"event_add: timeout in %ld seconds, call %p\", tv-&gt;tv_sec, ev-&gt;ev_callback)); event_queue_insert(base, ev, EVLIST_TIMEOUT); } return (res);} event_queue_insert()负责将事件插入到对应的链表中,下面是程序代码; event_queue_remove()负责将事件从对应的链表中删除,这里就不再重复贴代码了; 12345678910111213141516171819202122232425262728293031323334voidevent_queue_insert(struct event_base *base, struct event *ev, int queue){ //ev可能已经在激活列表中了，避免重复插入 if (ev-&gt;ev_flags &amp; queue) { /* Double insertion is possible for active events */ if (queue &amp; EVLIST_ACTIVE) return; event_errx(1, \"%s: %p(fd %d) already on queue %x\", __func__, ev, ev-&gt;ev_fd, queue); } if (~ev-&gt;ev_flags &amp; EVLIST_INTERNAL) base-&gt;event_count++; ev-&gt;ev_flags |= queue; //记录queue标记 switch (queue) { case EVLIST_INSERTED: //I/O或Signal事件，加入已注册事件链表 TAILQ_INSERT_TAIL(&amp;base-&gt;eventqueue, ev, ev_next); break; case EVLIST_ACTIVE: //就绪事件，加入已注册事件链表 base-&gt;event_count_active++; TAILQ_INSERT_TAIL(base-&gt;activequeues[ev-&gt;ev_pri], ev,ev_active_next); break; case EVLIST_TIMEOUT: { //定时事件，加入堆 min_heap_push(&amp;base-&gt;timeheap, ev); break; } default: event_errx(1, \"%s: unknown queue %x\", __func__, queue); }} 2)删除事件 函数原型为： 1int event_del(struct event*ev); 该函数将删除事件ev，对于I/O事件，从I/O 的demultiplexer上将事件注销；对于Signal事件，将从Signal事件链表中删除；对于定时事件，将从堆上删除； 同样删除事件的操作则不一定是原子的，比如删除时间事件之后，有可能从系统I/O机制中注销会失败。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 intevent_del(struct event *ev){ struct event_base *base; const struct eventop *evsel; void *evbase; event_debug((\"event_del: %p, callback %p\", ev, ev-&gt;ev_callback)); /* An event without a base has not been added */ //ev_base为NULL，表明ev没有被注册 if (ev-&gt;ev_base == NULL) return (-1); //取得注册的event_base和eventop指针 base = ev-&gt;ev_base; evsel = base-&gt;evsel; evbase = base-&gt;evbase; assert(!(ev-&gt;ev_flags &amp; ~EVLIST_ALL)); /* See if we are just active executing this event in a loop */ //将ev_callback调用次数设置为0 if (ev-&gt;ev_ncalls &amp;&amp; ev-&gt;ev_pncalls) { /* Abort loop */ *ev-&gt;ev_pncalls = 0; } //从对应的链表中删除 if (ev-&gt;ev_flags &amp; EVLIST_TIMEOUT) event_queue_remove(base, ev, EVLIST_TIMEOUT); if (ev-&gt;ev_flags &amp; EVLIST_ACTIVE) event_queue_remove(base, ev, EVLIST_ACTIVE); if (ev-&gt;ev_flags &amp; EVLIST_INSERTED) { event_queue_remove(base, ev, EVLIST_INSERTED); //EVLIST_INSERTED表明I/O或者Signal事件 //需要调用I/O demultiplexer注销事件 return (evsel-&gt;del(evbase, ev)); } return (0);} 4、小结分析了 event_base 这一重要结构体,初步看到了 libevent 对系统的 I/O demultiplex 机制的封装 event_op 结构,并结合源代码分析了事件的注册和删除处理,下面将会接着分析事件管理框架中的主事件循环部分。","link":"/2019/05/08/libevent源码剖析-4-事件处理框架/"},{"title":"libevent源码剖析(5)--事件主循环","text":"在初步了解了 libevent 的 Reactor 组件——event_base 和事件管理框架后,接下来就是 libevent 事件处理的中心部分——事件主循环,根据系统提供的事件多路分发机制执行事件循环,对已注册的就绪事件,调用注册事件的回调函数来处理事件。 1、事件处理主循环Libevent 的事件主循环主要是通过 event_base_loop () 函数完成的,其主要操作如下面的流程图所示,event_base_loop 所作的就是持续执行下面的循环。 清楚了 event_base_loop 所作的主要操作,就可以对比源代码看个究竟了,代码结构还是相当清晰的。 该函数完成以下工作： 1.信号标记被设置，则调用信号的回调函数 2.根据定时器最小时间，设置I/O多路复用的最大等待时间，这样即使没有I/O事件发生，也能在最小定时器超时时返回。 3.调用I/O多路复用，监听事件，将活跃事件添加到活跃事件链表中 4.检查定时事件，将就绪的定时事件从小根堆中删除，插入到活跃事件链表中 5.对event_base的活跃事件链表中的事件，调用event_process_active(）函数，在该函数内调用event的回调函数，优先级高的event先处理。 该函数内部调用了eventop.dispatch()监听事件，event_sigcb函数指针处理信号事件，timeout_process()将超时的定时事件加入到活跃事件链表中，event_process_active()处理活跃事件链表中的事件，调用相应的回调函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106intevent_base_loop(struct event_base *base, int flags){ const struct eventop *evsel = base-&gt;evsel; void *evbase = base-&gt;evbase; struct timeval tv; struct timeval *tv_p; int res, done; /* clear time cache */ //清时间缓存 base-&gt;tv_cache.tv_sec = 0; //evsignal_base是全局变量，在处理signal时，用于指明signal所属的event_base实例 if (base-&gt;sig.ev_signal_added) evsignal_base = base; done = 0; while (!done) { //事件主循环 /* Terminate the loop if we have been asked to */ //查看是否需要跳出循环，程序可以调用event_loopexit_cb()设置event_gotterm标记 if (base-&gt;event_gotterm) { base-&gt;event_gotterm = 0; break; } //调用event_base_loopbreak()设置event_break标记 if (base-&gt;event_break) { base-&gt;event_break = 0; break; } /* You cannot use this interface for multi-threaded apps */ //当event_gotsig被设置时，则event_sigcb就是信号处理的回调函数 while (event_gotsig) { event_gotsig = 0; if (event_sigcb) { res = (*event_sigcb)(); //调用信号处理的回调函数 if (res == -1) { errno = EINTR; return (-1); } } } /* 校正系统时间,如果系统使用的是非MONOTONIC时间,用户可能会向后调整了系统时间 在timeout_correct函数里,比较last wait time和当前时间,如果当前时间&lt; last wait time 表明时间有问题,这时需要更新timer_heap中所有定时事件的超时时间。 */ timeout_correct(base, &amp;tv); tv_p = &amp;tv; //根据定时器堆中最小超时时间计算I/O多路复用的最大等待时间tv_p if (!base-&gt;event_count_active &amp;&amp; !(flags &amp; EVLOOP_NONBLOCK)) { timeout_next(base, &amp;tv_p); } else { /* * if we have active events, we just poll new events * without waiting. */ // evutil_timerclear(&amp;tv); } /* If we have no events, we just exit */ //没有注册事件，则退出 if (!event_haveevents(base)) { event_debug((\"%s: no events registered.\", __func__)); return (1); } /* update last old time */ //更新last wait time,并清空time cache gettime(base, &amp;base-&gt;event_tv); /* clear time cache */ base-&gt;tv_cache.tv_sec = 0; //调用I/O多路复用，监听事件，等待就绪I/Oevents，可能是epoll_wait()，或者select等 //在evsel-&gt;dispatch()中,会把就绪signal event、I/O event插入到激活链表中 res = evsel-&gt;dispatch(base, evbase, tv_p); if (res == -1) return (-1); //将将time cache赋值为当前系统时间 gettime(base, &amp;base-&gt;tv_cache); //检查定时事件，将就绪的定时事件从小根堆中删除，插入到活跃事件链表中 timeout_process(base); if (base-&gt;event_count_active) { //处理event_base的活跃链表中的事件 //调用event的回调函数，优先级高的event先处理 event_process_active(base); if (!base-&gt;event_count_active &amp;&amp; (flags &amp; EVLOOP_ONCE)) done = 1; } else if (flags &amp; EVLOOP_NONBLOCK) done = 1; } /* clear time cache */ //循环结束，清空时间缓存 base-&gt;tv_cache.tv_sec = 0; event_debug((\"%s: asked to terminate loop.\", __func__)); return (0);} 在event_base_loop()中，有两个函数需要重点关注，一个是dispatch()函数，一个是event_process_active()函数,下面分别对这两个函数详解。 1.1dispatch()函数在上面我们看到，event_base_loop()中通过I/O多路复用的dispatch()函数完成监听事件功能。在之前的event_init()中我们看到，通过遍历eventops数组，从中选择一个I/O多路复用机制，所以不同的I/O多路复用机制有不同的eventop结构体，相应的也就有不同的 dispatch() 函数。下面，再次看下eventop结构体(event-internal.h) 12345678910struct eventop { const char *name; void *(*init)(struct event_base *); //初始化 int (*add)(void *, struct event *); //注册事件 int (*del)(void *, struct event *); //删除事件 int (*dispatch)(struct event_base *, void *, struct timeval *); //事件分发 void (*dealloc)(struct event_base *, void *); //注销，释放资源 /* set if we need to reinitialize the event base */ int need_reinit;}; 在event_add()中通过add()成员函数注册event到监听事件中，现在在event_base_loop()中通过dispatch()成员函数监听事件。libevent支持多种I/O多路复用机制，下面先看下epoll的eventop结构体(epoll.c) 123456789const struct eventop epollops = { \"epoll\", epoll_init, epoll_add, epoll_del, epoll_dispatch, epoll_dealloc, 1 /* need reinit */}; 然后看下epoll的dispatch()函数(epoll.c) 从下面源码可见，epoll_dispatch()的工作主要有： 1.调用epoll_wait()监听事件 2.如果有信号发生，调用evsignal_process()处理信号 3.将活跃的event根据其活跃的类型注册到活跃事件链表上 4.如果events数组大小不够，则重新分配为原来2倍大小 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768static intepoll_dispatch(struct event_base *base, void *arg, struct timeval *tv){ struct epollop *epollop = arg; struct epoll_event *events = epollop-&gt;events; struct evepoll *evep; int i, res, timeout = -1; if (tv != NULL) timeout = tv-&gt;tv_sec * 1000 + (tv-&gt;tv_usec + 999) / 1000; //转换为微米 if (timeout &gt; MAX_EPOLL_TIMEOUT_MSEC) { //设置最大超时时间 /* Linux kernels can wait forever if the timeout is too big; * see comment on MAX_EPOLL_TIMEOUT_MSEC. */ timeout = MAX_EPOLL_TIMEOUT_MSEC; } res = epoll_wait(epollop-&gt;epfd, events, epollop-&gt;nevents, timeout); //监听事件发生 if (res == -1) { if (errno != EINTR) { event_warn(\"epoll_wait\"); return (-1); } evsignal_process(base); //由于Signal事件发生中断，处理Signal事件 return (0); } else if (base-&gt;sig.evsignal_caught) { evsignal_process(base); //有Signal事件发生，处理Signal事件 } event_debug((\"%s: epoll_wait reports %d\", __func__, res)); for (i = 0; i &lt; res; i++) { //处理活跃事件 int what = events[i].events; //活跃类型 struct event *evread = NULL, *evwrite = NULL; int fd = events[i].data.fd; //event的文件描述符 if (fd &lt; 0 || fd &gt;= epollop-&gt;nfds) continue; evep = &amp;epollop-&gt;fds[fd]; if (what &amp; (EPOLLHUP|EPOLLERR)) { //判断epoll的events类型，并找到注册的event evread = evep-&gt;evread; evwrite = evep-&gt;evwrite; } else { if (what &amp; EPOLLIN) { evread = evep-&gt;evread; } if (what &amp; EPOLLOUT) { evwrite = evep-&gt;evwrite; } } if (!(evread||evwrite)) continue; //添加event到活跃事件链表中 if (evread != NULL) event_active(evread, EV_READ, 1); if (evwrite != NULL) event_active(evwrite, EV_WRITE, 1); } //如果注册的事件全部变为活跃，则增大events数组为原来两倍 if (res == epollop-&gt;nevents &amp;&amp; epollop-&gt;nevents &lt; MAX_NEVENTS) { /* We used all of the event space this time. We should be ready for more events next time. */ int new_nevents = epollop-&gt;nevents * 2; struct epoll_event *new_events; new_events = realloc(epollop-&gt;events, new_nevents * sizeof(struct epoll_event)); if (new_events) { epollop-&gt;events = new_events; epollop-&gt;nevents = new_nevents; } } return (0);} 1.2event_process_active()函数在event_base_loop()中，当活跃的I/O事件、定时器事件已经添加到活跃事件链表中后，开始调用这些event的回调函数进行处理了，这步是在event_base_loop()中调用event_process_active()来完成的。该函数从event_base的activequeueus链表数组上取出一个链表；对该链表上的event调用回调函数；优先调用优先级值最小的event 12345678910111213141516171819202122232425262728293031323334353637383940static voidevent_process_active(struct event_base *base){ struct event *ev; struct event_list *activeq = NULL; int i; short ncalls; for (i = 0; i &lt; base-&gt;nactivequeues; ++i) { //取出第一个活跃链表 if (TAILQ_FIRST(base-&gt;activequeues[i]) != NULL) { activeq = base-&gt;activequeues[i]; break; } } assert(activeq != NULL); //优先处理优先级值最小的event for (ev = TAILQ_FIRST(activeq); ev; ev = TAILQ_FIRST(activeq)) { if (ev-&gt;ev_events &amp; EV_PERSIST) event_queue_remove(base, ev, EVLIST_ACTIVE); //是持久事件，则从活跃链表移除 else event_del(ev); //不是持久事件，则直接删除该事件 /* Allows deletes to work */ ncalls = ev-&gt;ev_ncalls; ev-&gt;ev_pncalls = &amp;ncalls; while (ncalls) { ncalls--; ev-&gt;ev_ncalls = ncalls; //调用该event的回调函数，event.ev_res保存返回值 (*ev-&gt;ev_callback)((int)ev-&gt;ev_fd, ev-&gt;ev_res, ev-&gt;ev_arg); if (event_gotsig || base-&gt;event_break) { ev-&gt;ev_pncalls = NULL; return; } } ev-&gt;ev_pncalls = NULL; }} 2、I/O 和 Timer 事件的统一Libevent 将 Timer 和 Signal 事件都统一到了系统的 I/O 的 demultiplex 机制中了,相信读者从上面的流程和代码中也能窥出一斑了,下面就再啰嗦一次了。 首先将 Timer 事件融合到系统 I/O 多路复用机制中,还是相当清晰的,因为系统的 I/O机制像 select()和 epoll_wait()都允许程序制定一个最大等待时间(也称为最大超时时间)timeout,即使没有 I/O 事件发生,它们也保证能在 timeout 时间内返回。 那么根据所有 Timer 事件的最小超时时间来设置系统 I/O 的 timeout 时间;当系统 I/O返回时,再激活所有就绪的 Timer 事件就可以了,这样就能将 Timer 事件完美的融合到系统的 I/O 机制中了。 这是在 Reactor 和 Proactor 模式(主动器模式,比如 Windows 上的 IOCP)中处理 Timer事件的经典方法了,ACE 采用的也是这种方法,大家可以参考 POSA vol2 书中的 Reactor模式一节。 堆是一种经典的数据结构,向堆中插入、删除元素时间复杂度都是 O(lgN),N 为堆中元素的个数,而获取最小key 值(小根堆)的复杂度为 O(1);因此变成了管理 Timer 事件的绝佳人选(当然是非唯一的),libevent 就是采用的堆结构。 3、I/O 和 Signal 事件的统一Signal 是异步事件的经典事例,将 Signal 事件统一到系统的 I/O 多路复用中就不像 Timer事件那么自然了,Signal 事件的出现对于进程来讲是完全随机的,进程不能只是测试一个变量来判别是否发生了一个信号,而是必须告诉内核“在此信号发生时,请执行如下的操作”。 如果当 Signal 发生时,并不立即调用 event 的 callback 函数处理信号,而是设法通知系统的 I/O 机制,让其返回,然后再统一和 I/O 事件以及 Timer 一起处理,不就可以了嘛。是的,这也是 libevent 中使用的方法。 问题的核心在于,当 Signal 发生时,如何通知系统的 I/O 多路复用机制,这里先买个小关子,放到信号处理一节再详细说明,我想读者肯定也能想出通知的方法,比如使用 pipe。 4、小结本节介绍了 libevent 的事件主循环,描述了 libevent 是如何处理就绪的 I/O 事件、定时器和信号事件,以及如何将它们无缝的融合到一起。","link":"/2019/05/09/libevent源码剖析-5-事件主循环/"},{"title":"libevent源码剖析(6)--集成信号处理","text":"现在我们已经了解了 libevent 的基本框架:事件管理框架和事件主循环。上节提到了libevent 中 I/O 事件和 Signal 以及 Timer 事件的集成,这一节将分析如何将 Signal 集成到事件主循环的框架中。 1、集成策略——使用 socket pair前一节已经做了足够多的介绍了,基本方法就是采用“消息机制”。在 libevent 中这是通过 socket pair 完成的,下面就来详细分析一下。 Socket pair 就是一个 socket 对,包含两个 socket,一个读 socket,一个写 socket。工作方式如下图所示: 创建一个 socket pair 并不是复杂的操作,可以参见下面的流程图,清晰起见,其中忽略了一些错误处理和检查。 Libevent 提供了辅助函数 evutil_socketpair()来创建一个 socket pair,可以结合上面的创建流程来分析该函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697intevutil_socketpair(int family, int type, int protocol, int fd[2]){#ifndef WIN32 return socketpair(family, type, protocol, fd);#else /* This code is originally from Tor. Used with permission. */ /* This socketpair does not work when localhost is down. So * it's really not the same thing at all. But it's close enough * for now, and really, when localhost is down sometimes, we * have other problems too. */ int listener = -1; int connector = -1; int acceptor = -1; struct sockaddr_in listen_addr; struct sockaddr_in connect_addr; int size; int saved_errno = -1; if (protocol#ifdef AF_UNIX || family != AF_UNIX#endif ) { EVUTIL_SET_SOCKET_ERROR(WSAEAFNOSUPPORT); return -1; } if (!fd) { EVUTIL_SET_SOCKET_ERROR(WSAEINVAL); return -1; } listener = socket(AF_INET, type, 0); if (listener &lt; 0) return -1; memset(&amp;listen_addr, 0, sizeof(listen_addr)); listen_addr.sin_family = AF_INET; listen_addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK); listen_addr.sin_port = 0; /* kernel chooses port. */ if (bind(listener, (struct sockaddr *) &amp;listen_addr, sizeof (listen_addr)) == -1) goto tidy_up_and_fail; if (listen(listener, 1) == -1) goto tidy_up_and_fail; connector = socket(AF_INET, type, 0); if (connector &lt; 0) goto tidy_up_and_fail; /* We want to find out the port number to connect to. */ size = sizeof(connect_addr); if (getsockname(listener, (struct sockaddr *) &amp;connect_addr, &amp;size) == -1) goto tidy_up_and_fail; if (size != sizeof (connect_addr)) goto abort_tidy_up_and_fail; if (connect(connector, (struct sockaddr *) &amp;connect_addr, sizeof(connect_addr)) == -1) goto tidy_up_and_fail; size = sizeof(listen_addr); acceptor = accept(listener, (struct sockaddr *) &amp;listen_addr, &amp;size); if (acceptor &lt; 0) goto tidy_up_and_fail; if (size != sizeof(listen_addr)) goto abort_tidy_up_and_fail; EVUTIL_CLOSESOCKET(listener); /* Now check we are talking to ourself by matching port and host on the two sockets. */ if (getsockname(connector, (struct sockaddr *) &amp;connect_addr, &amp;size) == -1) goto tidy_up_and_fail; if (size != sizeof (connect_addr) || listen_addr.sin_family != connect_addr.sin_family || listen_addr.sin_addr.s_addr != connect_addr.sin_addr.s_addr || listen_addr.sin_port != connect_addr.sin_port) goto abort_tidy_up_and_fail; fd[0] = connector; fd[1] = acceptor; return 0; abort_tidy_up_and_fail: saved_errno = WSAECONNABORTED; tidy_up_and_fail: if (saved_errno &lt; 0) saved_errno = WSAGetLastError(); if (listener != -1) EVUTIL_CLOSESOCKET(listener); if (connector != -1) EVUTIL_CLOSESOCKET(connector); if (acceptor != -1) EVUTIL_CLOSESOCKET(acceptor); EVUTIL_SET_SOCKET_ERROR(saved_errno); return -1;#endif} 2、集成到事件主循环——通知 event_baseSocket pair 创建好了,可是 libevent 的事件主循环还是不知道 Signal 是否发生了啊,看来我们还差了最后一步,那就是:为 socket pair 的读 socket 在 libevent 的 event_base 实例上注册一个 persist 的读事件。 这样当向写 socket 写入数据时,读 socket 就会得到通知,触发读事件,从而 event_base就能相应的得到通知了。 前面提到过,Libevent 会在事件主循环中检查标记,来确定是否有触发的 signal,如果标记被设置就处理这些 signal,这段代码在各个具体的 I/O 机制中,以 Epoll 为例,在epoll_dispatch()函数中,代码片段如下: 12345678910111213res = epoll_wait(epollop-&gt;epfd, events, epollop-&gt;nevents, timeout); //监听事件发生 if (res == -1) { if (errno != EINTR) { event_warn(\"epoll_wait\"); return (-1); } evsignal_process(base); //由于Signal事件发生中断，处理Signal事件 return (0); } else if (base-&gt;sig.evsignal_caught) { evsignal_process(base); //有Signal事件发生，处理Signal事件 } 完整的处理框架如下所示: 注 1:libevent 中,初始化阶段并不注册读 socket 的读事件,而是在注册信号阶段才会测试并注册; 注 2:libevent 中,检查 I/O 事件是在各系统 I/O 机制的 dispatch()函数中完成的,该 dispatch()函数在event_base_loop()函数中被调用; 3、evsignal_info 结构体Libevent 中 Signal 事件的管理是通过结构体 evsignal_info 完成的,结构体位于 evsignal.h文件中，定义如下: 1234567891011121314struct evsignal_info { struct event ev_signal; int ev_signal_pair[2]; int ev_signal_added; volatile sig_atomic_t evsignal_caught; struct event_list evsigevents[NSIG]; sig_atomic_t evsigcaught[NSIG];#ifdef HAVE_SIGACTION struct sigaction **sh_old;#else ev_sighandler_t **sh_old;#endif int sh_old_max;}; 下面详细介绍一下个字段的含义和作用: 1)ev_signal 为 socket pair 的读 socket 向 event_base 注册读事件时使用的 event 结构体; 2)ev_signal_pair socket pair 对,作用见第一节的介绍; 3)ev_signal_added 记录 ev_signal 事件是否已经注册了; 4)evsignal_caught 是否有信号发生的标记;是 volatile 类型,因为它会在另外的线程中被修改; 5)evsigvents[NSIG] 数组,evsigevents[signo]表示注册到信号 signo 的事件链表; 6)evsigcaught[NSIG] 具体记录每个信号触发的次数,evsigcaught[signo]是记录信号 signo被触发的次数; 7)sh_old 记录了原来的 signal 处理函数指针,当信号 signo 注册的 event 被清空时,需要重新设置其处理函数; evsignal_info 的初始化包括，创建 socket pair，设置 ev_signal 事件(但并没有注册,而是等到有信号注册时才检查并注册)，并将所有标记置零，初始化信号的注册事件链表指针等。 4、注册、注销 signal 事件注册 signal 事件是通过 evsignal_add(struct event *ev) 函数完成的。 libevent 对所有的信号注册同一个处理函数 evsignal_handler()，该函数将在下一段介绍，注册过程如下： 1234567891011121314151617181920212223242526272829303132333435363738intevsignal_add(struct event *ev){ int evsignal; struct event_base *base = ev-&gt;ev_base; struct evsignal_info *sig = &amp;ev-&gt;ev_base-&gt;sig; if (ev-&gt;ev_events &amp; (EV_READ|EV_WRITE)) event_errx(1, \"%s: EV_SIGNAL incompatible use\", __func__); evsignal = EVENT_SIGNAL(ev); //取得 ev 要注册到的信号 signo assert(evsignal &gt;= 0 &amp;&amp; evsignal &lt; NSIG); //如果信号 signo 未被注册 if (TAILQ_EMPTY(&amp;sig-&gt;evsigevents[evsignal])) { event_debug((\"%s: %p: changing signal handler\", __func__, ev)); //为 signo 注册信号处理函数 evsignal_handler(); if (_evsignal_set_handler( base, evsignal, evsignal_handler) == -1) return (-1); /* catch signals if they happen quickly */ evsignal_base = base; //如果事件 ev_signal 还没哟注册,就注册 ev_signal 事件 if (!sig-&gt;ev_signal_added) { if (event_add(&amp;sig-&gt;ev_signal, NULL)) return (-1); sig-&gt;ev_signal_added = 1; } } /* multiple events may listen to the same signal */ //将事件 ev 添加到 signo 的 event 链表中; TAILQ_INSERT_TAIL(&amp;sig-&gt;evsigevents[evsignal], ev, ev_signal_next); return (0);} evsignal_add(struct event *ev)函数如下： 1 取得 ev 要注册到的信号 signo; 2 如果信号 signo 未被注册,那么就为 signo 注册信号处理函数 evsignal_handler(); 3 如果事件 ev_signal 还没哟注册,就注册 ev_signal 事件; 4 将事件 ev 添加到 signo 的 event 链表中; 从 signo 上注销一个已注册的 signal 事件就更简单了,直接从其已注册事件的链表中移除即可。如果事件链表已空,那么就恢复旧的处理函数; 下面的讲解都以 signal()函数为例,sigaction()函数的处理和 signal()相似。 处理函数evsignal_handler()函数做的事情很简单,就是记录信号的发生次数,并通知event_base有信号触发,需要处理: 12345678910111213141516171819202122232425static voidevsignal_handler(int sig){ int save_errno = errno; //不覆盖原来的错误代码 if (evsignal_base == NULL) { event_warn( \"%s: received signal %d, but have no base configured\", __func__, sig); return; } // 记录信号sig的触发次数,并设置event触发标记 evsignal_base-&gt;sig.evsigcaught[sig]++; evsignal_base-&gt;sig.evsignal_caught = 1;#ifndef HAVE_SIGACTION signal(sig, evsignal_handler); //重新注册信号#endif /* Wake up our notification mechanism */ // 向写socket写一个字节数据,触发event_base的I/O事件,从而通知其有信号触发,需要处理 send(evsignal_base-&gt;sig.ev_signal_pair[0], \"a\", 1, 0); errno = save_errno; //错误代码} 4、小结本节介绍了 libevent 对 signal 事件的具体处理框架,包括事件注册、删除和 socket pair通知机制,以及是如何将 Signal 事件集成到事件主循环之中的。","link":"/2019/05/09/libevent源码剖析-6-集成信号处理/"},{"title":"libevent源码剖析(7)--集成定时器事件","text":"现在再来详细分析 libevent 中 I/O 事件和 Timer 事件的集成,与 Signal 相比,Timer 事件的集成会直观和简单很多。Libevent 对堆的调整操作做了一些优化,本节还会描述这些优化方法。 1、集成到事件主循环因为系统的 I/O 机制像 select()和 epoll_wait()都允许程序制定一个最大等待时间(也称为最大超时时间)timeout,即使没有 I/O 事件发生,它们也保证能在 timeout 时间内返回。 那么根据所有 Timer 事件的最小超时时间来设置系统 I/O 的 timeout 时间;当系统 I/O返回时,再激活所有就绪的 Timer 事件就可以了,这样就能将 Timer 事件完美的融合到系统的 I/O 机制中了。 具体的代码在源文件 event.c 的 event_base_loop()中,现在就对比代码来看看这一处理方法: 12345678910111213141516171819202122232425262728293031323334static inttimeout_next(struct event_base *base, struct timeval **tv_p){ struct timeval now; struct event *ev; struct timeval *tv = *tv_p; //堆的首元素具有最小的超时值 if ((ev = min_heap_top(&amp;base-&gt;timeheap)) == NULL) { // 如果没有定时事件,将等待时间设置为NULL,表示一直阻塞直到有I/O事件发生 /* if no time-based events are active wait for I/O */ *tv_p = NULL; return (0); } //取得当前时间 if (gettime(base, &amp;now) == -1) return (-1); //如果超时事件&lt;=当前值，不能等待，需要立即返回 if (evutil_timercmp(&amp;ev-&gt;ev_timeout, &amp;now, &lt;=)) { evutil_timerclear(tv); return (0); } //计算等待的事件=当前事件-最小的超时时间 evutil_timersub(&amp;ev-&gt;ev_timeout, &amp;now, tv); assert(tv-&gt;tv_sec &gt;= 0); assert(tv-&gt;tv_usec &gt;= 0); event_debug((\"timeout_next: in %ld seconds\", tv-&gt;tv_sec)); return (0);} 2、Timer小根堆Libevent 使用堆来管理 Timer 事件,其 key 值就是事件的超时时间,源代码位于文件min_heap.h 中。 所有的数据结构书中都有关于堆的详细介绍,向堆中插入、删除元素时间复杂度都是O(lgN),N 为堆中元素的个数,而获取最小 key 值(小根堆)的复杂度为 O(1)。堆是一个完全二叉树,基本存储方式是一个数组。 Libevent 实现的堆还是比较轻巧的。轻巧到什么地方呢,就以插入元素为例,来对比说明,下面伪代码中的 size 表示当前堆的元素个数: 典型的代码逻辑如下： 而 libevent 的 heap 代码对这一过程做了优化,在插入新元素时,只是为新元素预留了一个位置 hole(初始时 hole 位于数组尾部),但并不立刻将新元素插入到 hole 上,而是不断向上调整 hole 的值,将父节点向下调整,最后确认 hole 就是新元素的所在位置时,才会真正的将新元素插入到 hole 上,因此在调整过程中就比上面的代码少了一次赋值的操作,代码逻辑是: 由于每次调整都少做一次赋值操作,在调整路径比较长时,调整效率会比第一种有所提高。libevent 中的min_heap_shift_up_()函数就是上面逻辑的具体实现,对应的向下调整函数是min_heap_shift_down_()。 1234567891011121314151617181920212223242526void min_heap_shift_up_(min_heap_t* s, unsigned hole_index, struct event* e){ unsigned parent = (hole_index - 1) / 2; while(hole_index &amp;&amp; min_heap_elem_greater(s-&gt;p[parent], e)) { (s-&gt;p[hole_index] = s-&gt;p[parent])-&gt;min_heap_idx = hole_index; hole_index = parent; parent = (hole_index - 1) / 2; } (s-&gt;p[hole_index] = e)-&gt;min_heap_idx = hole_index;}void min_heap_shift_down_(min_heap_t* s, unsigned hole_index, struct event* e){ unsigned min_child = 2 * (hole_index + 1); while(min_child &lt;= s-&gt;n) { min_child -= min_child == s-&gt;n || min_heap_elem_greater(s-&gt;p[min_child], s-&gt;p[min_child - 1]); if(!(min_heap_elem_greater(e, s-&gt;p[min_child]))) break; (s-&gt;p[hole_index] = s-&gt;p[min_child])-&gt;min_heap_idx = hole_index; hole_index = min_child; min_child = 2 * (hole_index + 1); } min_heap_shift_up_(s, hole_index, e);} 举个例子,向一个小根堆 3, 5, 8, 7, 12 中插入新元素 2,使用第一中典型的代码逻辑,其调整过程如下图所示: 使用 libevent 中的堆调整逻辑,调整过程如下图所示: 对于删除和元素修改操作,也遵从相同的逻辑,就不再罗嗦了。 3、小结通过设置系统 I/O 机制的 wait 时间,从而简捷的集成 Timer 事件;主要分析了 libevent对堆调整操作的优化。","link":"/2019/05/10/libevent源码剖析-7-集成定时器事件/"},{"title":"libevent源码剖析(8)--支持I/O多路复用","text":"Libevent 的核心是事件驱动、同步非阻塞,为了达到这一目标,必须采用系统提供的 I/O多路复用技术,而这些在Windows、Linux、Unix 等不同平台上却各有不同,如何能提供优雅而统一的支持方式,是首要关键的问题,这其实不难,本节就来分析一下。 1、统一的关键Libevent支持多种I/O多路复用技术的关键就在于结构体eventop,这个结构体前面也曾提到过,它的成员是一系列的函数指针, 定义在event-internal.h文件中: 12345678910struct eventop { const char *name; void *(*init)(struct event_base *); //初始化 int (*add)(void *, struct event *); //注册事件 int (*del)(void *, struct event *); //删除事件 int (*dispatch)(struct event_base *, void *, struct timeval *); //事件分发 void (*dealloc)(struct event_base *, void *); //注销，释放资源 /* set if we need to reinitialize the event base */ int need_reinit;}; 在 libevent 中,每种 I/O demultiplex 机制的实现都必须提供这五个函数接口,来完成自身的初始化、销毁释放;对事件的注册、注销和分发。 比如对于 epoll,libevent 实现了 5 个对应的接口函数,并在初始化时并将 eventop 的 5个函数指针指向这 5 个函数,那么程序就可以使用 epoll 作为 I/O demultiplex 机制了。 2、设置 I/O demultiplex 机制Libevent 把所有支持的 I/O demultiplex 机制存储在一个全局静态数组 eventops 中,并在初始化时选择使用何种机制,数组内容根据优先级顺序声明如下: 123456789101112131415161718192021222324static const struct eventop *eventops[] = {#ifdef HAVE_EVENT_PORTS &amp;evportops,#endif#ifdef HAVE_WORKING_KQUEUE &amp;kqops,#endif#ifdef HAVE_EPOLL &amp;epollops,#endif#ifdef HAVE_DEVPOLL &amp;devpollops,#endif#ifdef HAVE_POLL &amp;pollops,#endif#ifdef HAVE_SELECT &amp;selectops,#endif#ifdef WIN32 &amp;win32ops,#endif NULL}; 然后 libevent 根据系统配置和编译选项决定使用哪一种 I/O demultiplex 机制,这段代码在函数event_base_new()中: 1234567//初始化决定使用哪种I/O demultiplex机制 base-&gt;evbase = NULL; for (i = 0; eventops[i] &amp;&amp; !base-&gt;evbase; i++) { base-&gt;evsel = eventops[i]; //确定使用哪种I/O demultiplex机制 base-&gt;evbase = base-&gt;evsel-&gt;init(base); //实例化一个evsel，例如evsel为epoll，相当于调用epoll的init函数创建一个epoll实例 } 可以看出,libevent 在编译阶段选择系统的 I/O demultiplex 机制,而不支持在运行阶段根据配置再次选择。 以 Linux 下面的 epoll 为例,实现在源文件 epoll.c 中,eventops 对象 epollops 定义如下: 123456789const struct eventop epollops = { \"epoll\", epoll_init, epoll_add, epoll_del, epoll_dispatch, epoll_dealloc, 1 /* need reinit */}; 变量 epollops 中的函数指针具体声明如下,注意到其返回值和参数都和 eventop 中的定义严格一致,这是函数指针的语法限制。 12345static void *epoll_init (struct event_base *);static int epoll_add (void *, struct event *);static int epoll_del (void *, struct event *);static int epoll_dispatch (struct event_base *, void *, struct timeval *);static void epoll_dealloc (struct event_base *, void *); 那么如果选择的是 epoll,那么调用结构体 eventop 的 init 和 dispatch 函数指针时,实际调用的函数就是 epoll 的初始化函数 epoll_init()和事件分发函数 epoll_dispatch()了; 同样的,上面 epollops 以及 epoll 的各种函数都直接定义在了 epoll.c 源文件中,对外都是不可见的。对于 libevent 的使用者而言,完全不会知道它们的存在,对 epoll 的使用也是通过 eventop 来完成的,达到了信息隐藏的目的。 3、小结支持多种 I/O demultiplex 机制的方法其实挺简单的,借助于函数指针就 OK 了。通过对源代码的分析也可以看出,Libevent 是在编译阶段选择系统的 I/O demultiplex 机制的,而不支持在运行阶段根据配置再次择。","link":"/2019/05/10/libevent源码剖析-8-支持I-O多路复用/"},{"title":"libevent源码剖析(9)--时间管理","text":"为了支持定时器,Libevent 必须和系统时间打交道，这一部分的内容也比较简单，主要涉及到时间的加减辅助函数、时间缓存、时间校正和定时器堆的时间值调整等。下面就结合源代码来分析一下。 1、初始化检测Libevent 在初始化时会检测系统时间的类型，通过调用函数 detect_monotonic() 完成,它通过调用clock_gettime()来检测系统是否支持 monotonic 时钟类型: 12345678910static voiddetect_monotonic(void){#if defined(HAVE_CLOCK_GETTIME) &amp;&amp; defined(CLOCK_MONOTONIC) struct timespec ts; if (clock_gettime(CLOCK_MONOTONIC, &amp;ts) == 0) use_monotonic = 1; //系统支持，monotonic时间#endif} Monotonic 时间指示的是系统从 boot 后到现在所经过的时间,如果系统支持 Monotonic时间就将全局变量 use_monotonic 设置为 1,设置 use_monotonic 到底有什么用,这个在后面说到时间校正时就能看出来了。 2、时间缓存结构体 event_base 中的 tv_cache，用来记录时间缓存。这个还要从函数 gettime()说起，先来看看该函数的代码: 12345678910111213141516171819202122232425static intgettime(struct event_base *base, struct timeval *tp){ //如果tv_cache时间缓存已设置，就直接使用 if (base-&gt;tv_cache.tv_sec) { *tp = base-&gt;tv_cache; return (0); } //如果支持monotonic，就用clock_gettime获取monotonic时间#if defined(HAVE_CLOCK_GETTIME) &amp;&amp; defined(CLOCK_MONOTONIC) if (use_monotonic) { struct timespec ts; if (clock_gettime(CLOCK_MONOTONIC, &amp;ts) == -1) return (-1); tp-&gt;tv_sec = ts.tv_sec; tp-&gt;tv_usec = ts.tv_nsec / 1000; return (0); }#endif //否则只能取得系统当前时间 return (evutil_gettimeofday(tp, NULL));} 如果 tv_cache 已经设置，那么就直接使用缓存的时间；否则需要再次执行系统调用获取系统时间。 函数 evutil_gettimeofday()用来获取当前系统时间，在 Linux 下其实就是系统调用gettimeofday()；Windows 没有提供函数 gettimeofday，而是通过调用_ftime()来完成的。 在每次系统事件循环中，时间缓存 tv_cache 将会被相应的清空和设置，再次来看看下面 event_base_loop 的主要代码逻辑: 12345678910111213141516171819202122int event_base_loop(struct event_base *base, int flags){ // 清空时间缓存 base-&gt;tv_cache.tv_sec = 0; while(!done){ timeout_correct(base, &amp;tv); // 时间校正 // 更新event_tv到tv_cache指示的时间或者当前时间(第一次) // event_tv &lt;--- tv_cache gettime(base, &amp;base-&gt;event_tv); // 清空时间缓存-- 时间点1 base-&gt;tv_cache.tv_sec = 0; // 等待I/O事件就绪 res = evsel-&gt;dispatch(base, evbase, tv_p); // 缓存tv_cache存储了当前时间的值-- 时间点2 // tv_cache &lt;--- now gettime(base, &amp;base-&gt;tv_cache); // .. 处理就绪事件 } // 退出时也要清空时间缓存 base-&gt;tv_cache.tv_sec = 0; return (0);} 时间 event_tv 指示了 dispatch()上次返回，也就是 I/O 事件就绪时的时间，第一次进入循环时,由于 tv_cache 被清空，因此 gettime()执行系统调用获取当前系统时间；而后将会更新为 tv_cache 指示的时间。 时间 tv_cache 在 dispatch()返回后被设置为当前系统时间,因此它缓存了本次 I/O 事件就绪时的时间(event_tv)。 从代码逻辑里可以看出 event_tv 取得的是 tv_cache 上一次的值，因此 event_tv 应该小于tv_cache 的值。 设置时间缓存的优点是不必每次获取时间都执行系统调用，这是个相对费时的操作；在上面标注的时间点 2 到时间点 1 的这段时间(处理就绪事件时)，调用 gettime()取得的都是tv_cache 缓存的时间。 3、时间校正如果系统支持 monotonic 时间，该时间是系统从 boot 后到现在所经过的时间，因此不需要执行校正。 根据前面的代码逻辑，如果系统不支持 monotonic 时间，用户可能会手动的调整时间，如果时间被向前调整了(MS 前面第 7 部分讲成了向后调整,要改正)，比如从 5 点调整到了 3 点,那么在时间点 2 取得的值可能会小于上次的时间，这就需要调整了，下面来看看校正的具体代码，由函数 timeout_correct()完成： 123456789101112131415161718192021222324252627282930313233343536373839static voidtimeout_correct(struct event_base *base, struct timeval *tv){ struct event **pev; unsigned int size; struct timeval off; if (use_monotonic) //monotonic时间就直接返回，无需调整 return; /* Check if time is running backwards */ gettime(base, tv); //tv &lt;---tv_cache //如果tv &lt; event_tv表明用户向前调整时间了，需要校正时间 if (evutil_timercmp(tv, &amp;base-&gt;event_tv, &gt;=)) { base-&gt;event_tv = *tv; return; } event_debug((\"%s: time is running backwards, corrected\", __func__)); //计算时间差值 evutil_timersub(&amp;base-&gt;event_tv, tv, &amp;off); /* * We can modify the key element of the node without destroying * the key, beause we apply it to all in the right order. */ //调整定时事件小根堆 pev = base-&gt;timeheap.p; size = base-&gt;timeheap.n; for (; size-- &gt; 0; ++pev) { struct timeval *ev_tv = &amp;(**pev).ev_timeout; evutil_timersub(ev_tv, &amp;off, ev_tv); } /* Now remember what the new time turned out to be. */ base-&gt;event_tv = *tv; //更新event_tv为tv_cache} 在调整小根堆时，因为所有定时事件的时间值都会被减去相同的值，因此虽然堆中元素的时间键值改变了，但是相对关系并没有改变，不会改变堆的整体结构。因此只需要遍历堆中的所有元素，将每个元素的时间键值减去相同的值即可完成调整，不需要重新调整堆的结构。 当然调整完后，要将 event_tv 值重新设置为 tv_cache 值了。 4、小结主要分析了一下 libevent 对系统时间的处理，时间缓存、时间校正和定时堆的时间值调整等，逻辑还是很简单的,时间的加减、设置等辅助函数则非常简单，主要在头文件 evutil.h中，就不再多说了。","link":"/2019/05/12/libevent源码剖析-9-时间管理/"},{"title":"libevent源码剖析(2)--Reactor模式","text":"Reactor模式 整个 libevent 本身就是一个 Reactor，所以在剖析libevent源码之前需要对Reactor有一个深入的理解。 在事件驱动的应用中,同步地、有序地处理同时接收的多个服务请求。在分布式系统尤其是服务器这一类事件驱动应用中,虽然这些请求最终会被序列化地处理,但是必须时刻准备着处理多个同时到来的服务请求。在实际应用 中,这些请求总是通过一个事件(如CONNECTOR、READ、WRITE等)来表示的。在有序地处理这些服务请求之前,应用程序必须先分离和调度这些 同时到达的事件。 为了有效地解决这个问题,我们需要做到以下4方面:为了提高系统的可测量性和反应时间,应用程序不能长时间阻塞在某个事件源上而停止对其他事件的处理,这样会严重降低对客户端的响应度。为了提高吞吐量,任何没有必要的上下文切换、同步和CPU之间的数据移动都要避免。引进新的服务或改良已有的服务都要对既有的事件分离和调度机制带来尽可能小的影响。大量的应用程序代码需要隐藏在复杂的多线程和同步机制之后。 这就是Ractor模式要解决的问题。 1、Reactor事件处理机制首先来回想一下普通函数调用的机制:程序调用某函数函数执行，程序等待函数将结果和控制权返回给程序程序继续处理。 Reactor 释义“反应堆”,是一种事件驱动机制。和普通函数调用的不同之处在于:应用程序不是主动的调用某个 API 完成处理,而是恰恰相反,Reactor 逆置了事件处理流程,应用程序需要提供相应的接口并注册到 Reactor 上,如果相应的时间发生, Reactor 将主动调用应用程序注册的接口,这些接口又称为“回调函数”。使用 Libevent 也是想 Libevent 框架注册相应的事件和回调函数;当这些时间发声时,Libevent 会调用这些回调函数处理相应的事件(I/O 读写、定时和信号)。 用“好莱坞原则”来形容 Reactor 再合适不过了:不要打电话给我们,我们会打电话通知你。 举个例子:你去应聘某 xx 公司,面试结束后。 “普通函数调用机制”公司 HR 比较懒,不会记你的联系方式,那怎么办呢,你只能面试完后自己打电话去问结果;有没有被录取啊,还是被据了; “Reactor”公司 HR 就记下了你的联系方式,结果出来后会主动打电话通知你:有没有被录取啊,还是被据了;你不用自己打电话去问结果,事实上也不能,你没有HR的留联系方式。 2、Reactor模式的优点Reactor 模式是编写高性能网络服务器的必备技术之一,它具有如下的优点: 1)响应快,不必为单个同步时间所阻塞,虽然 Reactor 本身依然是同步的; 2)编程相对简单,可以最大程度的避免复杂的多线程及同步问题,并且避免了多线程/进程的切换开销; 3)可扩展性,可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源; 4)可复用性,reactor 框架本身与具体事件处理逻辑无关,具有很高的复用性; 3、Reactor模式框架 描述符(handle) 由操作系统提供,用于识别每一个事件,如Socket描述符、文件描述符等。在Linux中,它用一个整数来表示。事件可以来自外部,如来自客户端的连接请求、数据等。事件也可以来自内部,如定时器事件。 同步事件分离器(demultiplexer) 一个函数,用来等待一个或多个事件的发生。调用者会被阻塞,直到分离器分离的描述符集上有事件发生。Linux的select函数是一个经常被使用的分离器。 事件处理器接口(event handler) 由一个或多个模板函数组成的接口。这些模板函数描述了和应用程序相关的对某个事件的操作。 具体的事件处理器 事件处理器接口的实现。它实现了应用程序提供的某个服务。每个具体的事件处理器总和一个描述符相关。它使用描述符来识别事件、识别应用程序提供的服务。 Reactor 管理器(reactor) 定义了一些接口,用于应用程序控制事件调度,以及应用程序注册、删除事件处理器和相关的描述符。它是事件处理器的调度核心。 Reactor管理器使用同步事件分离器来等待事件的发生。一旦事件发生,Reactor管理器先是分离每个事件,然后调度事件处理器,最后调用相关的模 板函数来处理这个事件。 通过上述分析,我们注意到,是Reactor管理器而不是应用程序负责等待事件、分离事件和调度事件。实际上,Reactor管理器并没有被具体的事件处理器调用,而是管理器调度具体的事件处理器,由事件处理器对发生的事件做出处理。这就是类似Hollywood原则的“反向控制”。应用程序要做的 仅仅是实现一个具体的事件处理器,然后把它注册到Reactor管理器中。接下来的工作由管理器来完成。 这些参与者的相互关系如下图所示。 4、Reactor典型启动过程Reator模式的典型启动过程如下： (1)创建Reactor (2)注册事件处理器（Reactor::register_handler()） (3)调用事件多路分发器进入无限事件循环（Reacor:handle_events） (4)当操作系统通知某描述符状态就绪时，事件多路分发器找出并调用此描述符注册的事件处理器。","link":"/2019/05/07/libevent源码剖析-Reactor模式/"},{"title":"libevent源码剖析(1)--libevent简介及源码结构","text":"libevent简介及源码结构1、libevent简介libevent是一个C语言编写的轻量级的开源高性能网络库，其主要优点有如下几个： 事件驱动，高性能 轻量级，专注于网络 跨平台,支持 Windows、Linux、Mac Os等 支持多种 I/O多路复用技术, epoll、poll、dev/poll、select 和kqueue 等 支持 I/O,定时器和信号等事件 2、libevent源码结构Libevent 的源代码虽然都在一层文件夹下面,但是其代码分类还是相当清晰的,主要可分为头文件、内部使用的头文件、辅助功能函数、日志、libevent 框架、对系统 I/O 多路复用机制的封装、信号管理、定时事件管理、缓冲区管理、基本数据结构和基于 libevent 的两个实用库等几个部分,有些部分可能就是一个源文件。 1)头文件 主要就是 event.h:事件宏定义、接口函数声明,主要结构体 event 的声明; 2)内部头文件 xxx-internal.h:内部数据结构和函数,对外不可见,以达到信息隐藏的目的; 3)libevent 框架 event.c:event 整体框架的代码实现; 4)对系统 I/O 多路复用机制的封装 epoll.c:对 epoll 的封装;select.c:对 select 的封装;devpoll.c:对 dev/poll 的封装;kqueue.c:对 kqueue 的封装; 5)定时事件管理 min-heap.h:其实就是一个以时间作为 key 的小根堆结构; 6)信号管理 signal.c:对信号事件的处理; 7)辅助功能函数 evutil.h 和 evutil.c:一些辅助功能函数,包括创建 socket pair 和一些时间操作函数:加、减和比较等。 8)日志 log.h 和 log.c:log 日志函数 9)缓冲区管理 evbuffer.c 和 buffer.c:libevent 对缓冲区的封装; 10)基本数据结构 compat\\sys 下的两个源文件: queue.h 是 libevent 基本数据结构的实现,包括链表,双向链表,队列等;_libevent_time.h:一些用于时间操作的结构体定义、函数和宏定义; 11)实用网络库 http 和 evdns:是基于 libevent 实现的 http 服务器和异步 dns 查询库","link":"/2019/05/07/libevent源码剖析-libevent简介/"},{"title":"多态原理探究","text":"多态原理探究1、多态的实现原理 当类中声明虚函数时，编译器会在类中生成一个虚函数表 虚函数表是一个存储类成员函数指针的数据结构 虚函数表是由编译器自动生成与维护的 virtual成员函数会被编译器放入虚函数表中 存在虚函数时，每个对象中都有一个指向虚函数表的指针(vptr指针) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Parent{public: Parent(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } virtual void print(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"Parent..\"&lt;&lt;endl; }private: int a;};class Child:public Parent{public: Child(int a=0,int b=0):Parent(a),b(b){ cout&lt;&lt;\"儿子的构造函数\"&lt;&lt;endl; } virtual void print(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"Child..\"&lt;&lt;endl; }private: int b;};//证明vptr指针的存在class Parent2{public: Parent2(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } void print1(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"我是爹。。。\"&lt;&lt;endl; }private: int a;};class Parent3{public: Parent3(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } virtual void print2(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 // cout&lt;&lt;\"Child..\"&lt;&lt;endl; }private: int a;};void Objplay(Parent *base){ base-&gt;print(); //有多态发生 //动手脚2 //效果：传来子类时 执行子类的print函数 传来父类时执行父类的print函数 //C++编译器根本不需要区分是子类对象还是父类对象 //父类对象和子类对象分别有vptr指针，==&gt;虚函数表===&gt;函数的入口地址 //迟绑定（运行的时候，C++编译器才去判断)}int main(){ Parent p1; //动手脚3 用类定义对象的时候 C++编译其会在对象中添加一个vptr指针 Child c1; //子类里面也有一个vptr指针 Objplay(&amp;p1); Objplay(&amp;c1); //证明vptr指针的存在 cout&lt;&lt;\"证明vptr指针的存在\"&lt;&lt;endl; cout&lt;&lt;endl; cout&lt;&lt;\"sizeof(Parent2): \"&lt;&lt;sizeof(Parent2)&lt;&lt;endl; cout&lt;&lt;\"sizeof(Parent3): \"&lt;&lt;sizeof(Parent3)&lt;&lt;endl; std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;}/* 说明1： 通过虚函数表指针VPTR调用重写函数是在程序运行时进行的，因此需要通过寻址操作才能确定真正应该调用的函数。 而普通成员函数是在编译时就确定了调用的函数。在效率上，虚函数的效率要低很多。 说明2： 出于效率考虑，没必要将所有成员函数都声明为虚函数*/ 2、vptr指针的分步初始化vptr指针的初始化是分步的，当对象在创建的时候，由编译器对vptr指针进行初始化。只有当对象的构造完全结束后vptr的指向才最终确定。父类对象的vptr指向父类虚函数表，子类对象的vptr指向子类虚函数表。有如下代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std;class Parent{public: Parent(int a=0):a(a){ cout&lt;&lt;\"Parent..\"&lt;&lt;endl; print();//是调用父类print函数 } virtual void print(){ cout&lt;&lt;\"Parent::print()\"&lt;&lt;endl; }private: int a;};class Child:public Parent{public: Child(int a=0,int b=0):Parent(a),b(b){ cout&lt;&lt;\"Child..\"&lt;&lt;endl; } virtual void print(){ cout&lt;&lt;\"Child::print()\"&lt;&lt;endl; }private: int b;};int main(){ /* 1、要初始化c1.vptr，初始化是分步 2、当执行父类的构造函数时，c1.vptr指向父类的虚函数表 当父类的构造函数运行完毕后，会指向子类的虚函数表 3、结论：子类的c1.vptr指针分步完成 ===&gt;子类对象构造时，在父类的构造函数调用虚函数，产生不了多态。 在子类的父类构造期间，对象的类型是父类而不是子类 */ Child c1; std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;} 在上述代码中，定义c1对象时，执行的是父类的print函数，这是因为在子类对象构造时，要先调用父类的构造函数，此时c1.vptr指向父类的虚函数表，所以在父类的构造函数中调用print函数执行的是父类的print函数，并不会产生多态。因此， 不要在构造函数和析构函数中调用虚函数 。","link":"/2019/03/18/多态原理探究/"},{"title":"重载与重写","text":"重载与重写之前一直没考虑过重载与重写这两个概念，脑子了只有重载这个概念。 1、函数重载： （1）必须在同一类中进行； （2）子类无法继承父类的函数，父类同名函数将被名称覆盖； （3）重载是在编译期间根据参数类型和个数决定函数调用。 2、函数重写 （1）必须发生与父类与子类之间； （2）父类与子类中的函数必须有完全相同的原型； （3）使用virtual声明后能够产生多态（如果不使用virtual，那叫重定义），多态是在运行期间根据具体对象的类型决定函数的调用。 下面是示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566 #include &lt;iostream&gt;using namespace std;class Parent{public: //同一类中为函数重载，这几个函数为重载 virtual void fun(){ cout&lt;&lt;\"fun of Parent\"&lt;&lt;endl; } virtual void fun(int i){ cout&lt;&lt;\"fun2 \"&lt;&lt;i&lt;&lt;endl; } virtual void abc(){ cout&lt;&lt;\"abc\"&lt;&lt;endl; }protected:private:};class Child:public Parent{public: void abc(){ cout&lt;&lt;\"abc\"&lt;&lt;endl; } void abc(int i){ cout&lt;&lt;i&lt;&lt;endl;}//如果没有下面的代码，那么可以通过子类对象调用父类函数，c1.fun();//如果有下面代码，子类无法重载父类函数，父类同名函数将被覆盖，所以c1.fun()无法调用//如果和父类原型相同，那么可以是重写。 void fun(int i,int j){ cout&lt;&lt;\"fun1 of Child \"&lt;&lt;i&lt;&lt;\" \"&lt;&lt;j&lt;&lt;endl; } void fun(int i,int j,int n){ cout&lt;&lt;\"fun2 of Child\"&lt;&lt;endl; }};int main(){ Child c1; c1.Parent::fun(); /* 1、c++编译器 看到fun名字，在子类中已经存在了（名称覆盖），所以C++编译器不会去找父类的4个参 数的func函数 2、c++编译器只会在子类中查找fun函数，找到了两个fun，一个是2个参数的，一个是3个参数的 3、c++编译器开始报错。。。 4、若想调用，只能加作用域运算符 */ c1.fun(1);//报错，没有匹配的函数，无法调用父类的fun(int i); //c1.fun(); //fun函数的名字，在子类中发生了名称覆盖，子类的函数的名字，占用了父类的函数的名字的位置 //因为子类中已经有了fun函数重载形式。。。。 //编译器开始在子类中找fun函数。。。但是没有0个参数的fun函数 std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;}","link":"/2019/03/17/重载与重写/"}],"tags":[{"name":"网络编程","slug":"网络编程","link":"/tags/网络编程/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"LInux","slug":"LInux","link":"/categories/LInux/"},{"name":"C++心得","slug":"C-心得","link":"/categories/C-心得/"}]}