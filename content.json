{"pages":[{"title":"xinde","text":"","link":"/xinde/index.html"}],"posts":[{"title":"I/O之同步、异步、阻塞、非阻塞","text":"今天在看到 Libevent 的核心是事件驱动、同步非阻塞 这句话时，突然产生了疑惑，libevent是同步的吗？，一直以为是异步的，然后开始搜索资料，发现众说纷纭，有人说是异步，有人说是同步。答案是同步。。 之所以产生疑惑，根本原因是自己对同步、异步以及阻塞、非阻塞理解的不够透彻。 通过查找网上的资料发现，关于同步/异步，阻塞/非阻塞的解释有各种各样的版本，有各种各样的理解，搞得自己也有点懵，所以还是以比较权威的 W.Richard Stevens的UNP 6.2 I/O模型 来理解。 书中介绍了在Unix下可用的5中I/O模型，分别是： 阻塞式I/O 非阻塞式I/O I/O复用（select和poll） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 一个输入操作通常包括两个不同的阶段： (1)等待数据准备好 (2)从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 阻塞式I/O模型默认情形下，所有套接字都是阻塞的。。以数据报套接字作为例子，如下图所示： 在上图中，进程调用recvfrom，其系统调用直到数据报到达且被复制到应用进程的缓冲区中或者发生错误才返回。最常见的错误是系统调用被信号中断。这里，进程在从调用recvfrom开始到它返回的整段时间内是被阻塞的。recvfrom成功返回后，应用进程开始处理数据报。 非阻塞I/O模型套接字的默认状态是阻塞的。这就意味着当发出一个不能立即完成的套接字调用时，其进程将被投入睡眠，等待相应操作完成。进程把一个套接字设置称非阻塞是在通知内核：当所请求的I/O操作非得把本进程投入睡眠才能完成时，而是返回一个错误。 看下图的例子： 前三次调用recvfrom时没有数据可返回，因此内核转而立即返回一个EWOULDBLOCK错误。第四次调用recvfrom时已有一个数据报准备好，它被复制到应用进程缓冲区，于是recvfrom成功返回。接着处理数据。 当一个应用进程像这样一个非阻塞描述符循环调用recvfrom时，我们称之为轮询(polling)。应用进程持续轮询内核，以查看某个操作是否就绪。这么做往往消耗大量CPU时间，不过这种模型偶尔也会遇到，通常是在专门提供某一种功能的系统中才有。 I/O复用模型有了I/O复用模型，我们就可以调用select或poll，阻塞在这两个系统调用中的某一个之上，而不是阻塞在真正的I/O系统调用上。下图概括展示了I/O复用模型。 阻塞于select调用，等待数据报套接字变为可读。当select返回套接字可读这一条件时，我们调用recvfrom把所读数据报复制到进程缓冲区。 比较图6-3和6-1，I/O复用并不显得有说明优势，事实上由于使用select需要两个而不是单个系统调用，I/O复用还稍显劣势。但是，使用select的优势在于可以等待多个描述符就绪。 信号驱动式I/O模型此外，还可以用信号，让内核在描述符就绪时发送SIGIO信号通知我们。我们称这种模型为信号驱动式I/O模型，下图是它的概要展示。 我们首先开启套接字的信号驱动式I/O工呢，并通过sigaction系统调用安装一个信号处理函数。该系统调用立即返回，我们进程继续工作，也就是说它没有被阻塞。当数据报准备好读取时，内核九尾该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它读书数据报。 无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达期间进程不被阻塞。主循环可以继续执行，只要等待来自信号处理函数的通知：既可以是数据已准备好被处理，也可以是数据已准备好被读取。 异步I/O模型工作机制是：告知内核启动某个操作，并让内核在整个操作（包括将数据从内核复制到我们自己的缓冲区）完成后通知我们。这种模型与信号驱动模型的主要区别在于：信号驱动式I/O是由内核通知我们何时可以启动一个I/O操作，而异步I/O模型是由内核通知我们I/O操作何时完成 。 看下图例子。 我们调用aio_read函数，给内核传递描述符、缓冲区指针、缓冲区大小和文件偏移，并告诉内核当整个操作完成时何时通知我们。系统调用立即返回，而且在等待I/O完成期间，我们进程不被阻塞。本例中我们假设要求内核在操作完成时产生某个信号。该信号直到数据已复制到应用进程缓冲区才产生，这一点不同于信号驱动式I/O模型。 各种I/O模型的比较下图对比了上述五种不同的I/O模型。 可以看出，前4种模型的主要区别在于第一阶段，因为它们的第二阶段是一样的：在数据从内核复制到调用者的缓冲区期间，进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理，进程不需要阻塞，从而不同于其他4种模型。 同步I/O和异步I/O对比POSIX 把这两个术语定义如下： 同步I/O操作：导致请求进程阻塞，直到I/O操作完成 异步I/O操作：不导致请求进程阻塞 所以，根据上述定义，前4种模型：阻塞式I/O模型、非阻塞I/O模型、I/O复用模型、信号驱动式I/O模型都是同步I/O模型，因为其中真正的I/O操作（recvfrom）将进程阻塞。只有异步I/O模型与POSIX定义的异步I/O相匹配。 此外，在知乎上，陈硕大佬给出了这样的解答 所以，最后我理解的是 同步/异步的区别主要在于第二阶段，即将数据从内核复制到用户空间这一阶段。同步需要主动去读写数据，在读写的系统调用过程中，是阻塞的。而异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写，在这个过程中进程不阻塞。 阻塞/非阻塞，如果是阻塞，进程在发起系统调用后，在调用返回之前，进程会被挂起，等待返回结果；如果是非阻塞，调用会立即返回一个状态值，进程不需要挂起，不需要等待，这就是非阻塞，如果进程时不时去查看是否有结果，就是轮询。 在清楚了同步、异步、阻塞、非阻塞的概念和区别后，再来看Reactor模型，其读操作的具体步骤如下： 应用程序注册读就绪事件和相关联的事件处理器 事件分离器等待事件的发生 当发生读就绪事件的时候，事件分离器调用第一步注册的事件处理器 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理 当读就绪事件发生时，事件处理器会主动执行读取操作，所以Reactor是同步的。 此外还有一种异步I/O模型，就是Proactor模型，其读取操作步骤如下： 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注读取就绪事件，而是关注读取完成事件，这是区别于Reactor的关键。 事件分离器等待读取操作完成事件 在事件分离器等待读取操作完成的时候，操作系统调用内核线程完成读取操作（异步IO都是操作系统负责将数据读写到应用传递进来的缓冲区供应用程序操作，操作系统扮演了重要角色），并将读取的内容放入用户传递过来的缓存区中。这也是区别于Reactor的一点，Proactor中，应用程序需要传递缓存区。 事件分离器捕获到读取完成事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。 可以清楚地看到两者的区别，Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的，Reactor中需要应用程序自己读取或者写入数据，而Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，操作系统会读取缓存区或者写入缓存区到真正的IO设备。 最后Libevent的核心就是Reactor模型，再看libevent的底层代码，实际仍然是一个while循环，调用epoll或者select去监听event是否准备就绪，事件准备就绪后，事件处理器还是会主动去执行需要的操作，例如读事件，是要主动去读取数据，所以，这样看libevent是同步的。","link":"/2019/05/10/I-O之同步、异步、阻塞、非阻塞/"},{"title":"epoll反应堆模型","text":"epoll反应堆模型epoll是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率，因为它会复用文件描述符集合来传递结果而不用迫使开发者每次等待事件之前都必须重新准备要被侦听的文件描述符集合，另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。epoll除了提供select/poll那种IO事件的电平触发（Level Triggered）外，还提供了边沿触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。 1、epoll函数原型（1）创建一个epoll句柄，参数size用来告诉内核监听的文件描述符的个数，跟内存大小有关。返回代表新创建的epoll实例的文件描述符，其实创建的为一颗红黑树，返回根节点。12#include &lt;sys/epoll.h&gt;int epoll_create(int size) size：监听数目 （2）控制某个epoll监控的文件描述符上的事件：注册、修改、删除。 123456789101112131415161718192021222324252627#include &lt;sys/epoll.h&gt; int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) epfd： 为epoll_creat的句柄 op： 表示动作，用3个宏来表示： EPOLL_CTL_ADD (注册新的fd到epfd，给红黑树加入新节点)， EPOLL_CTL_MOD (修改已经注册的fd的监听事件)， EPOLL_CTL_DEL (从epfd删除一个fd，删除红黑树节点)； event： 告诉内核需要监听的事件 struct epoll_event { __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; EPOLLIN ： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭） EPOLLOUT： 表示对应的文件描述符可以写 EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来） EPOLLERR： 表示对应的文件描述符发生错误 EPOLLHUP： 表示对应的文件描述符被挂断； EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)而言的 EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 （3）等待所监控文件描述符上有事件的产生，类似于select()调用。123456789#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout) events： 用来存内核得到事件的集合， maxevents： 告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size， timeout： 是超时时间 -1： 阻塞 0： 立即返回，非阻塞 &gt;0： 指定毫秒 返回值： 成功返回有多少文件描述符就绪，时间到时返回0，出错返回-1 epoll的设计： （1）epoll在Linux内核中构建了一个文件系统，该文件系统采用红黑树来构建，红黑树在增加和删除上面的效率极高，因此是epoll高效的原因之一。有兴趣可以百度红黑树了解，但在这里你只需知道其算法效率超高即可。 （2）epoll红黑树上采用事件异步唤醒，内核监听I/O，事件发生后内核搜索红黑树并将对应节点数据放入异步唤醒的事件队列中。 （3）epoll的数据从用户空间到内核空间采用mmap存储I/O映射来加速。该方法是目前Linux进程间通信中传递最快,消耗最小,传递数据过程不涉及系统调用的方法。 epoll接口相对于传统的select/poll而言，有以下优点： （1）支持单个进程打开大数量的文件描述符。受进程最大打开的文件描述符数量限制，而不是受自身实现限制。而select单个进程能够打开的文件描述符的数量存在最大限制，这个限制是select自身实现的限制。通常是1024。poll采用链表，也是远超select的。 （2）Linux的I/O效率不会随着文件描述符数量的增加而线性下降。较之于select/poll，当处于一个高并发时(例如10万，100万)。在如此庞大的socket集合中，任一时间里其实只有部分的socket是“活跃”的。select/poll的处理方式是，对用如此庞大的集合进行线性扫描并对有事件发生的socket进行处理，这将极大的浪费CPU资源。因此epoll的改进是，由于I/O事件发生，内核将活跃的socket放入队列并交给mmap加速到用户空间，程序拿到的集合是处于活跃的socket集合，而不是所有socket集合。 （3）使用mmap加速内核与用户空间的消息传递。select/poll采用的方式是，将所有要监听的文件描述符集合拷贝到内核空间（用户态到内核态切换）。接着内核对集合进行轮询检测，当有事件发生时，内核从中集合并将集合复制到用户空间。 再看看epoll怎么做的，内核与程序共用一块内存，用户与mmap加速区进行数据交互不涉及权限的切换(用户态到内核态，内核态到用户态)。内核对于处于非内核空间的内存有权限进行读取。 作者：青城山小和尚 来源：CSDN 原文：https://blog.csdn.net/qq_36359022/article/details/81355897 2、边沿触发（ET）和水平触发（LT） （1）水平触发, 此方式为默认情况。当设置了水平触发以后，以可读事件为例，当有数据到来并且数据在缓冲区待读。即使我这一次没有读取完数据，只要缓冲区里还有数据就会触发第二次，直到缓冲区里没数据。 （2）epoll边沿触发，当设置了边沿触发以后，以可读事件为例，对“有数据到来”这件事为触发。 为什么说边沿触发(ET) 的效率更高呢？ (1) 边沿触发只在数据到来的一刻才触发，很多时候服务器在接受大量数据时会先接受数据头部(水平触发在此触发第一次，边沿触发第一次)。 (2) 接着服务器通过解析头部决定要不要接这个数据。此时，如果不接受数据，水平触发需要手动清除，而边沿触发可以将清除工作交给一个定时的清除程序去做，自己立刻返回。 (3) 如果接受，两种方式都可以用while接收完整数据。 3、epoll非阻塞I/O使用的读取数据函数为read函数，在默认情况下此类函数是阻塞式的，在没有数据时会一直阻塞等待数据到来。 （1）数据到来100B，在epoll模式下调用read时，即使read()是阻塞式的也不会在这里等待，因为既然运行到read()，说明数据缓冲区已经有数据，因此这处无影响。 （2）在服务器开发中，一般不会直接用采用类似read()函数这一类系统调用(只有内核缓冲区)，会使用封装好的一些库函数(有内核缓冲区+用户缓冲区)或者自己封装的函数。 例如：使用readn()函数，设置读取200B返回，假设数据到来100B,可读事件触发，而程序要使用readn()读200B，那么此时如果是阻塞式的，将在此处形成死锁 流程是：100B ⇒ 触发可读事件 ⇒ readn()调用 ⇒ readn()都不够200B,阻塞 ⇒ cfd又到来200B ⇒ 此时程序在readn()处暂停，没有机会调用epoll_wait() ⇒ 完成死锁 解决：将该cfd在上树前设置为非阻塞式 4、epoll反应堆模型（Libevent核心思想）1) epoll — 服务器 — 监听 — cfd —- 可读 —- epoll返回 —- read – cfd从树上摘下 — 设置监听cfd写事件， 操作 — 小写转大写 – 等待epoll_wait 返回 — 回写客户端 – cfd从树上摘下 —– 设置监听cfd读事件， 操作 – epoll继续监听。关键为回调函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280/* *epoll基于非阻塞I/O事件驱动 */#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#define MAX_EVENTS 1024 //监听上限数#define BUFLEN 4096#define SERV_PORT 8080void recvdata(int fd, int events, void *arg);void senddata(int fd, int events, void *arg);/* 描述就绪文件描述符相关信息 */struct myevent_s { int fd; //要监听的文件描述符 int events; //对应的监听事件 void *arg; //泛型参数 void (*call_back)(int fd, int events, void *arg); //回调函数 int status; //是否在监听:1-&gt;在红黑树上(监听), 0-&gt;不在(不监听) char buf[BUFLEN]; int len; long last_active; //记录每次加入红黑树 g_efd 的时间值};int g_efd; //全局变量, 保存epoll_create返回的文件描述符struct myevent_s g_events[MAX_EVENTS+1]; //自定义结构体类型数组. +1--&gt;listen fd/*将结构体 myevent_s 成员变量 初始化*/void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg){ ev-&gt;fd = fd; ev-&gt;call_back = call_back; ev-&gt;events = 0; ev-&gt;arg = arg; ev-&gt;status = 0; //memset(ev-&gt;buf, 0, sizeof(ev-&gt;buf)); //ev-&gt;len = 0; ev-&gt;last_active = time(NULL); //调用eventset函数的时间 return;}/* 向 epoll监听的红黑树 添加一个 文件描述符 */void eventadd(int efd, int events, struct myevent_s *ev){ struct epoll_event epv = {0, {0}}; int op; epv.data.ptr = ev; epv.events = ev-&gt;events = events; //EPOLLIN 或 EPOLLOUT if (ev-&gt;status == 1) { //已经在红黑树 g_efd 里 op = EPOLL_CTL_MOD; //修改其属性 } else { //不在红黑树里 op = EPOLL_CTL_ADD; //将其加入红黑树 g_efd, 并将status置1 ev-&gt;status = 1; } if (epoll_ctl(efd, op, ev-&gt;fd, &amp;epv) &lt; 0) //实际添加/修改 printf(\"event add failed [fd=%d], events[%d]\\n\", ev-&gt;fd, events); else printf(\"event add OK [fd=%d], op=%d, events[%0X]\\n\", ev-&gt;fd, op, events); return ;}/* 从epoll 监听的 红黑树中删除一个 文件描述符*/void eventdel(int efd, struct myevent_s *ev){ struct epoll_event epv = {0, {0}}; if (ev-&gt;status != 1) //不在红黑树上 return ; epv.data.ptr = ev; ev-&gt;status = 0; //修改状态 epoll_ctl(efd, EPOLL_CTL_DEL, ev-&gt;fd, &amp;epv); //从红黑树 efd 上将 ev-&gt;fd 摘除 return ;}/* 当有文件描述符就绪, epoll返回, 调用该函数 与客户端建立链接 */void acceptconn(int lfd, int events, void *arg){ struct sockaddr_in cin; socklen_t len = sizeof(cin); int cfd, i; if ((cfd = accept(lfd, (struct sockaddr *)&amp;cin, &amp;len)) == -1) { if (errno != EAGAIN &amp;&amp; errno != EINTR) { /* 暂时不做出错处理 */ } printf(\"%s: accept, %s\\n\", __func__, strerror(errno)); return ; } do { for (i = 0; i &lt; MAX_EVENTS; i++) //从全局数组g_events中找一个空闲元素 if (g_events[i].status == 0) //类似于select中找值为-1的元素 break; //跳出 for if (i == MAX_EVENTS) { printf(\"%s: max connect limit[%d]\\n\", __func__, MAX_EVENTS); break; //跳出do while(0) 不执行后续代码 } int flag = 0; if ((flag = fcntl(cfd, F_SETFL, O_NONBLOCK)) &lt; 0) { //将cfd也设置为非阻塞 printf(\"%s: fcntl nonblocking failed, %s\\n\", __func__, strerror(errno)); break; } /* 给cfd设置一个 myevent_s 结构体, 回调函数 设置为 recvdata */ eventset(&amp;g_events[i], cfd, recvdata, &amp;g_events[i]); eventadd(g_efd, EPOLLIN, &amp;g_events[i]); //将cfd添加到红黑树g_efd中,监听读事件 } while(0); printf(\"new connect [%s:%d][time:%ld], pos[%d]\\n\", inet_ntoa(cin.sin_addr), ntohs(cin.sin_port), g_events[i].last_active, i); return ;}void recvdata(int fd, int events, void *arg){ struct myevent_s *ev = (struct myevent_s *)arg; int len; len = recv(fd, ev-&gt;buf, sizeof(ev-&gt;buf), 0); //读文件描述符, 数据存入myevent_s成员buf中 eventdel(g_efd, ev); //将该节点从红黑树上摘除 if (len &gt; 0) { ev-&gt;len = len; ev-&gt;buf[len] = '\\0'; //手动添加字符串结束标记 printf(\"C[%d]:%s\\n\", fd, ev-&gt;buf); eventset(ev, fd, senddata, ev); //设置该 fd 对应的回调函数为 senddata eventadd(g_efd, EPOLLOUT, ev); //将fd加入红黑树g_efd中,监听其写事件 } else if (len == 0) { close(ev-&gt;fd); /* ev-g_events 地址相减得到偏移元素位置 */ printf(\"[fd=%d] pos[%ld], closed\\n\", fd, ev-g_events); } else { close(ev-&gt;fd); printf(\"recv[fd=%d] error[%d]:%s\\n\", fd, errno, strerror(errno)); } return;}void senddata(int fd, int events, void *arg){ struct myevent_s *ev = (struct myevent_s *)arg; int len; len = send(fd, ev-&gt;buf, ev-&gt;len, 0); //直接将数据 回写给客户端。未作处理 /* printf(\"fd=%d\\tev-&gt;buf=%s\\ttev-&gt;len=%d\\n\", fd, ev-&gt;buf, ev-&gt;len); printf(\"send len = %d\\n\", len); */ if (len &gt; 0) { printf(\"send[fd=%d], [%d]%s\\n\", fd, len, ev-&gt;buf); eventdel(g_efd, ev); //从红黑树g_efd中移除 eventset(ev, fd, recvdata, ev); //将该fd的 回调函数改为 recvdata eventadd(g_efd, EPOLLIN, ev); //从新添加到红黑树上， 设为监听读事件 } else { close(ev-&gt;fd); //关闭链接 eventdel(g_efd, ev); //从红黑树g_efd中移除 printf(\"send[fd=%d] error %s\\n\", fd, strerror(errno)); } return ;}/*创建 socket, 初始化lfd */void initlistensocket(int efd, short port){ int lfd = socket(AF_INET, SOCK_STREAM, 0); fcntl(lfd, F_SETFL, O_NONBLOCK); //将socket设为非阻塞 /* void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg); */ eventset(&amp;g_events[MAX_EVENTS], lfd, acceptconn, &amp;g_events[MAX_EVENTS]); /* void eventadd(int efd, int events, struct myevent_s *ev) */ eventadd(efd, EPOLLIN, &amp;g_events[MAX_EVENTS]); struct sockaddr_in sin; memset(&amp;sin, 0, sizeof(sin)); //bzero(&amp;sin, sizeof(sin)) sin.sin_family = AF_INET; sin.sin_addr.s_addr = INADDR_ANY; sin.sin_port = htons(port); bind(lfd, (struct sockaddr *)&amp;sin, sizeof(sin)); listen(lfd, 20); return ;}int main(int argc, char *argv[]){ unsigned short port = SERV_PORT; if (argc == 2) port = atoi(argv[1]); //使用用户指定端口.如未指定,用默认端口 g_efd = epoll_create(MAX_EVENTS+1); //创建红黑树,返回给全局 g_efd if (g_efd &lt;= 0) printf(\"create efd in %s err %s\\n\", __func__, strerror(errno)); initlistensocket(g_efd, port); //初始化监听socket struct epoll_event events[MAX_EVENTS+1]; //保存已经满足就绪事件的文件描述符数组 printf(\"server running:port[%d]\\n\", port); int checkpos = 0, i; while (1) { /* 超时验证，每次测试100个链接，不测试listenfd 当客户端60秒内没有和服务器通信，则关闭此客户端链接 */ long now = time(NULL); //当前时间 for (i = 0; i &lt; 100; i++, checkpos++) { //一次循环检测100个。 使用checkpos控制检测对象 if (checkpos == MAX_EVENTS) checkpos = 0; if (g_events[checkpos].status != 1) //不在红黑树 g_efd 上 continue; long duration = now - g_events[checkpos].last_active; //客户端不活跃的世间 if (duration &gt;= 60) { close(g_events[checkpos].fd); //关闭与该客户端链接 printf(\"[fd=%d] timeout\\n\", g_events[checkpos].fd); eventdel(g_efd, &amp;g_events[checkpos]); //将该客户端 从红黑树 g_efd移除 } } /*监听红黑树g_efd, 将满足的事件的文件描述符加至events数组中, 1秒没有事件满足, 返回 0*/ int nfd = epoll_wait(g_efd, events, MAX_EVENTS+1, 1000); if (nfd &lt; 0) { printf(\"epoll_wait error, exit\\n\"); break; } for (i = 0; i &lt; nfd; i++) { /*使用自定义结构体myevent_s类型指针, 接收 联合体data的void *ptr成员*/ struct myevent_s *ev = (struct myevent_s *)events[i].data.ptr; if ((events[i].events &amp; EPOLLIN) &amp;&amp; (ev-&gt;events &amp; EPOLLIN)) { //读就绪事件 ev-&gt;call_back(ev-&gt;fd, events[i].events, ev-&gt;arg); } if ((events[i].events &amp; EPOLLOUT) &amp;&amp; (ev-&gt;events &amp; EPOLLOUT)) { //写就绪事件 ev-&gt;call_back(ev-&gt;fd, events[i].events, ev-&gt;arg); } } } /* 退出前释放所有资源 */ return 0;}","link":"/2019/04/26/epoll反应堆模型/"},{"title":"libevent源码剖析(10)--让libevent支持多线程","text":"Libevent 本身不是多线程安全的，在多核的时代，如何能充分利用 CPU 的能力呢，这一节来说说如何在多线程环境中使用 libevent，跟源代码并没有太大的关系，纯粹是使用上的技巧。 1、错误使用示例在多核的 CPU 上只使用一个线程始终是对不起 CPU 的处理能力啊，那好吧，那就多创建几个线程，比如下面的简单服务器场景。 1 主线程创建工作线程1； 2 接着主线程监听在端口上,等待新的连接； 3 在线程 1 中执行 event 事件循环,等待事件到来; 4 新连接到来,主线程调用 libevent 接口 event_add 将新连接注册到 libevent 上; … … 上面的逻辑看起来没什么错误,在很多服务器设计中都可能用到主线程和工作线程的模式…. 可是就在线程 1 注册事件时,主线程很可能也在操作事件,比如删除,修改,通过 libevent的源代码也能看到,没有同步保护机制,问题麻烦了,看起来不能这样做啊,难道只能使用单线程不成!? 2、支持多线程的几种模式Libevent 并不是线程安全的,但这不代表 libevent 不支持多线程模式,其实方法在前面已经将 signal 事件处理时就接触到了,那就是消息通知机制。 一句话，“你发消息通知我,然后再由我在合适的时间来处理”； 说到这就再多说几句，再打个比方，把你自己比作一个工作线程，而你的头是主线程，你有一个消息信箱来接收别人发给你的消息，当时头有个新任务要指派给你。 2.1暴力抢占那么第一节中使用的多线程方法相当下面的流程: 1 当时你正在做事,比如在写文档; 2 你的头找到了一个任务,要指派给你,比如帮他搞个 PPT,哈; 3 头命令你马上搞 PPT,你这是不得不停止手头的工作,把 PPT 搞定了再接着写文档; … 2.2纯粹的消息通知机制那么基于纯粹的消息通知机制的多线程方式就像下面这样: 1 当时你正在写文档; 2 你的头找到了一个任务,要指派给你,帮他搞个 PPT; 3 头发个消息到你信箱,有个 PPT 要帮他搞定,这时你并不鸟他; 4 你写好文档,接着检查消息发现头有个 PPT 要你搞定,你开始搞 PPT; … 第一种的好处是消息可以立即得到处理,但是很方法很粗暴,你必须立即处理这个消息,所以你必须处理好切换问题,省得把文档上的内容不小心写到 PPT 里。在操作系统的进程通信中,消息队列(消息信箱)都是操作系统维护的,你不必关心。 第二种的优点是通过消息通知,切换问题省心了,不过消息是不能立即处理的(基于消息通知机制,这个总是难免的),而且所有的内容都通过消息发送,比如 PPT 的格式、内容等等信息,这无疑增加了通信开销。 2.3消息通知+同步有个折中机制可以减少消息通信的开销,就是提取一个同步层,还拿上面的例子来说,你把工作安排都存放在一个工作队列中,而且你能够保证“任何人把新任务扔到这个队列”,“自己取出当前第一个任务”等这些操作都能够保证不会把队列搞乱(其实就是个加锁的队列容器)。 再来看看处理过程和上面有什么不同: 1 当时你正在写文档; 2 你的头找到了一个任务,要指派给你,帮他搞个 PPT; 2 头有个 PPT 要你搞定,他把任务 push 到你的工作队列中,包括了 PPT 的格式、内容等信息; 3 头发个消息(一个字节)到你信箱,有个 PPT 要帮他搞定,这时你并不鸟他; 4 你写好文档,发现有新消息(这预示着有新任务来了),检查工作队列知道头有个 PPT要你搞定,你开始搞 PPT; … 工作队列其实就是一个加锁的容器(队列、链表等等),这个很容易实现实现;而消息通知仅需要一个字节,具体的任务都 push 到了在工作队列中,因此相比 2.2 减少了不少通信开销。 多线程编程有很多陷阱,线程间资源的同步互斥不是一两句能说得清的,而且出现 bug很难跟踪调试;这也有很多的经验和教训,因此如果让我选择,在绝大多数情况下都会选择机制 3 作为实现多线程的方法。 3、例子——memcachedMemcached 中的网络部分就是基于 libevent 完成的,其中的多线程模型就是典型的消息通知+同步层机制。下面的图足够说明其多线程模型了,其中有详细的文字说明。 注:该图的具体出处忘记了,感谢原作者。 4、小结本节更是 libevent 的使用方面的技巧,讨论了一下如何让 libevent 支持多线程,以及几种支持多线程的机制,memcached 使用 libevent 的多线程模型。","link":"/2019/05/12/libevent源码剖析-10-让libevent支持多线程/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/05/19/hello-world/"},{"title":"libevent源码剖析(3)--核心event","text":"对事件处理流程有了高层的认识后,本节将详细介绍 libevent 的核心结构 event,以及libevent 对 event 的管理。 1、libevent的核心-eventLibevent 是基于事件驱动(event-driven)的,从名字也可以看到 event 是整个库的核心。event 就是 Reactor 框架中的事件处理程序组件;它提供了函数接口,供 Reactor 在事件发生时调用,以执行相应的事件处理,通常它会绑定一个有效的句柄。首先给出 event 结构体的声明,它位于 event.h 文件中: 1234567891011121314151617181920212223struct event { TAILQ_ENTRY (event) ev_next; TAILQ_ENTRY (event) ev_active_next; TAILQ_ENTRY (event) ev_signal_next; unsigned int min_heap_idx; /* for managing timeouts */ struct event_base *ev_base; int ev_fd; short ev_events; short ev_ncalls; short *ev_pncalls; /* Allows deletes in callback */ struct timeval ev_timeout; int ev_pri; /* smaller numbers are higher priority */ void (*ev_callback)(int, short, void *arg); void *ev_arg; int ev_res; /* result passed to event callback */ int ev_flags;}; 下面详细解释一下结构体中各字段的含义。 1)ev_events: event关注的事件类型，可以是以下3种类型： I/O事件: EV_WRITE和EV_READ 定时事件:EV_TIMEOUT 信号：EV_SIGNAL 辅助选项:EV_PERSIST,表明是一个永久事件 libevent中的定义为： 12345#define EV_TIMEOUT 0x01#define EV_READ 0x02#define EV_WRITE 0x04#define EV_SIGNAL 0x08#define EV_PERSIST 0x10 /* Persistant event */ 可以看出事件类型可以使用“|”运算符进行组合,需要说明的是,信号和I/O事件不能同时设置;还可以看出libevent使用event结构体将这3种事件的处理统一起来; 2)ev_next,ev_active_next 和 ev_signal_next 都是双向链表节点指针;它们是 libevent 对不同事件类型和在不同的时期,对事件的管理时使用到的字段。 libevent 使用双向链表保存所有注册的 I/O 和 Signal 事件,ev_next 就是该 I/O 事件在链表中的位置;称此链表为“已注册事件链表”; 同样 ev_signal_next 就是 signal 事件在 signal 事件链表中的位置; ev_active_next:libevent 将所有的激活事件放入到链表 active list 中,然后遍历 active list 执行调度,ev_active_next 就指明了 event 在 active list 中的位置; 3)min_heap_idx 和 ev_timeout 如果是 timeout 事件,它们是 event 在小根堆中的索引和超时值,libevent 使用小根堆来管理定时事件,这将在后面定时事件处理时专门讲解; 4)ev_base 该事件所属的反应堆实例,这是一个 event_base 结构体,下一节将会详细讲解; 5)ev_fd 对于 I/O 事件,是绑定的文件描述符;对于 signal 事件,是绑定的信号; 5)ev_callback event 的回调函数,被 ev_base 调用,执行事件处理程序,这是一个函数指针,原型为: 1void (*ev_callback)(int fd, short events, void *arg) 其中参数 fd 对应于 ev_fd；events 对应于 ev_events；arg 对应于 ev_arg； 6)ev_arg: void* 表明可以是任意类型的数据,在设置 event 时指定; 7)eb_flags:libevent 用于标记 event 信息的字段,表明其当前的状态,可能的值有: 123456#define EVLIST_TIMEOUT 0x01 //event在time堆中#define EVLIST_INSERTED 0x02 //event在已注册事件链表中#define EVLIST_SIGNAL 0x04 //未见使用#define EVLIST_ACTIVE 0x08 //event在激活链表中#define EVLIST_INTERNAL 0x10 //内部使用标记#define EVLIST_INIT 0x80 //event已被初始化 8)ev_ncalls:事件就绪执行时,调用 ev_callback 的次数,通常为 1; 9)ev_pncalls:指针,通常指向 ev_ncalls 或者为 NULL; 10)ev_res:记录了当前激活事件的类型; 2、libevent 对 event 的管理从event结构体中的3个链表节点指针和一个堆索引出发，大体上也能窥出 libevent 对event 的管理方法了，可以参见下面的示意图。 每次当有事件 event 转变为就绪状态时, libevent 就会把它移入到 active event list[priority]中,其中 priority 是 event 的优先级; 接着 libevent 会根据自己的调度策略选择就绪事件,调用其 cb_callback()函数执行事件处理;并根据就绪的句柄和事件类型填充 cb_callback 函数的参数。 3、事件设置的接口函数要向 libevent 添加一个事件,需要首先设置 event 对象,这通过调用 libevent 提供的函数有:event_set(), event_base_set(), event_priority_set() 来完成；下面分别进行讲解。 12345678910111213141516171819202122voidevent_set(struct event *ev, int fd, short events, void (*callback)(int, short, void *), void *arg){ /* Take the current base - caller needs to set the real base later */ ev-&gt;ev_base = current_base; ev-&gt;ev_callback = callback; ev-&gt;ev_arg = arg; ev-&gt;ev_fd = fd; ev-&gt;ev_events = events; ev-&gt;ev_res = 0; ev-&gt;ev_flags = EVLIST_INIT; ev-&gt;ev_ncalls = 0; ev-&gt;ev_pncalls = NULL; min_heap_elem_init(ev); /* by default, we put new events into the middle priority */ if(current_base) ev-&gt;ev_pri = current_base-&gt;nactivequeues/2;} 设置事件 ev 绑定的文件描述符或者信号,对于定时事件,设为-1 即可; 设置事件类型,比如 EV_READ|EV_PERSIST, EV_WRITE, EV_SIGNAL 等; 设置事件的回调函数以及参数 arg; 初始化其它字段,比如缺省的 event_base 和优先级; 123456789101112intevent_base_set(struct event_base *base, struct event *ev){ /* Only innocent events may be assigned to a different base */ if (ev-&gt;ev_flags != EVLIST_INIT) return (-1); ev-&gt;ev_base = base; ev-&gt;ev_pri = base-&gt;nactivequeues/2; return (0);} 设置 event ev 将要注册到的 event_base; libevent 有一个全局 event_base 指针 current_base,默认情况下事件 ev将被注册到 current_base 上,使用该函数可以指定不同的 event_base; 如果一个进程中存在多个 libevent 实例,则必须要调用该函数为 event 设置不同的 event_base; 1234567891011121314151617/* * Set's the priority of an event - if an event is already scheduled * changing the priority is going to fail. */intevent_priority_set(struct event *ev, int pri){ if (ev-&gt;ev_flags &amp; EVLIST_ACTIVE) return (-1); if (pri &lt; 0 || pri &gt;= ev-&gt;ev_base-&gt;nactivequeues) return (-1); ev-&gt;ev_pri = pri; return (0);} 设置event ev的优先级,没什么可说的,注意的一点就是:当ev正处于就绪状态时,不能设置,返回-1。 4、小结本节讲述了libevent的核心event结构,以及libevent支持的事件类型和libevent对event的管理模型;接下来将会描述libevent的事件处理框架,以及其中使用的重要的结构体event_base。 —-节选自《libevent 源码深度剖析》 张亮","link":"/2019/05/08/libevent源码剖析-3-核心event/"},{"title":"libevent源码剖析(4)--事件处理框架","text":"前面已经对 libevent 的事件处理框架和 event 结构体做了描述,现在是时候剖析 libevent 对事件的详细处理流程了,本节将分析 libevent 的事件处理框架 event_base 和 libevent 注册、 删除事件的具体流程,可结合前一节 libevent 对 event 的管理。 1、事件处理框架回想 Reactor 模式的几个基本组件,本节讲解的部分对应于 Reactor 框架组件。在 libevent中,这就表现为 event_base 结构体,结构体声明如下,它位于 event-internal.h 文件中: 1234567891011121314151617181920212223struct event_base { const struct eventop *evsel; void *evbase; int event_count; /* counts number of total events */ int event_count_active; /* counts number of active events */ int event_gotterm; /* Set to terminate loop */ int event_break; /* Set to terminate loop immediately */ /* active event management */ struct event_list **activequeues; int nactivequeues; /* signal handling info */ struct evsignal_info sig; struct event_list eventqueue; struct timeval event_tv; struct min_heap timeheap; struct timeval tv_cache;}; 下面详细解释一下结构体中各字段的含义。 1）evsel和evbase 这两个字段的设置可能会让人有些迷惑，这里你可以把evsel和evbase看作是类和静态函数的关系，比如添加事件时的调用行为：evsel-&gt;add(evbase, ev)，实际执行操作的是evbase；这相当于class::add(instance, ev)，instance就是class的一个对象实例。 evsel指向了全局变量static const struct eventop * eventops[]中的一个；前面也说过，libevent将系统提供的I/O demultiplex机制统一封装成了eventop结构；因此eventops[]包含了select、poll、kequeue和epoll等等其中的若干个全局实例对象。 evbase实际上是一个eventop实例对象； 先来看看eventop结构体，它的成员是一系列的函数指针, 在event-internal.h文件中： 1234567891011//一系列函数指针，对应epoll,select,poll等I/O demultiplex的五个接口函数struct eventop { const char *name; void *(*init)(struct event_base *); int (*add)(void *, struct event *); int (*del)(void *, struct event *); int (*dispatch)(struct event_base *, void *, struct timeval *); void (*dealloc)(struct event_base *, void *); /* set if we need to reinitialize the event base */ int need_reinit;}; 再来看全局变量static const struct eventop * eventops[]，在event.h文件中，该变量定义了几种可选的I/O demultiplex机制： 12345678910111213141516171819202122232425 /* In order of preference */static const struct eventop *eventops[] = {#ifdef HAVE_EVENT_PORTS &amp;evportops,#endif#ifdef HAVE_WORKING_KQUEUE &amp;kqops,#endif#ifdef HAVE_EPOLL &amp;epollops,#endif#ifdef HAVE_DEVPOLL &amp;devpollops,#endif#ifdef HAVE_POLL &amp;pollops,#endif#ifdef HAVE_SELECT &amp;selectops,#endif#ifdef WIN32 &amp;win32ops,#endif NULL}; 也就是说，在libevent中，每种I/O demultiplex机制的实现都必须提供这五个函数接口，来完成自身的初始化、销毁释放；对事件的注册、注销和分发。 比如对于epoll，libevent 实现了5个对应的接口函数，并在初始化时并将eventop的5个函数指针指向这5个函数，那么程序就可以使用epoll作为I/O demultiplex机制了，这个在后面会再次提到。 2）activequeues 是一个二级指针，前面讲过libevent支持事件优先级，因此你可以把它看作是数组，其中的元素activequeues[priority]是一个链表，链表的每个节点指向一个优先级为priority的就绪事件event。 3）eventqueue，链表，保存了所有的注册事件event的指针。 4）sig是用来管理信号的结构体，将在后面信号处理时专门讲解； 5）timeheap是管理定时事件的小根堆，将在后面定时事件处理时专门讲解； 6）event_tv和tv_cache是libevent用于时间管理的变量，将在后面讲到；其它各个变量都能因名知意，就不再啰嗦了。 2、创建和初始化event_base创建一个event_base 对象也既是创建了一个新的libevent 实例，程序需要通过调用event_init()（内部调用event_base_new函数执行具体操作）函数来创建，该函数同时还对新生成的libevent实例进行了初始化。该函数如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455struct event_base *event_init(void){ struct event_base *base = event_base_new(); if (base != NULL) current_base = base; return (base);}//event_base_new()函数实现struct event_base *event_base_new(void){ int i; struct event_base *base; //分配实例空间 if ((base = calloc(1, sizeof(struct event_base))) == NULL) event_err(1, \"%s: calloc\", __func__); event_sigcb = NULL; event_gotsig = 0; detect_monotonic(); gettime(base, &amp;base-&gt;event_tv); min_heap_ctor(&amp;base-&gt;timeheap); TAILQ_INIT(&amp;base-&gt;eventqueue); base-&gt;sig.ev_signal_pair[0] = -1; base-&gt;sig.ev_signal_pair[1] = -1; //初始化决定使用哪种I/O demultiplex机制 base-&gt;evbase = NULL; for (i = 0; eventops[i] &amp;&amp; !base-&gt;evbase; i++) { //确定使用哪种I/O demultiplex机制 base-&gt;evsel = eventops[i]; //实例化一个evsel，例如evsel为epoll，相当于调用epoll的init函数创建一个epoll实例 base-&gt;evbase = base-&gt;evsel-&gt;init(base); } if (base-&gt;evbase == NULL) event_errx(1, \"%s: no event mechanism available\", __func__); if (evutil_getenv(\"EVENT_SHOW_METHOD\")) event_msgx(\"libevent using: %s\\n\", base-&gt;evsel-&gt;name); /* allocate a single active event queue */ event_base_priority_init(base, 1); return (base);} 该函数首先为event_base实例申请空间，然后初始化timer mini-heap，选择并初始化合适的系统I/O 的demultiplexer机制，初始化各事件链表；函数还检测了系统的时间设置，为后面的时间管理打下基础。 3、接口函数前面提到Reactor框架（管理器）的作用就是提供事件的注册、注销接口；根据系统提供的事件多路分发机制执行事件循环，当有事件进入“就绪”状态时，调用注册事件的回调函数来处理事件。Libevent中对应的接口函数主要就是： int event_add(struct eventev, const struct timevaltimeout); int event_del(struct event*ev); int event_base_loop(struct event_base*base, int loops); void event_active(struct event*event, int res, short events); void event_process_active(struct event_base*base); 本节将按介绍事件注册和删除的代码流程，libevent 的事件循环框架将在下一节再具体描述。对于定时事件，这些函数将调用timer heap管理接口执行插入和删除操作；对于I/O和Signal事件将调用eventopadd和delete接口函数执行插入和删除操作（eventop会对Signal事件调用Signal处理接口执行操作）；这些组件将在后面的内容描述。 1）注册事件 函数原型： 1int event_add(struct event*ev, const struct timeval*tv) 参数：ev：指向要注册的事件； tv：超时时间； 函数将ev注册到ev-&gt;ev_base上，事件类型由ev-&gt;ev_events指明，如果注册成功，ev将被插入到已注册链表中；如果tv不是NULL，则会同时注册定时事件，将ev添加到timer堆上； 如果其中有一步操作失败，那么函数保证没有事件会被注册，可以讲这相当于一个原子操作。这个函数也体现了libevent细节之处的巧妙设计，且仔细看程序代码，部分有省略，注释直接附在代码中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889intevent_add(struct event *ev, const struct timeval *tv){ struct event_base *base = ev-&gt;ev_base; //要注册到的event_base const struct eventop *evsel = base-&gt;evsel; void *evbase = base-&gt;evbase; //base使用的系统I/O策略 int res = 0; //定义在log.h中 event_debug(( \"event_add: event: %p, %s%s%scall %p\", ev, ev-&gt;ev_events &amp; EV_READ ? \"EV_READ \" : \" \", ev-&gt;ev_events &amp; EV_WRITE ? \"EV_WRITE \" : \" \", tv ? \"EV_TIMEOUT \" : \" \", ev-&gt;ev_callback)); assert(!(ev-&gt;ev_flags &amp; ~EVLIST_ALL)); /* * prepare for timeout insertion further below, if we get a * failure on any step, we should not change any state. */ // 新的timer事件，调用timer heap接口在堆上预留一个位置 // 注：这样能保证该操作的原子性： // 向系统I/O机制注册可能会失败，而当在堆上预留成功后， // 定时事件的添加将肯定不会失败； // 而预留位置的可能结果是堆扩充，但是内部元素并不会改变 if (tv != NULL &amp;&amp; !(ev-&gt;ev_flags &amp; EVLIST_TIMEOUT)) { if (min_heap_reserve(&amp;base-&gt;timeheap, 1 + min_heap_size(&amp;base-&gt;timeheap)) == -1) return (-1); /* ENOMEM == errno */ } // 如果事件ev不在已注册或者激活链表中,则调用evbase注册事件 if ((ev-&gt;ev_events &amp; (EV_READ|EV_WRITE|EV_SIGNAL)) &amp;&amp; !(ev-&gt;ev_flags &amp; (EVLIST_INSERTED|EVLIST_ACTIVE))) { res = evsel-&gt;add(evbase, ev); if (res != -1) //注册成功，插入event到已注册链表中 event_queue_insert(base, ev, EVLIST_INSERTED); } /* * we should change the timout state only if the previous event * addition succeeded. */ //准备添加定时事件 if (res != -1 &amp;&amp; tv != NULL) { struct timeval now; /* * we already reserved memory above for the case where we * are not replacing an exisiting timeout. */ //EVLIST_TIMEOUT表明event已经在定时器堆中了，删除旧的 if (ev-&gt;ev_flags &amp; EVLIST_TIMEOUT) event_queue_remove(base, ev, EVLIST_TIMEOUT); /* Check if it is active due to a timeout. Rescheduling * this timeout before the callback can be executed * removes it from the active list. */ //如果事件已经是就绪状态则从激活链表中删除 if ((ev-&gt;ev_flags &amp; EVLIST_ACTIVE) &amp;&amp; (ev-&gt;ev_res &amp; EV_TIMEOUT)) { /* See if we are just active executing this * event in a loop */ //将ev_callback调用次数设置为0 if (ev-&gt;ev_ncalls &amp;&amp; ev-&gt;ev_pncalls) { /* Abort loop */ *ev-&gt;ev_pncalls = 0; } event_queue_remove(base, ev, EVLIST_ACTIVE); } //计算时间，并插入到timer小根堆中 gettime(base, &amp;now); evutil_timeradd(&amp;now, tv, &amp;ev-&gt;ev_timeout); event_debug(( \"event_add: timeout in %ld seconds, call %p\", tv-&gt;tv_sec, ev-&gt;ev_callback)); event_queue_insert(base, ev, EVLIST_TIMEOUT); } return (res);} event_queue_insert()负责将事件插入到对应的链表中,下面是程序代码; event_queue_remove()负责将事件从对应的链表中删除,这里就不再重复贴代码了; 12345678910111213141516171819202122232425262728293031323334voidevent_queue_insert(struct event_base *base, struct event *ev, int queue){ //ev可能已经在激活列表中了，避免重复插入 if (ev-&gt;ev_flags &amp; queue) { /* Double insertion is possible for active events */ if (queue &amp; EVLIST_ACTIVE) return; event_errx(1, \"%s: %p(fd %d) already on queue %x\", __func__, ev, ev-&gt;ev_fd, queue); } if (~ev-&gt;ev_flags &amp; EVLIST_INTERNAL) base-&gt;event_count++; ev-&gt;ev_flags |= queue; //记录queue标记 switch (queue) { case EVLIST_INSERTED: //I/O或Signal事件，加入已注册事件链表 TAILQ_INSERT_TAIL(&amp;base-&gt;eventqueue, ev, ev_next); break; case EVLIST_ACTIVE: //就绪事件，加入已注册事件链表 base-&gt;event_count_active++; TAILQ_INSERT_TAIL(base-&gt;activequeues[ev-&gt;ev_pri], ev,ev_active_next); break; case EVLIST_TIMEOUT: { //定时事件，加入堆 min_heap_push(&amp;base-&gt;timeheap, ev); break; } default: event_errx(1, \"%s: unknown queue %x\", __func__, queue); }} 2)删除事件 函数原型为： 1int event_del(struct event*ev); 该函数将删除事件ev，对于I/O事件，从I/O 的demultiplexer上将事件注销；对于Signal事件，将从Signal事件链表中删除；对于定时事件，将从堆上删除； 同样删除事件的操作则不一定是原子的，比如删除时间事件之后，有可能从系统I/O机制中注销会失败。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 intevent_del(struct event *ev){ struct event_base *base; const struct eventop *evsel; void *evbase; event_debug((\"event_del: %p, callback %p\", ev, ev-&gt;ev_callback)); /* An event without a base has not been added */ //ev_base为NULL，表明ev没有被注册 if (ev-&gt;ev_base == NULL) return (-1); //取得注册的event_base和eventop指针 base = ev-&gt;ev_base; evsel = base-&gt;evsel; evbase = base-&gt;evbase; assert(!(ev-&gt;ev_flags &amp; ~EVLIST_ALL)); /* See if we are just active executing this event in a loop */ //将ev_callback调用次数设置为0 if (ev-&gt;ev_ncalls &amp;&amp; ev-&gt;ev_pncalls) { /* Abort loop */ *ev-&gt;ev_pncalls = 0; } //从对应的链表中删除 if (ev-&gt;ev_flags &amp; EVLIST_TIMEOUT) event_queue_remove(base, ev, EVLIST_TIMEOUT); if (ev-&gt;ev_flags &amp; EVLIST_ACTIVE) event_queue_remove(base, ev, EVLIST_ACTIVE); if (ev-&gt;ev_flags &amp; EVLIST_INSERTED) { event_queue_remove(base, ev, EVLIST_INSERTED); //EVLIST_INSERTED表明I/O或者Signal事件 //需要调用I/O demultiplexer注销事件 return (evsel-&gt;del(evbase, ev)); } return (0);} 4、小结分析了 event_base 这一重要结构体,初步看到了 libevent 对系统的 I/O demultiplex 机制的封装 event_op 结构,并结合源代码分析了事件的注册和删除处理,下面将会接着分析事件管理框架中的主事件循环部分。","link":"/2019/05/08/libevent源码剖析-4-事件处理框架/"},{"title":"libevent源码剖析(5)--事件主循环","text":"在初步了解了 libevent 的 Reactor 组件——event_base 和事件管理框架后,接下来就是 libevent 事件处理的中心部分——事件主循环,根据系统提供的事件多路分发机制执行事件循环,对已注册的就绪事件,调用注册事件的回调函数来处理事件。 1、事件处理主循环Libevent 的事件主循环主要是通过 event_base_loop () 函数完成的,其主要操作如下面的流程图所示,event_base_loop 所作的就是持续执行下面的循环。 清楚了 event_base_loop 所作的主要操作,就可以对比源代码看个究竟了,代码结构还是相当清晰的。 该函数完成以下工作： 1.信号标记被设置，则调用信号的回调函数 2.根据定时器最小时间，设置I/O多路复用的最大等待时间，这样即使没有I/O事件发生，也能在最小定时器超时时返回。 3.调用I/O多路复用，监听事件，将活跃事件添加到活跃事件链表中 4.检查定时事件，将就绪的定时事件从小根堆中删除，插入到活跃事件链表中 5.对event_base的活跃事件链表中的事件，调用event_process_active(）函数，在该函数内调用event的回调函数，优先级高的event先处理。 该函数内部调用了eventop.dispatch()监听事件，event_sigcb函数指针处理信号事件，timeout_process()将超时的定时事件加入到活跃事件链表中，event_process_active()处理活跃事件链表中的事件，调用相应的回调函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106intevent_base_loop(struct event_base *base, int flags){ const struct eventop *evsel = base-&gt;evsel; void *evbase = base-&gt;evbase; struct timeval tv; struct timeval *tv_p; int res, done; /* clear time cache */ //清时间缓存 base-&gt;tv_cache.tv_sec = 0; //evsignal_base是全局变量，在处理signal时，用于指明signal所属的event_base实例 if (base-&gt;sig.ev_signal_added) evsignal_base = base; done = 0; while (!done) { //事件主循环 /* Terminate the loop if we have been asked to */ //查看是否需要跳出循环，程序可以调用event_loopexit_cb()设置event_gotterm标记 if (base-&gt;event_gotterm) { base-&gt;event_gotterm = 0; break; } //调用event_base_loopbreak()设置event_break标记 if (base-&gt;event_break) { base-&gt;event_break = 0; break; } /* You cannot use this interface for multi-threaded apps */ //当event_gotsig被设置时，则event_sigcb就是信号处理的回调函数 while (event_gotsig) { event_gotsig = 0; if (event_sigcb) { res = (*event_sigcb)(); //调用信号处理的回调函数 if (res == -1) { errno = EINTR; return (-1); } } } /* 校正系统时间,如果系统使用的是非MONOTONIC时间,用户可能会向后调整了系统时间 在timeout_correct函数里,比较last wait time和当前时间,如果当前时间&lt; last wait time 表明时间有问题,这时需要更新timer_heap中所有定时事件的超时时间。 */ timeout_correct(base, &amp;tv); tv_p = &amp;tv; //根据定时器堆中最小超时时间计算I/O多路复用的最大等待时间tv_p if (!base-&gt;event_count_active &amp;&amp; !(flags &amp; EVLOOP_NONBLOCK)) { timeout_next(base, &amp;tv_p); } else { /* * if we have active events, we just poll new events * without waiting. */ // evutil_timerclear(&amp;tv); } /* If we have no events, we just exit */ //没有注册事件，则退出 if (!event_haveevents(base)) { event_debug((\"%s: no events registered.\", __func__)); return (1); } /* update last old time */ //更新last wait time,并清空time cache gettime(base, &amp;base-&gt;event_tv); /* clear time cache */ base-&gt;tv_cache.tv_sec = 0; //调用I/O多路复用，监听事件，等待就绪I/Oevents，可能是epoll_wait()，或者select等 //在evsel-&gt;dispatch()中,会把就绪signal event、I/O event插入到激活链表中 res = evsel-&gt;dispatch(base, evbase, tv_p); if (res == -1) return (-1); //将将time cache赋值为当前系统时间 gettime(base, &amp;base-&gt;tv_cache); //检查定时事件，将就绪的定时事件从小根堆中删除，插入到活跃事件链表中 timeout_process(base); if (base-&gt;event_count_active) { //处理event_base的活跃链表中的事件 //调用event的回调函数，优先级高的event先处理 event_process_active(base); if (!base-&gt;event_count_active &amp;&amp; (flags &amp; EVLOOP_ONCE)) done = 1; } else if (flags &amp; EVLOOP_NONBLOCK) done = 1; } /* clear time cache */ //循环结束，清空时间缓存 base-&gt;tv_cache.tv_sec = 0; event_debug((\"%s: asked to terminate loop.\", __func__)); return (0);} 在event_base_loop()中，有两个函数需要重点关注，一个是dispatch()函数，一个是event_process_active()函数,下面分别对这两个函数详解。 1.1dispatch()函数在上面我们看到，event_base_loop()中通过I/O多路复用的dispatch()函数完成监听事件功能。在之前的event_init()中我们看到，通过遍历eventops数组，从中选择一个I/O多路复用机制，所以不同的I/O多路复用机制有不同的eventop结构体，相应的也就有不同的 dispatch() 函数。下面，再次看下eventop结构体(event-internal.h) 12345678910struct eventop { const char *name; void *(*init)(struct event_base *); //初始化 int (*add)(void *, struct event *); //注册事件 int (*del)(void *, struct event *); //删除事件 int (*dispatch)(struct event_base *, void *, struct timeval *); //事件分发 void (*dealloc)(struct event_base *, void *); //注销，释放资源 /* set if we need to reinitialize the event base */ int need_reinit;}; 在event_add()中通过add()成员函数注册event到监听事件中，现在在event_base_loop()中通过dispatch()成员函数监听事件。libevent支持多种I/O多路复用机制，下面先看下epoll的eventop结构体(epoll.c) 123456789const struct eventop epollops = { \"epoll\", epoll_init, epoll_add, epoll_del, epoll_dispatch, epoll_dealloc, 1 /* need reinit */}; 然后看下epoll的dispatch()函数(epoll.c) 从下面源码可见，epoll_dispatch()的工作主要有： 1.调用epoll_wait()监听事件 2.如果有信号发生，调用evsignal_process()处理信号 3.将活跃的event根据其活跃的类型注册到活跃事件链表上 4.如果events数组大小不够，则重新分配为原来2倍大小 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768static intepoll_dispatch(struct event_base *base, void *arg, struct timeval *tv){ struct epollop *epollop = arg; struct epoll_event *events = epollop-&gt;events; struct evepoll *evep; int i, res, timeout = -1; if (tv != NULL) timeout = tv-&gt;tv_sec * 1000 + (tv-&gt;tv_usec + 999) / 1000; //转换为微米 if (timeout &gt; MAX_EPOLL_TIMEOUT_MSEC) { //设置最大超时时间 /* Linux kernels can wait forever if the timeout is too big; * see comment on MAX_EPOLL_TIMEOUT_MSEC. */ timeout = MAX_EPOLL_TIMEOUT_MSEC; } res = epoll_wait(epollop-&gt;epfd, events, epollop-&gt;nevents, timeout); //监听事件发生 if (res == -1) { if (errno != EINTR) { event_warn(\"epoll_wait\"); return (-1); } evsignal_process(base); //由于Signal事件发生中断，处理Signal事件 return (0); } else if (base-&gt;sig.evsignal_caught) { evsignal_process(base); //有Signal事件发生，处理Signal事件 } event_debug((\"%s: epoll_wait reports %d\", __func__, res)); for (i = 0; i &lt; res; i++) { //处理活跃事件 int what = events[i].events; //活跃类型 struct event *evread = NULL, *evwrite = NULL; int fd = events[i].data.fd; //event的文件描述符 if (fd &lt; 0 || fd &gt;= epollop-&gt;nfds) continue; evep = &amp;epollop-&gt;fds[fd]; if (what &amp; (EPOLLHUP|EPOLLERR)) { //判断epoll的events类型，并找到注册的event evread = evep-&gt;evread; evwrite = evep-&gt;evwrite; } else { if (what &amp; EPOLLIN) { evread = evep-&gt;evread; } if (what &amp; EPOLLOUT) { evwrite = evep-&gt;evwrite; } } if (!(evread||evwrite)) continue; //添加event到活跃事件链表中 if (evread != NULL) event_active(evread, EV_READ, 1); if (evwrite != NULL) event_active(evwrite, EV_WRITE, 1); } //如果注册的事件全部变为活跃，则增大events数组为原来两倍 if (res == epollop-&gt;nevents &amp;&amp; epollop-&gt;nevents &lt; MAX_NEVENTS) { /* We used all of the event space this time. We should be ready for more events next time. */ int new_nevents = epollop-&gt;nevents * 2; struct epoll_event *new_events; new_events = realloc(epollop-&gt;events, new_nevents * sizeof(struct epoll_event)); if (new_events) { epollop-&gt;events = new_events; epollop-&gt;nevents = new_nevents; } } return (0);} 1.2event_process_active()函数在event_base_loop()中，当活跃的I/O事件、定时器事件已经添加到活跃事件链表中后，开始调用这些event的回调函数进行处理了，这步是在event_base_loop()中调用event_process_active()来完成的。该函数从event_base的activequeueus链表数组上取出一个链表；对该链表上的event调用回调函数；优先调用优先级值最小的event 12345678910111213141516171819202122232425262728293031323334353637383940static voidevent_process_active(struct event_base *base){ struct event *ev; struct event_list *activeq = NULL; int i; short ncalls; for (i = 0; i &lt; base-&gt;nactivequeues; ++i) { //取出第一个活跃链表 if (TAILQ_FIRST(base-&gt;activequeues[i]) != NULL) { activeq = base-&gt;activequeues[i]; break; } } assert(activeq != NULL); //优先处理优先级值最小的event for (ev = TAILQ_FIRST(activeq); ev; ev = TAILQ_FIRST(activeq)) { if (ev-&gt;ev_events &amp; EV_PERSIST) event_queue_remove(base, ev, EVLIST_ACTIVE); //是持久事件，则从活跃链表移除 else event_del(ev); //不是持久事件，则直接删除该事件 /* Allows deletes to work */ ncalls = ev-&gt;ev_ncalls; ev-&gt;ev_pncalls = &amp;ncalls; while (ncalls) { ncalls--; ev-&gt;ev_ncalls = ncalls; //调用该event的回调函数，event.ev_res保存返回值 (*ev-&gt;ev_callback)((int)ev-&gt;ev_fd, ev-&gt;ev_res, ev-&gt;ev_arg); if (event_gotsig || base-&gt;event_break) { ev-&gt;ev_pncalls = NULL; return; } } ev-&gt;ev_pncalls = NULL; }} 2、I/O 和 Timer 事件的统一Libevent 将 Timer 和 Signal 事件都统一到了系统的 I/O 的 demultiplex 机制中了,相信读者从上面的流程和代码中也能窥出一斑了,下面就再啰嗦一次了。 首先将 Timer 事件融合到系统 I/O 多路复用机制中,还是相当清晰的,因为系统的 I/O机制像 select()和 epoll_wait()都允许程序制定一个最大等待时间(也称为最大超时时间)timeout,即使没有 I/O 事件发生,它们也保证能在 timeout 时间内返回。 那么根据所有 Timer 事件的最小超时时间来设置系统 I/O 的 timeout 时间;当系统 I/O返回时,再激活所有就绪的 Timer 事件就可以了,这样就能将 Timer 事件完美的融合到系统的 I/O 机制中了。 这是在 Reactor 和 Proactor 模式(主动器模式,比如 Windows 上的 IOCP)中处理 Timer事件的经典方法了,ACE 采用的也是这种方法,大家可以参考 POSA vol2 书中的 Reactor模式一节。 堆是一种经典的数据结构,向堆中插入、删除元素时间复杂度都是 O(lgN),N 为堆中元素的个数,而获取最小key 值(小根堆)的复杂度为 O(1);因此变成了管理 Timer 事件的绝佳人选(当然是非唯一的),libevent 就是采用的堆结构。 3、I/O 和 Signal 事件的统一Signal 是异步事件的经典事例,将 Signal 事件统一到系统的 I/O 多路复用中就不像 Timer事件那么自然了,Signal 事件的出现对于进程来讲是完全随机的,进程不能只是测试一个变量来判别是否发生了一个信号,而是必须告诉内核“在此信号发生时,请执行如下的操作”。 如果当 Signal 发生时,并不立即调用 event 的 callback 函数处理信号,而是设法通知系统的 I/O 机制,让其返回,然后再统一和 I/O 事件以及 Timer 一起处理,不就可以了嘛。是的,这也是 libevent 中使用的方法。 问题的核心在于,当 Signal 发生时,如何通知系统的 I/O 多路复用机制,这里先买个小关子,放到信号处理一节再详细说明,我想读者肯定也能想出通知的方法,比如使用 pipe。 4、小结本节介绍了 libevent 的事件主循环,描述了 libevent 是如何处理就绪的 I/O 事件、定时器和信号事件,以及如何将它们无缝的融合到一起。","link":"/2019/05/09/libevent源码剖析-5-事件主循环/"},{"title":"libevent源码剖析(7)--集成定时器事件","text":"现在再来详细分析 libevent 中 I/O 事件和 Timer 事件的集成,与 Signal 相比,Timer 事件的集成会直观和简单很多。Libevent 对堆的调整操作做了一些优化,本节还会描述这些优化方法。 1、集成到事件主循环因为系统的 I/O 机制像 select()和 epoll_wait()都允许程序制定一个最大等待时间(也称为最大超时时间)timeout,即使没有 I/O 事件发生,它们也保证能在 timeout 时间内返回。 那么根据所有 Timer 事件的最小超时时间来设置系统 I/O 的 timeout 时间;当系统 I/O返回时,再激活所有就绪的 Timer 事件就可以了,这样就能将 Timer 事件完美的融合到系统的 I/O 机制中了。 具体的代码在源文件 event.c 的 event_base_loop()中,现在就对比代码来看看这一处理方法: 12345678910111213141516171819202122232425262728293031323334static inttimeout_next(struct event_base *base, struct timeval **tv_p){ struct timeval now; struct event *ev; struct timeval *tv = *tv_p; //堆的首元素具有最小的超时值 if ((ev = min_heap_top(&amp;base-&gt;timeheap)) == NULL) { // 如果没有定时事件,将等待时间设置为NULL,表示一直阻塞直到有I/O事件发生 /* if no time-based events are active wait for I/O */ *tv_p = NULL; return (0); } //取得当前时间 if (gettime(base, &amp;now) == -1) return (-1); //如果超时事件&lt;=当前值，不能等待，需要立即返回 if (evutil_timercmp(&amp;ev-&gt;ev_timeout, &amp;now, &lt;=)) { evutil_timerclear(tv); return (0); } //计算等待的事件=当前事件-最小的超时时间 evutil_timersub(&amp;ev-&gt;ev_timeout, &amp;now, tv); assert(tv-&gt;tv_sec &gt;= 0); assert(tv-&gt;tv_usec &gt;= 0); event_debug((\"timeout_next: in %ld seconds\", tv-&gt;tv_sec)); return (0);} 2、Timer小根堆Libevent 使用堆来管理 Timer 事件,其 key 值就是事件的超时时间,源代码位于文件min_heap.h 中。 所有的数据结构书中都有关于堆的详细介绍,向堆中插入、删除元素时间复杂度都是O(lgN),N 为堆中元素的个数,而获取最小 key 值(小根堆)的复杂度为 O(1)。堆是一个完全二叉树,基本存储方式是一个数组。 Libevent 实现的堆还是比较轻巧的。轻巧到什么地方呢,就以插入元素为例,来对比说明,下面伪代码中的 size 表示当前堆的元素个数: 典型的代码逻辑如下： 而 libevent 的 heap 代码对这一过程做了优化,在插入新元素时,只是为新元素预留了一个位置 hole(初始时 hole 位于数组尾部),但并不立刻将新元素插入到 hole 上,而是不断向上调整 hole 的值,将父节点向下调整,最后确认 hole 就是新元素的所在位置时,才会真正的将新元素插入到 hole 上,因此在调整过程中就比上面的代码少了一次赋值的操作,代码逻辑是: 由于每次调整都少做一次赋值操作,在调整路径比较长时,调整效率会比第一种有所提高。libevent 中的min_heap_shift_up_()函数就是上面逻辑的具体实现,对应的向下调整函数是min_heap_shift_down_()。 1234567891011121314151617181920212223242526void min_heap_shift_up_(min_heap_t* s, unsigned hole_index, struct event* e){ unsigned parent = (hole_index - 1) / 2; while(hole_index &amp;&amp; min_heap_elem_greater(s-&gt;p[parent], e)) { (s-&gt;p[hole_index] = s-&gt;p[parent])-&gt;min_heap_idx = hole_index; hole_index = parent; parent = (hole_index - 1) / 2; } (s-&gt;p[hole_index] = e)-&gt;min_heap_idx = hole_index;}void min_heap_shift_down_(min_heap_t* s, unsigned hole_index, struct event* e){ unsigned min_child = 2 * (hole_index + 1); while(min_child &lt;= s-&gt;n) { min_child -= min_child == s-&gt;n || min_heap_elem_greater(s-&gt;p[min_child], s-&gt;p[min_child - 1]); if(!(min_heap_elem_greater(e, s-&gt;p[min_child]))) break; (s-&gt;p[hole_index] = s-&gt;p[min_child])-&gt;min_heap_idx = hole_index; hole_index = min_child; min_child = 2 * (hole_index + 1); } min_heap_shift_up_(s, hole_index, e);} 举个例子,向一个小根堆 3, 5, 8, 7, 12 中插入新元素 2,使用第一中典型的代码逻辑,其调整过程如下图所示: 使用 libevent 中的堆调整逻辑,调整过程如下图所示: 对于删除和元素修改操作,也遵从相同的逻辑,就不再罗嗦了。 3、小结通过设置系统 I/O 机制的 wait 时间,从而简捷的集成 Timer 事件;主要分析了 libevent对堆调整操作的优化。","link":"/2019/05/10/libevent源码剖析-7-集成定时器事件/"},{"title":"libevent源码剖析(6)--集成信号处理","text":"现在我们已经了解了 libevent 的基本框架:事件管理框架和事件主循环。上节提到了libevent 中 I/O 事件和 Signal 以及 Timer 事件的集成,这一节将分析如何将 Signal 集成到事件主循环的框架中。 1、集成策略——使用 socket pair前一节已经做了足够多的介绍了,基本方法就是采用“消息机制”。在 libevent 中这是通过 socket pair 完成的,下面就来详细分析一下。 Socket pair 就是一个 socket 对,包含两个 socket,一个读 socket,一个写 socket。工作方式如下图所示: 创建一个 socket pair 并不是复杂的操作,可以参见下面的流程图,清晰起见,其中忽略了一些错误处理和检查。 Libevent 提供了辅助函数 evutil_socketpair()来创建一个 socket pair,可以结合上面的创建流程来分析该函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697intevutil_socketpair(int family, int type, int protocol, int fd[2]){#ifndef WIN32 return socketpair(family, type, protocol, fd);#else /* This code is originally from Tor. Used with permission. */ /* This socketpair does not work when localhost is down. So * it's really not the same thing at all. But it's close enough * for now, and really, when localhost is down sometimes, we * have other problems too. */ int listener = -1; int connector = -1; int acceptor = -1; struct sockaddr_in listen_addr; struct sockaddr_in connect_addr; int size; int saved_errno = -1; if (protocol#ifdef AF_UNIX || family != AF_UNIX#endif ) { EVUTIL_SET_SOCKET_ERROR(WSAEAFNOSUPPORT); return -1; } if (!fd) { EVUTIL_SET_SOCKET_ERROR(WSAEINVAL); return -1; } listener = socket(AF_INET, type, 0); if (listener &lt; 0) return -1; memset(&amp;listen_addr, 0, sizeof(listen_addr)); listen_addr.sin_family = AF_INET; listen_addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK); listen_addr.sin_port = 0; /* kernel chooses port. */ if (bind(listener, (struct sockaddr *) &amp;listen_addr, sizeof (listen_addr)) == -1) goto tidy_up_and_fail; if (listen(listener, 1) == -1) goto tidy_up_and_fail; connector = socket(AF_INET, type, 0); if (connector &lt; 0) goto tidy_up_and_fail; /* We want to find out the port number to connect to. */ size = sizeof(connect_addr); if (getsockname(listener, (struct sockaddr *) &amp;connect_addr, &amp;size) == -1) goto tidy_up_and_fail; if (size != sizeof (connect_addr)) goto abort_tidy_up_and_fail; if (connect(connector, (struct sockaddr *) &amp;connect_addr, sizeof(connect_addr)) == -1) goto tidy_up_and_fail; size = sizeof(listen_addr); acceptor = accept(listener, (struct sockaddr *) &amp;listen_addr, &amp;size); if (acceptor &lt; 0) goto tidy_up_and_fail; if (size != sizeof(listen_addr)) goto abort_tidy_up_and_fail; EVUTIL_CLOSESOCKET(listener); /* Now check we are talking to ourself by matching port and host on the two sockets. */ if (getsockname(connector, (struct sockaddr *) &amp;connect_addr, &amp;size) == -1) goto tidy_up_and_fail; if (size != sizeof (connect_addr) || listen_addr.sin_family != connect_addr.sin_family || listen_addr.sin_addr.s_addr != connect_addr.sin_addr.s_addr || listen_addr.sin_port != connect_addr.sin_port) goto abort_tidy_up_and_fail; fd[0] = connector; fd[1] = acceptor; return 0; abort_tidy_up_and_fail: saved_errno = WSAECONNABORTED; tidy_up_and_fail: if (saved_errno &lt; 0) saved_errno = WSAGetLastError(); if (listener != -1) EVUTIL_CLOSESOCKET(listener); if (connector != -1) EVUTIL_CLOSESOCKET(connector); if (acceptor != -1) EVUTIL_CLOSESOCKET(acceptor); EVUTIL_SET_SOCKET_ERROR(saved_errno); return -1;#endif} 2、集成到事件主循环——通知 event_baseSocket pair 创建好了,可是 libevent 的事件主循环还是不知道 Signal 是否发生了啊,看来我们还差了最后一步,那就是:为 socket pair 的读 socket 在 libevent 的 event_base 实例上注册一个 persist 的读事件。 这样当向写 socket 写入数据时,读 socket 就会得到通知,触发读事件,从而 event_base就能相应的得到通知了。 前面提到过,Libevent 会在事件主循环中检查标记,来确定是否有触发的 signal,如果标记被设置就处理这些 signal,这段代码在各个具体的 I/O 机制中,以 Epoll 为例,在epoll_dispatch()函数中,代码片段如下: 12345678910111213res = epoll_wait(epollop-&gt;epfd, events, epollop-&gt;nevents, timeout); //监听事件发生 if (res == -1) { if (errno != EINTR) { event_warn(\"epoll_wait\"); return (-1); } evsignal_process(base); //由于Signal事件发生中断，处理Signal事件 return (0); } else if (base-&gt;sig.evsignal_caught) { evsignal_process(base); //有Signal事件发生，处理Signal事件 } 完整的处理框架如下所示: 注 1:libevent 中,初始化阶段并不注册读 socket 的读事件,而是在注册信号阶段才会测试并注册; 注 2:libevent 中,检查 I/O 事件是在各系统 I/O 机制的 dispatch()函数中完成的,该 dispatch()函数在event_base_loop()函数中被调用; 3、evsignal_info 结构体Libevent 中 Signal 事件的管理是通过结构体 evsignal_info 完成的,结构体位于 evsignal.h文件中，定义如下: 1234567891011121314struct evsignal_info { struct event ev_signal; int ev_signal_pair[2]; int ev_signal_added; volatile sig_atomic_t evsignal_caught; struct event_list evsigevents[NSIG]; sig_atomic_t evsigcaught[NSIG];#ifdef HAVE_SIGACTION struct sigaction **sh_old;#else ev_sighandler_t **sh_old;#endif int sh_old_max;}; 下面详细介绍一下个字段的含义和作用: 1)ev_signal 为 socket pair 的读 socket 向 event_base 注册读事件时使用的 event 结构体; 2)ev_signal_pair socket pair 对,作用见第一节的介绍; 3)ev_signal_added 记录 ev_signal 事件是否已经注册了; 4)evsignal_caught 是否有信号发生的标记;是 volatile 类型,因为它会在另外的线程中被修改; 5)evsigvents[NSIG] 数组,evsigevents[signo]表示注册到信号 signo 的事件链表; 6)evsigcaught[NSIG] 具体记录每个信号触发的次数,evsigcaught[signo]是记录信号 signo被触发的次数; 7)sh_old 记录了原来的 signal 处理函数指针,当信号 signo 注册的 event 被清空时,需要重新设置其处理函数; evsignal_info 的初始化包括，创建 socket pair，设置 ev_signal 事件(但并没有注册,而是等到有信号注册时才检查并注册)，并将所有标记置零，初始化信号的注册事件链表指针等。 4、注册、注销 signal 事件注册 signal 事件是通过 evsignal_add(struct event *ev) 函数完成的。 libevent 对所有的信号注册同一个处理函数 evsignal_handler()，该函数将在下一段介绍，注册过程如下： 1234567891011121314151617181920212223242526272829303132333435363738intevsignal_add(struct event *ev){ int evsignal; struct event_base *base = ev-&gt;ev_base; struct evsignal_info *sig = &amp;ev-&gt;ev_base-&gt;sig; if (ev-&gt;ev_events &amp; (EV_READ|EV_WRITE)) event_errx(1, \"%s: EV_SIGNAL incompatible use\", __func__); evsignal = EVENT_SIGNAL(ev); //取得 ev 要注册到的信号 signo assert(evsignal &gt;= 0 &amp;&amp; evsignal &lt; NSIG); //如果信号 signo 未被注册 if (TAILQ_EMPTY(&amp;sig-&gt;evsigevents[evsignal])) { event_debug((\"%s: %p: changing signal handler\", __func__, ev)); //为 signo 注册信号处理函数 evsignal_handler(); if (_evsignal_set_handler( base, evsignal, evsignal_handler) == -1) return (-1); /* catch signals if they happen quickly */ evsignal_base = base; //如果事件 ev_signal 还没哟注册,就注册 ev_signal 事件 if (!sig-&gt;ev_signal_added) { if (event_add(&amp;sig-&gt;ev_signal, NULL)) return (-1); sig-&gt;ev_signal_added = 1; } } /* multiple events may listen to the same signal */ //将事件 ev 添加到 signo 的 event 链表中; TAILQ_INSERT_TAIL(&amp;sig-&gt;evsigevents[evsignal], ev, ev_signal_next); return (0);} evsignal_add(struct event *ev)函数如下： 1 取得 ev 要注册到的信号 signo; 2 如果信号 signo 未被注册,那么就为 signo 注册信号处理函数 evsignal_handler(); 3 如果事件 ev_signal 还没哟注册,就注册 ev_signal 事件; 4 将事件 ev 添加到 signo 的 event 链表中; 从 signo 上注销一个已注册的 signal 事件就更简单了,直接从其已注册事件的链表中移除即可。如果事件链表已空,那么就恢复旧的处理函数; 下面的讲解都以 signal()函数为例,sigaction()函数的处理和 signal()相似。 处理函数evsignal_handler()函数做的事情很简单,就是记录信号的发生次数,并通知event_base有信号触发,需要处理: 12345678910111213141516171819202122232425static voidevsignal_handler(int sig){ int save_errno = errno; //不覆盖原来的错误代码 if (evsignal_base == NULL) { event_warn( \"%s: received signal %d, but have no base configured\", __func__, sig); return; } // 记录信号sig的触发次数,并设置event触发标记 evsignal_base-&gt;sig.evsigcaught[sig]++; evsignal_base-&gt;sig.evsignal_caught = 1;#ifndef HAVE_SIGACTION signal(sig, evsignal_handler); //重新注册信号#endif /* Wake up our notification mechanism */ // 向写socket写一个字节数据,触发event_base的I/O事件,从而通知其有信号触发,需要处理 send(evsignal_base-&gt;sig.ev_signal_pair[0], \"a\", 1, 0); errno = save_errno; //错误代码} 4、小结本节介绍了 libevent 对 signal 事件的具体处理框架,包括事件注册、删除和 socket pair通知机制,以及是如何将 Signal 事件集成到事件主循环之中的。","link":"/2019/05/09/libevent源码剖析-6-集成信号处理/"},{"title":"libevent源码剖析(9)--时间管理","text":"为了支持定时器,Libevent 必须和系统时间打交道，这一部分的内容也比较简单，主要涉及到时间的加减辅助函数、时间缓存、时间校正和定时器堆的时间值调整等。下面就结合源代码来分析一下。 1、初始化检测Libevent 在初始化时会检测系统时间的类型，通过调用函数 detect_monotonic() 完成,它通过调用clock_gettime()来检测系统是否支持 monotonic 时钟类型: 12345678910static voiddetect_monotonic(void){#if defined(HAVE_CLOCK_GETTIME) &amp;&amp; defined(CLOCK_MONOTONIC) struct timespec ts; if (clock_gettime(CLOCK_MONOTONIC, &amp;ts) == 0) use_monotonic = 1; //系统支持，monotonic时间#endif} Monotonic 时间指示的是系统从 boot 后到现在所经过的时间,如果系统支持 Monotonic时间就将全局变量 use_monotonic 设置为 1,设置 use_monotonic 到底有什么用,这个在后面说到时间校正时就能看出来了。 2、时间缓存结构体 event_base 中的 tv_cache，用来记录时间缓存。这个还要从函数 gettime()说起，先来看看该函数的代码: 12345678910111213141516171819202122232425static intgettime(struct event_base *base, struct timeval *tp){ //如果tv_cache时间缓存已设置，就直接使用 if (base-&gt;tv_cache.tv_sec) { *tp = base-&gt;tv_cache; return (0); } //如果支持monotonic，就用clock_gettime获取monotonic时间#if defined(HAVE_CLOCK_GETTIME) &amp;&amp; defined(CLOCK_MONOTONIC) if (use_monotonic) { struct timespec ts; if (clock_gettime(CLOCK_MONOTONIC, &amp;ts) == -1) return (-1); tp-&gt;tv_sec = ts.tv_sec; tp-&gt;tv_usec = ts.tv_nsec / 1000; return (0); }#endif //否则只能取得系统当前时间 return (evutil_gettimeofday(tp, NULL));} 如果 tv_cache 已经设置，那么就直接使用缓存的时间；否则需要再次执行系统调用获取系统时间。 函数 evutil_gettimeofday()用来获取当前系统时间，在 Linux 下其实就是系统调用gettimeofday()；Windows 没有提供函数 gettimeofday，而是通过调用_ftime()来完成的。 在每次系统事件循环中，时间缓存 tv_cache 将会被相应的清空和设置，再次来看看下面 event_base_loop 的主要代码逻辑: 12345678910111213141516171819202122int event_base_loop(struct event_base *base, int flags){ // 清空时间缓存 base-&gt;tv_cache.tv_sec = 0; while(!done){ timeout_correct(base, &amp;tv); // 时间校正 // 更新event_tv到tv_cache指示的时间或者当前时间(第一次) // event_tv &lt;--- tv_cache gettime(base, &amp;base-&gt;event_tv); // 清空时间缓存-- 时间点1 base-&gt;tv_cache.tv_sec = 0; // 等待I/O事件就绪 res = evsel-&gt;dispatch(base, evbase, tv_p); // 缓存tv_cache存储了当前时间的值-- 时间点2 // tv_cache &lt;--- now gettime(base, &amp;base-&gt;tv_cache); // .. 处理就绪事件 } // 退出时也要清空时间缓存 base-&gt;tv_cache.tv_sec = 0; return (0);} 时间 event_tv 指示了 dispatch()上次返回，也就是 I/O 事件就绪时的时间，第一次进入循环时,由于 tv_cache 被清空，因此 gettime()执行系统调用获取当前系统时间；而后将会更新为 tv_cache 指示的时间。 时间 tv_cache 在 dispatch()返回后被设置为当前系统时间,因此它缓存了本次 I/O 事件就绪时的时间(event_tv)。 从代码逻辑里可以看出 event_tv 取得的是 tv_cache 上一次的值，因此 event_tv 应该小于tv_cache 的值。 设置时间缓存的优点是不必每次获取时间都执行系统调用，这是个相对费时的操作；在上面标注的时间点 2 到时间点 1 的这段时间(处理就绪事件时)，调用 gettime()取得的都是tv_cache 缓存的时间。 3、时间校正如果系统支持 monotonic 时间，该时间是系统从 boot 后到现在所经过的时间，因此不需要执行校正。 根据前面的代码逻辑，如果系统不支持 monotonic 时间，用户可能会手动的调整时间，如果时间被向前调整了(MS 前面第 7 部分讲成了向后调整,要改正)，比如从 5 点调整到了 3 点,那么在时间点 2 取得的值可能会小于上次的时间，这就需要调整了，下面来看看校正的具体代码，由函数 timeout_correct()完成： 123456789101112131415161718192021222324252627282930313233343536373839static voidtimeout_correct(struct event_base *base, struct timeval *tv){ struct event **pev; unsigned int size; struct timeval off; if (use_monotonic) //monotonic时间就直接返回，无需调整 return; /* Check if time is running backwards */ gettime(base, tv); //tv &lt;---tv_cache //如果tv &lt; event_tv表明用户向前调整时间了，需要校正时间 if (evutil_timercmp(tv, &amp;base-&gt;event_tv, &gt;=)) { base-&gt;event_tv = *tv; return; } event_debug((\"%s: time is running backwards, corrected\", __func__)); //计算时间差值 evutil_timersub(&amp;base-&gt;event_tv, tv, &amp;off); /* * We can modify the key element of the node without destroying * the key, beause we apply it to all in the right order. */ //调整定时事件小根堆 pev = base-&gt;timeheap.p; size = base-&gt;timeheap.n; for (; size-- &gt; 0; ++pev) { struct timeval *ev_tv = &amp;(**pev).ev_timeout; evutil_timersub(ev_tv, &amp;off, ev_tv); } /* Now remember what the new time turned out to be. */ base-&gt;event_tv = *tv; //更新event_tv为tv_cache} 在调整小根堆时，因为所有定时事件的时间值都会被减去相同的值，因此虽然堆中元素的时间键值改变了，但是相对关系并没有改变，不会改变堆的整体结构。因此只需要遍历堆中的所有元素，将每个元素的时间键值减去相同的值即可完成调整，不需要重新调整堆的结构。 当然调整完后，要将 event_tv 值重新设置为 tv_cache 值了。 4、小结主要分析了一下 libevent 对系统时间的处理，时间缓存、时间校正和定时堆的时间值调整等，逻辑还是很简单的,时间的加减、设置等辅助函数则非常简单，主要在头文件 evutil.h中，就不再多说了。","link":"/2019/05/12/libevent源码剖析-9-时间管理/"},{"title":"libevent源码剖析(1)--libevent简介及源码结构","text":"libevent简介及源码结构1、libevent简介libevent是一个C语言编写的轻量级的开源高性能网络库，其主要优点有如下几个： 事件驱动，高性能 轻量级，专注于网络 跨平台,支持 Windows、Linux、Mac Os等 支持多种 I/O多路复用技术, epoll、poll、dev/poll、select 和kqueue 等 支持 I/O,定时器和信号等事件 2、libevent源码结构Libevent 的源代码虽然都在一层文件夹下面,但是其代码分类还是相当清晰的,主要可分为头文件、内部使用的头文件、辅助功能函数、日志、libevent 框架、对系统 I/O 多路复用机制的封装、信号管理、定时事件管理、缓冲区管理、基本数据结构和基于 libevent 的两个实用库等几个部分,有些部分可能就是一个源文件。 1)头文件 主要就是 event.h:事件宏定义、接口函数声明,主要结构体 event 的声明; 2)内部头文件 xxx-internal.h:内部数据结构和函数,对外不可见,以达到信息隐藏的目的; 3)libevent 框架 event.c:event 整体框架的代码实现; 4)对系统 I/O 多路复用机制的封装 epoll.c:对 epoll 的封装;select.c:对 select 的封装;devpoll.c:对 dev/poll 的封装;kqueue.c:对 kqueue 的封装; 5)定时事件管理 min-heap.h:其实就是一个以时间作为 key 的小根堆结构; 6)信号管理 signal.c:对信号事件的处理; 7)辅助功能函数 evutil.h 和 evutil.c:一些辅助功能函数,包括创建 socket pair 和一些时间操作函数:加、减和比较等。 8)日志 log.h 和 log.c:log 日志函数 9)缓冲区管理 evbuffer.c 和 buffer.c:libevent 对缓冲区的封装; 10)基本数据结构 compat\\sys 下的两个源文件: queue.h 是 libevent 基本数据结构的实现,包括链表,双向链表,队列等;_libevent_time.h:一些用于时间操作的结构体定义、函数和宏定义; 11)实用网络库 http 和 evdns:是基于 libevent 实现的 http 服务器和异步 dns 查询库","link":"/2019/05/07/libevent源码剖析-libevent简介/"},{"title":"libevent源码剖析(2)--Reactor模式","text":"Reactor模式 整个 libevent 本身就是一个 Reactor，所以在剖析libevent源码之前需要对Reactor有一个深入的理解。 在事件驱动的应用中,同步地、有序地处理同时接收的多个服务请求。在分布式系统尤其是服务器这一类事件驱动应用中,虽然这些请求最终会被序列化地处理,但是必须时刻准备着处理多个同时到来的服务请求。在实际应用 中,这些请求总是通过一个事件(如CONNECTOR、READ、WRITE等)来表示的。在有序地处理这些服务请求之前,应用程序必须先分离和调度这些 同时到达的事件。 为了有效地解决这个问题,我们需要做到以下4方面:为了提高系统的可测量性和反应时间,应用程序不能长时间阻塞在某个事件源上而停止对其他事件的处理,这样会严重降低对客户端的响应度。为了提高吞吐量,任何没有必要的上下文切换、同步和CPU之间的数据移动都要避免。引进新的服务或改良已有的服务都要对既有的事件分离和调度机制带来尽可能小的影响。大量的应用程序代码需要隐藏在复杂的多线程和同步机制之后。 这就是Ractor模式要解决的问题。 1、Reactor事件处理机制首先来回想一下普通函数调用的机制:程序调用某函数函数执行，程序等待函数将结果和控制权返回给程序程序继续处理。 Reactor 释义“反应堆”,是一种事件驱动机制。和普通函数调用的不同之处在于:应用程序不是主动的调用某个 API 完成处理,而是恰恰相反,Reactor 逆置了事件处理流程,应用程序需要提供相应的接口并注册到 Reactor 上,如果相应的时间发生, Reactor 将主动调用应用程序注册的接口,这些接口又称为“回调函数”。使用 Libevent 也是想 Libevent 框架注册相应的事件和回调函数;当这些时间发声时,Libevent 会调用这些回调函数处理相应的事件(I/O 读写、定时和信号)。 用“好莱坞原则”来形容 Reactor 再合适不过了:不要打电话给我们,我们会打电话通知你。 举个例子:你去应聘某 xx 公司,面试结束后。 “普通函数调用机制”公司 HR 比较懒,不会记你的联系方式,那怎么办呢,你只能面试完后自己打电话去问结果;有没有被录取啊,还是被据了; “Reactor”公司 HR 就记下了你的联系方式,结果出来后会主动打电话通知你:有没有被录取啊,还是被据了;你不用自己打电话去问结果,事实上也不能,你没有HR的留联系方式。 2、Reactor模式的优点Reactor 模式是编写高性能网络服务器的必备技术之一,它具有如下的优点: 1)响应快,不必为单个同步时间所阻塞,虽然 Reactor 本身依然是同步的; 2)编程相对简单,可以最大程度的避免复杂的多线程及同步问题,并且避免了多线程/进程的切换开销; 3)可扩展性,可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源; 4)可复用性,reactor 框架本身与具体事件处理逻辑无关,具有很高的复用性; 3、Reactor模式框架 描述符(handle) 由操作系统提供,用于识别每一个事件,如Socket描述符、文件描述符等。在Linux中,它用一个整数来表示。事件可以来自外部,如来自客户端的连接请求、数据等。事件也可以来自内部,如定时器事件。 同步事件分离器(demultiplexer) 一个函数,用来等待一个或多个事件的发生。调用者会被阻塞,直到分离器分离的描述符集上有事件发生。Linux的select函数是一个经常被使用的分离器。 事件处理器接口(event handler) 由一个或多个模板函数组成的接口。这些模板函数描述了和应用程序相关的对某个事件的操作。 具体的事件处理器 事件处理器接口的实现。它实现了应用程序提供的某个服务。每个具体的事件处理器总和一个描述符相关。它使用描述符来识别事件、识别应用程序提供的服务。 Reactor 管理器(reactor) 定义了一些接口,用于应用程序控制事件调度,以及应用程序注册、删除事件处理器和相关的描述符。它是事件处理器的调度核心。 Reactor管理器使用同步事件分离器来等待事件的发生。一旦事件发生,Reactor管理器先是分离每个事件,然后调度事件处理器,最后调用相关的模 板函数来处理这个事件。 通过上述分析,我们注意到,是Reactor管理器而不是应用程序负责等待事件、分离事件和调度事件。实际上,Reactor管理器并没有被具体的事件处理器调用,而是管理器调度具体的事件处理器,由事件处理器对发生的事件做出处理。这就是类似Hollywood原则的“反向控制”。应用程序要做的 仅仅是实现一个具体的事件处理器,然后把它注册到Reactor管理器中。接下来的工作由管理器来完成。 这些参与者的相互关系如下图所示。 4、Reactor典型启动过程Reator模式的典型启动过程如下： (1)创建Reactor (2)注册事件处理器（Reactor::register_handler()） (3)调用事件多路分发器进入无限事件循环（Reacor:handle_events） (4)当操作系统通知某描述符状态就绪时，事件多路分发器找出并调用此描述符注册的事件处理器。","link":"/2019/05/07/libevent源码剖析-Reactor模式/"},{"title":"libevent源码剖析(8)--支持I/O多路复用","text":"Libevent 的核心是事件驱动、同步非阻塞,为了达到这一目标,必须采用系统提供的 I/O多路复用技术,而这些在Windows、Linux、Unix 等不同平台上却各有不同,如何能提供优雅而统一的支持方式,是首要关键的问题,这其实不难,本节就来分析一下。 1、统一的关键Libevent支持多种I/O多路复用技术的关键就在于结构体eventop,这个结构体前面也曾提到过,它的成员是一系列的函数指针, 定义在event-internal.h文件中: 12345678910struct eventop { const char *name; void *(*init)(struct event_base *); //初始化 int (*add)(void *, struct event *); //注册事件 int (*del)(void *, struct event *); //删除事件 int (*dispatch)(struct event_base *, void *, struct timeval *); //事件分发 void (*dealloc)(struct event_base *, void *); //注销，释放资源 /* set if we need to reinitialize the event base */ int need_reinit;}; 在 libevent 中,每种 I/O demultiplex 机制的实现都必须提供这五个函数接口,来完成自身的初始化、销毁释放;对事件的注册、注销和分发。 比如对于 epoll,libevent 实现了 5 个对应的接口函数,并在初始化时并将 eventop 的 5个函数指针指向这 5 个函数,那么程序就可以使用 epoll 作为 I/O demultiplex 机制了。 2、设置 I/O demultiplex 机制Libevent 把所有支持的 I/O demultiplex 机制存储在一个全局静态数组 eventops 中,并在初始化时选择使用何种机制,数组内容根据优先级顺序声明如下: 123456789101112131415161718192021222324static const struct eventop *eventops[] = {#ifdef HAVE_EVENT_PORTS &amp;evportops,#endif#ifdef HAVE_WORKING_KQUEUE &amp;kqops,#endif#ifdef HAVE_EPOLL &amp;epollops,#endif#ifdef HAVE_DEVPOLL &amp;devpollops,#endif#ifdef HAVE_POLL &amp;pollops,#endif#ifdef HAVE_SELECT &amp;selectops,#endif#ifdef WIN32 &amp;win32ops,#endif NULL}; 然后 libevent 根据系统配置和编译选项决定使用哪一种 I/O demultiplex 机制,这段代码在函数event_base_new()中: 1234567//初始化决定使用哪种I/O demultiplex机制 base-&gt;evbase = NULL; for (i = 0; eventops[i] &amp;&amp; !base-&gt;evbase; i++) { base-&gt;evsel = eventops[i]; //确定使用哪种I/O demultiplex机制 base-&gt;evbase = base-&gt;evsel-&gt;init(base); //实例化一个evsel，例如evsel为epoll，相当于调用epoll的init函数创建一个epoll实例 } 可以看出,libevent 在编译阶段选择系统的 I/O demultiplex 机制,而不支持在运行阶段根据配置再次选择。 以 Linux 下面的 epoll 为例,实现在源文件 epoll.c 中,eventops 对象 epollops 定义如下: 123456789const struct eventop epollops = { \"epoll\", epoll_init, epoll_add, epoll_del, epoll_dispatch, epoll_dealloc, 1 /* need reinit */}; 变量 epollops 中的函数指针具体声明如下,注意到其返回值和参数都和 eventop 中的定义严格一致,这是函数指针的语法限制。 12345static void *epoll_init (struct event_base *);static int epoll_add (void *, struct event *);static int epoll_del (void *, struct event *);static int epoll_dispatch (struct event_base *, void *, struct timeval *);static void epoll_dealloc (struct event_base *, void *); 那么如果选择的是 epoll,那么调用结构体 eventop 的 init 和 dispatch 函数指针时,实际调用的函数就是 epoll 的初始化函数 epoll_init()和事件分发函数 epoll_dispatch()了; 同样的,上面 epollops 以及 epoll 的各种函数都直接定义在了 epoll.c 源文件中,对外都是不可见的。对于 libevent 的使用者而言,完全不会知道它们的存在,对 epoll 的使用也是通过 eventop 来完成的,达到了信息隐藏的目的。 3、小结支持多种 I/O demultiplex 机制的方法其实挺简单的,借助于函数指针就 OK 了。通过对源代码的分析也可以看出,Libevent 是在编译阶段选择系统的 I/O demultiplex 机制的,而不支持在运行阶段根据配置再次择。","link":"/2019/05/10/libevent源码剖析-8-支持I-O多路复用/"},{"title":"《现代操作系统》学习笔记(1)--引言","text":"引言多数计算机有两种运行模式：内核态和用户态。软件中最基础的部分是操作系统，它运行在内核态（也称为管态、核心态）。在这个模式中，操作系统具有对所有硬件的完全访问权，可以执行机器能够运行的任何指令。软件的其余部分运行在用户态。在用户态下，只使用了机器指令中的一个子集。 操作系统所在的位置如下图所示。 1、什么是操作系统操作系统是一种运行在内核态的软件（并不总是符合事实）。 对于用户（应用程序）来说，操作系统的一个主要任务是隐藏硬件，呈现给程序（程序员）良好、清晰、优雅、一致的抽象（直接操作硬件难，所以操作系统做了抽象）。把操作系统看作向应用程序提供抽象的概念，是一种自顶向下的观点。 按照自底向上的观点，操作系统则用来管理一个复杂系统的各个部分。计算机有CPU、存储器等各种资源和设备。从这个角度看，操作系统的任务是在相互竞争的程序之间有序地控制对处理器、存储器等资源和设备的分配。 2、计算机硬件组成一个典型的计算机组成如下图所示（图来自深入理解计算机系统）","link":"/2019/06/09/《现代操作系统》学习笔记-1-引言/"},{"title":"《现代操作系统》学习笔记(2)--进程","text":"进程（1）进程本质上是正在执行的一个程序。 与每个进程相关的是地址空间， 该地址空间中存放有可执行程序、程序的数据以及程序的堆栈。进程基本上是容纳运行一个程序所需要所有信息的容器。 （2）严格地说，在某一瞬间，CPU只能运行一个进程。 （3）一个进程就是一个正在执行的实例，包括程序计数器、寄存器和变量的当前值。 （4）真正的CPU在各进程间来回切换，这种快速的切换称作多道程序设计。在下图中，每道程序有自己的逻辑计数器，当该程序运行时，它的逻辑计数器装入实际的程序计数器。在任何给定的瞬间仅有一个进程真正地运行。即使有两个核（或CPU），每一个核也只能一次运行一个进程。 （5）由于CPU在各进程之间来回快速切换，所以每个进程执行其运算的速度是不确定的。而且当同一进程再次运行时，其运算速度通常也不再可现。所以，在对进程编程时决不能对时序做任何想当然的假设。 （6）一个进程是某种类型的一个活动，它有程序、输入、输出以及状态。如果一个程序运行了两遍，则算作两个进程，操作系统能够使他们共享代码，因此只有一个副本放在内存中。 （7）4种主要事件会导致进程的创建： 系统初始化 正在运行的程序执行了创建进程的系统调用 用户请求创建一个新进程 一个批处理作业的初始化 （8）守护进程：启动操作系统时创建的后台进程，例如处理电子邮件、web页面、新闻、打印之类活动的进程。 （9）在调用fork后，这两个进程（父进程和子进程）拥有相同的内存映像、同样的环境字符串和同样的打开文件。但是有各自不同的地址空间。 （10）进程的终止通常由下列条件引起： 正常退出（自愿的）； 出错退出（自愿的）； 严重错误（非自愿）； 被其他进程杀死（非自愿）。 （11）进程只有一个父进程（但是可以有零个、一个、两个或多个子进程）。 （12）进程的状态: 1)运行态（该时刻进程实际占用CPU）。 2)就绪态（可运行，但因为其他进程正在运行而暂时停止）。 3)阻塞态（除非某种外部事件发生，否则进程不能运行）。 前两种状态的进程都可以运行，只是对于第二种状态暂时没有CPU分配给它。第三种状态即使CPU空闲进程也不能运行。调度程序的主要工作就是决定应当运行哪个进程、何时运行及它应该运行多长时间。 在图2-3中，操作系统的最底层是调度程序，在它上面有许多进程。所有关于中断处理、启动进程和停止进程的具体细节都隐藏在调度程序中。调度程序是一段非常短小的程序。 （13）进程表：为了实现进程模型，操作系统维护着一张表格（一个结构数组），即进程表。每个进程占用一个进程表项。（有些作者称这些表项为进程控制块。该表项包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其他在进程由运行状态转换到就绪态或阻塞态时必须保存的信息。 (14) 与每一I/O类关联的是一个称作中断向量的位置（靠近内存底部的固定区域）。它包含中断服务程序的入口地址。 一个进程在执行过程中可能被中断数千次，但关键是每次中断后，被中断的进程都返回到与中断生前完全相同的状态。","link":"/2019/06/09/《现代操作系统》学习笔记-2-进程/"},{"title":"《现代操作系统》学习笔记(3)--线程","text":"线程在传统的操作系统中，每个进程有一个地址空间和一个控制线程。经常存在在同一个地址空间中准并行运行多个控制线程的情形，这些线程就像（差不多）分离的进程（共享地址空间除外）。 （1）为什么使用线程 主要原因是，在许多应用中同时发生着多种活动，其中某些活动随着时间的推移会被阻塞。通过将这些应用程序分解成可以准并行运行的多个顺序线程，程序设计模型会变得简单。并行实体拥有共享同一个地址空间和所有可用数据的能力。对于某些应用，这种能力是必须的，而这正是多进程模型（具有不同的地址空间）所无法表达的。 由于线程比进程更轻量级，所以它们比进程更容易（即更快）创建，也更容易撤销。 若多个线程都是CPU密集型的，那么并不能获得性能上的增强，但是如果存在着大量的计算和大量的I/O处理，拥有多个线程允许这些活动彼此重叠进行，从而加快应用程序执行的速度。 在多CPU系统中，多线程是有益的，真正的并行有了实现的可能。 PS: CPU密集型： 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 I/O密集型： 涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 例子：字符处理软件，一个程序，多个线程，一个线程用于和用户交互，一个用于后台处理文字，一个处理磁盘备份，三个线程互不干扰。 多线程使得顺序进程的思想得以保留下来，这种顺序进程阻塞了系统调用（如磁盘I/O），但是仍旧实现了并行性。 （2）经典的线程模型 进程模型基于两种独立的概念：资源分组处理与执行。 理解进程的一个角度是，用某种方法把相关的资源集中在一起。 另一个概念是，进程拥有一个执行的线程，通常简写为线程。 进程用于把资源集中到一起，而线程则是在CPU上被调度执行的实体。进程是资源分配和调度的基本单元，线程是CPU调度的基本单元。 CPU在线程之间快速切换，制造了线程并行运行的假象，好似它们在一个比实际CPU慢一些的CPU上同时运行。在一个有三个计算密集型线程的进程中，线程以并行方式运行，每个线程在一个CPU上得到了真实CPU速度的三分之一。 线程有完全一样的地址空间，线程共享内容如下图所示： 线程概念试图实现的是，共享一组资源的多个线程的执行能力，以便这些线程可以为完成某一任务而共同工作。 线程可以处于若干种状态的任何一个：运行、阻塞、就绪或终止，线程状态之间的转换和进程之间的转换是一样的。 每个线程有其自己的堆栈，通常每个线程会调用不同的过程（函数），从而各自有一个不同的执行历史。 创建线程通常都返回一个线程标识符，该标识符就是新线程的名字。在某些线程系统中，通过调用一个过程，例如thread_join，一个线程可以等待一个（特定）线程退出。thread_yield，允许线程自动放弃CPU而让另一个线程运行，因为不同于进程，（线程库）无法利用时钟中断强制线程让出CPU。 （3）POSIX线程 （4）在用户空间中实现线程 有两种主要的方法实现线程包：在用户空间中和内核中。 第一种方法是把整个线程包放在用户空间中，内核对线程包一无所知。从内核的角度，就是正常的方式管理，即单线程进程。有个明显的优点，用户级线程包可以在不支持线程的操作系统上实现。 由库函数实现线程就是这一方法。线程在一个运行时系统的上层运行，该运行时系统是一个管理线程的过程（函数）的集合，例如pthread_create,pthread_exit,pthread_join等，如下图所示： 在用户管理线程时，每个进程需要有其专用的线程表，用来跟踪该进程中的线程，与内核中的进程表类似。由运行时系统管理。线程切换至少比陷入内核要快一个数量级（或许更多），这是用户级线程包的极大优点。 在一个线程完成运行时，保存该线程状态的过程和调度程序都只是本地过程，所以启动它们比进行内核调用效率更高。另一方面，不需要陷入内核，不需要上下文切换，也不需要对内存高速缓存进行刷新，使得线程调度非常快。 用户级线程还有另一个优点。它允许每个进程有自己定制的调度算法。用户线程还具有较好的可扩展性。 用户级线程存在的问题： 如何实现阻塞系统调用。使用线程的一个主要目标是，首先要允许每个线程使用阻塞调用，但是还要避免被阻塞的线程影响其他的线程。 如果一个线程开始运行，那么在该进程中的其他线程就不能运行，除非第一个线程自动放弃CPU。 （5）在内核中实现线程： 在内核中有用来记录操作系统中所有线程的进程表。当创建或撤销一个已有线程时，进行一个系统调用，这个系统调用通过对线程表的更新完成线程创建和撤销工作。 当一个线程阻塞时，内核根据其选择，可以运行同一个进程中的另一个线程（若有一个就绪线程）或者运行另一个进程中的线程。 内核中创建或撤销线程的代价比较大，某些系统可以回收线程。当某个线程被撤销时，标记为不可运行的，但是其内核数据结构没有受到影响，创建新线程时就重启某个旧线程。 （6）混合实现 （7）调度程序激活机制 调度程序激活工作的目标是模拟内核线程的功能，但是为线程包提供通常在用户空间中才能实现的更好的性能和更大的灵活性。特别地，如果用户线程从事某种系统调用时是安全的，那就不应该进行专门的非阻塞调用或者进行提前检查。无论如何，如果线程阻塞在某个系统调用或页面故障上，只要在一个进程中有任何就绪的线程，就应该有可能运行其他线程。 （8）弹出式线程 一个消息的到达导致系统创建一个处理该消息的线程，这种线程称为弹出式线程。每个线程都是新的，有可能快速创建这类线程。消息到达与处理开始之间的时间非常短。 面试题：","link":"/2019/06/12/《现代操作系统》学习笔记-3-线程/"},{"title":"《现代操作系统》学习笔记(5)--调度","text":"4、调度多个进程或线程同时竞争cpu。完成选择工作的称为调度程序，该程序使用的算法称为调度算法。 1.调度简介(1)进程行为几乎所有进程的（磁盘或网络）I/O请求和 计算都是交替突发的。某些I/O活动可以看作计算。当一个进程等待外部设备完成工作而被阻塞时，才是I/O活动 某些进程花费大多数时间在计算上，称为计算密集型。某些进程花费大多数时间在等待I/O上，称为I/O密集型。 随着cpu变得越来越快，更多的进程倾向为I/O密集型。如果需要运行I/O密集型进程，你那么就应该让它尽快得到机会，以便发出磁盘请求并保持磁盘始终忙碌。如果进程是I/O密集型的，则需要多运行一些这类进程以保持cpu的充分利用。 (2)何时调度有关调度处理的一个关键问题是何时进行调度决策 第一，在创建一个新进程后，需要决定是运行父进程还是运行子进程。 第二，在一个进程退出时必须做出调度决策。 第三，当一个进程阻塞在I/O和信号量上或由于其他原因阻塞时，必须选择另一个进程运行。 第四，在一个I/O中断发生时，必须做出调度决策。 根据如何处理时钟中断，可以把调度算法分为两类： 非抢占式：该调度算法挑选一个进程，然后让其运行至被阻塞，或者直到自动释放CPU。在时钟中断发生时不会不会进行调度。 抢占式：该调度算法挑选一个进程，让其运行某个固定时段的最大值。如果该时段结束时，进程仍在运行，则挂起，调度程序挑选另一个进程运行。 (3)调度算法分类不同环境需要不同的调度算法： 批处理：在商业领域广泛应用。可以用非抢占式算法，或对每个进程都有长时间周期的抢占式算法。这种处理方式减少了进程的切换从而改善了性能。 交互式：为避免一个进程霸占CPU拒绝为其他进程服务，抢占是必须的。服务器也一样。 实时：抢占有时不是必需的，因为进程了解它们可能会长时间得不到运行。实时系统只运行哪些用来推进现有应用的程序，而交互式系统是通用的，它可以运行任意非协作甚至是有而已的程序。 (4)调度算法的目标 在以上情况中，公平是最重要的。与公平有关的是系统策略的强制执行。 另一个共同的目标是保持系统的所有部分尽可能忙碌。 对于批处理系统：CPU并不是一个很好的度量参数，真正有价值的是，吞吐量和周转时间。 对于交互式系统：最重要的是最小响应事件。 对于实时系统：或多或少必须满足截止时间。 2.批处理系统中的调度(1)先来先服务(2)最短作业优先该算法适用于运行时间可以预知的另一个非抢占式的批处理调度算法。 如上图，a的平均周转时间为14分钟，b的平均周转时间为11分钟。只有在所有的作业都可同时到来的情况下，最短作业优先才是最优化的。 (3)最短剩余时间优先该算法是最短作业优先的抢占式版本，选择剩余运行时间最短的那个进程，如果有新的进程比当前进程需要更少的时间，当前进程就挂起，运行新进程。 3.交互式系统中的调度(1)轮转调度(round robin)每个进程被分配一个时间段，称为时间片，即允许该进程在该时间段中运行。调度程序所要做的就是维护一张可运行进程列表。 从一个进程切换到另一个进程是需要一定时间进行管理事务处理——保存和装入寄存器值及内存影响，更新 各种表格和列表、清除和重新调入内存告诉缓存等。 进程切换也称为上下文切换。 时间片设得太短会导致过多的进程切换，降低了CPU效率；而设得太长又可能引起对短的交互请求的响应时间变长。将时间片设为20~50ms通常是一个比较合理的折中。 轮转调度假设所有的进程同等重要。 (2)优先级调度基本思想：每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。 为防止高优先级进程无休止运行，调度程序可能在每个时钟滴答（时钟中断）降低当前进程的优先级。或者给每个进程赋予一个允许运行的最大时间片。 优先级可以静态赋予或动态赋予。为达到某种目的，优先级也可以由系统动态确定。 e.g.有些进程为I/O密集型，多数事件用来等待IO结束。这样的进程需要CPU时，应立即分配，以便启动下一个I/O请求，这样在另一个进程计算的同时执行I/O操作。一种较简单的算法是，将其优先级设为1/f，f为该进程在上一事件片中所占的部分。 可以将一组进程分成若干类，并在各类之间采用优先级调度，而在各类进程的内部采用轮转调度。 (3)多级队列CPU密集型进程设置较长的时间片比频繁地分给它们很短的时间要更为高效（减少交换次数）。另一方面，长时间片的进程又会影响到响应时间。 解决办法是设立优先级类。属于最高优先级类的进程运行一个时间片，属于次高优先级类的进程运行2个时间片…4..以此类推。当一个进程运行完分配的时间后，被移到下一级。 (4)最短进程优先交互进程通常遵循下列模式：等待命令、执行命令、等待命令、执行命令，如此反复，把命令的执行看成”作业”，运行最短的作业来使响应事件对端。 (5)保证调度(6)彩票调度为进程提供各种系统资源（如CPU时间）的彩票。需要调度决策时，抽一张彩票，拥有该彩票的进程就获得资源。 (7)公平分享调度4.实时系统中的调度实时系统是一种时间起着主导作用的系统。 实时系统通常分为硬实时和软实时，前者的含义是必须满足绝对的截止时间，后者的含义是虽然不希望偶尔错失截止时间，但可以容忍。 实时系统的调度算法可以是静态或动态的。 5.策略和机制调度机制和调度策略分离，将调度算法以某种形式参数化，而参数可以由用户进程填写。调度机制位于内核，而调度策略则由用户进程决定。 6.线程调度 上图中，a为用户级线程，当内核选取一个进程后，运行时系统选取要运行的线程。b为内核级线程，内核选择一个特定的线程运行，上图中，b中的A1,B1,A2,B2,A3,B3这种情况是可能的，图中有错误。 用户级线程和内核级线程之间的差别在于性能。","link":"/2019/06/19/《现代操作系统》学习笔记-5-调度/"},{"title":"《现代操作系统》学习笔记(4)--进程间通信","text":"进程间通信（IPC）有三个问题： 一个进程如何把信息传递给另一个。 确保两个或更多的进程在关键活动中不会出现交叉。 第三个问题与正确的顺序有关（如果该顺序是有关联的话）。 下面讨论进程剑通信问题，（1）-（7）针对的是共享内存，（8）是消息传递系统 （1）竞态条件在一些操作系统中，协作的进程可能共享一些彼此都能读写公用存储区，这个存储区可能在内存中（可能是在内核数据结构中），也可能是一个共享文件。 两个或多个进程读写某些共享进程，而最后的结果取决于进程运行的精确时序，称为竞态条件。 （2）临界区互斥：以某种手段确保当一个进程在使用一个共享变量或文件时，其他进程不能做同样的操作。 临界区：对共享内存进行访问的程序片段。 对于一个好的解决方案，需要满足以下4个条件： 任何两个进程不能处于其临界区。 不应对CPU的速度和数量做任何假设。 临界区外运行的进程不得阻塞其他进程。 不得使进程无限期等待进入临界区。 （3）忙等待的互斥几种实现互斥的方案： A.屏蔽中断屏蔽中断对于操作系统本身而言是一项很有用的技术，但对于用户进程则不是一种合适的通用互斥机制。 B.锁变量 C.严格轮换法连续测试一个变量直到某个值出现为止，称为忙等待，用于忙等待的锁称为自旋锁。一个进程比另一个进程慢很多的时候，进程被临界区之外的进程阻塞。 D.Peterson解法 E.TSL指令 （4）睡眠与唤醒生产者-消费者模型（有界缓冲区）： 两个进程共享一个固定大小的缓冲区，一个是生产则，将信息放入缓冲区；另一个是消费者，从缓冲区取出信息。 （5）信号量进化版的互斥量mutex，两个操作 ，P（down）和V（up）操作。在进入临界区前，执行P操作，信号量减1；在离开临界区前，执行V操作，信号量加1。操作信号量是原子操作，是指一组相关联的操作要么不间断地执行，要么都不执行。 (6）互斥量信号量的一个简化版本，可以处于两态：解锁和加锁。当一个线程（或进程）需要访问临界区时，它调用mutex_lock。 如果该互斥量已经加锁，调用线程被阻塞，直到在临界区中的线程完成并调用mutex_unlock。如果多个线程被阻塞在互斥量上，将随机选择一个线程并允许它获得锁。 互斥量和自旋锁的区别（阻塞对比忙等待）：使用互斥量时，互斥量已加锁，调用线程会被阻塞，然后调用thread_yield将CPU释放给另一个线程。 使用自旋锁，在获取锁之前一直处于忙等待（自旋），一直占用CPU，如果，短时间内无法获得锁，则会造成CPU浪费。 所以，自旋锁适用于：锁被持有的时间短，并且线程并不希望在重新调度上花费太多的成本。 而使用互斥量阻塞时，不会浪费CPU，但是阻塞后重新调用会花费成本。 a.快速用户区互斥量futex b.pthread中的互斥量 pthread提供了互斥量操作，还提供了条件变量。条件变量允许线程由于一些未达到的条件而阻塞，一般和互斥量一起使用。（生产者拿到锁以后，发现缓冲区满的，利用条件变量，等待消费者消费，线程阻塞，当消费者消费后，唤醒自己）。 pthread_cond_wait原子性地调用并解锁它持有的互斥量。以下是互斥量与条件变量一起使用的生产者-消费者模型。 （7）管程一个管程是一个由过程、变量及数据结构等组成的一个集合，他们组成一个特殊的模块或软件包。管程是语言概念，JAVA支持。管程有一个很重要的特性，即任一时刻管程中只能有一个活跃进程，这一特性使管程能够有效地完成互斥。 （8）消息传递这种进程间通信的方法使用两条原语send和receive，它们像信号量而不像管程，是系统调用而不是语言成分。 （9）屏障当一个进程到达屏障时，它就被阻拦，直到所有进程都到达该屏障。屏障可用于一组进程同步。 （10）避免锁：读-复制-更新","link":"/2019/06/15/《现代操作系统》学习笔记-4-进程间通信/"},{"title":"《现代操作系统》学习笔记(6)--死锁","text":"死锁1、资源资源就是随着时间的推移，必须能够获得、使用以及释放的任何东西。 1.1可抢占资源和不可抢占资源 可抢占资源：可以送拥有它的进程中抢占而不会产生任何副作用，例如存储器。 不可抢占资源：指在不引起相关的计算失败的情况下，无法把它从战占有它的进程出抢占过来。 死锁与不可抢占资源有关。 使用一个资源所需要的事件顺序可以抽象表示为如下： 1)请求资源。。 2)使用资源。 3)释放资源。 1.2资源获取一种允许用户管理资源的可能方法是为每一个资源配置一个信号量。 如果进程需要两个以上的资源，通常都是连续获取。 2、死锁简介死锁的规范定义如下：如果一个进程集合中的每个进程都在等待只能由该进程集合中的其他进程才能引发的事件，那么该进程集合就是死锁的。 大多数情况下，死锁进程集合中的每一个进程都在等待另一个死锁的进程已经占有的资源，所有进程都不能运行，就无法释放资源，就不会被唤醒。这种死锁称为资源死锁。 2.1资源死锁的条件 互斥条件。每个资源要么已经分配给了一个进程，要么就是可用的 占有和等待条件。 不可抢占条件。 环路等待条件。 破坏上述条件来预防死锁 2.2死锁建模 方框代表资源，圆圈代表进程，方框指向进程代表资源被该进程占用，圆圈指向资源，代表进程请求该资源并已被阻塞。 在上图中，假设系统知道有引起死锁的可能，那么系统不把资源S分配给B，B被挂起，就不会出现死锁。 上图中，按请求和释放的次序一步步进行，每一步检测图中是否有环路，有环路，就有死锁。 四种处理死锁的策略： 忽略该问题。 检测死锁并恢复。让死锁发生，检测是否发生，一旦发生死锁，采取行动解决问题。 仔细对资源进行分配，动态地避免死锁。 破坏死锁的四个必要条件，防止死锁的产生。 3、鸵鸟算法不考虑死锁问题。 因为实际工作中很可能有其他更为严重的问题。如果死锁的频率不高可不考虑死锁。 鸵鸟算法的实质：出现死锁的概率很小，并且出现之后处理死锁会花费很大的代价，还不如不做处理，OS中这种置之不理的策略称之为鸵鸟算法。所以鸵鸟算法，是平衡性能和复杂性而选择的一种方法。 4、死锁检测和死锁恢复系统允许死锁发生，当检测到死锁发生后，采取错失进行恢复。 4.1每种类型一个资源的死锁检测每种资源类型只有一个资源。 检测上面有向图中有无环路的算法步骤如下： 这一算法是一次将每一个节点作为一棵树的根节点，并进行深度优先搜索。如果碰到已经遇到过的节点，那么就算找到了一个环。如果从任何给定的节点触发的弧都被穷举了，那么就回溯到前面的节点。如果回溯到根并且不能再深入下去，那么从当前节点出发的子图中就不包含任何环。如果所有的节点都是如此，那么整个图就不存在环，也就是说系统不存在死锁。 不是最佳算法。 4.2每种类型多个资源的死锁检测E是现有资源向量，代表每种已存在的资源总数。 A是可用资源向量，Ai表示当前可供使用的资源数。 C代表当前分配矩阵，C的第i行代表Pi当前所持有的每一种类型资源的资源数。 R代表请求矩阵，Rij代表Pi所需要的资源j的数量。 4.3从死锁中恢复(1)利用抢占恢复 临时将某个资源从它当前所有者那里转移给另一个进程。 (2)利用回滚恢复 将进程复位到一个更早的状态。 (3)通过杀死进程恢复 杀掉环中的一个进程或多个进程 选一个环外的进程作为牺牲品释放该进程的资源 5、死锁避免5.1资源轨迹图 在t点处唯一的办法是运行进程A直到I4。 5.2安全状态和不安全状态 安全状态： 如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕则称该状态是安全的。 不安全状态 不安全状态不是死锁。 安全状态和不安全状态的区别是：从安全状态出发，系统能够保证所有进程都能完成；而从不安全状态出发，就没有这样的保证。 存在一个分配序列使得所有的进程都能完成。 不能保证多有进程都能完成。 5.3单个资源的银行家算法 银行家算法就是对每一个请求进行检查，检查如果满足这一请求是否会达到安全状态。若是，那么就满足该请求，否则就推迟对这一请求的满足。 5.4多个资源的银行家算法略 实际中很少用，因为很少有进程能够在运行前就直到其所需资源的最大值。 6、死锁预防死锁避免从本质上来说是不可能的。可以破坏死锁条件中的一个来预防死锁。 破坏互斥条件 破坏占有并等待条件 禁止已持有资源的进程再等待其他资源便可以消除死锁。一种方法是进程执行前请求所需的全部资源，另一种方法是请求资源时先释放所占有资源，再尝试一次获得所需的全部资源。 破坏不可抢占条件 破坏环路等待条件 7、其他问题7.1两阶段加锁7.2通信死锁资源死锁是竞争性同步问题。 通信死锁：进程A向B发送请求信息，然后阻塞直至B回复，假设请求信息丢失，A将阻塞以等待回复，而B会阻塞等待一个向其发送命令的请求，发生死锁。 通信死锁是协同同步的异常情况。解决办法可以是：超时。 7.3活锁活锁指的是任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试—失败—尝试—失败的过程。处于活锁的实体是在不断的改变状态，活锁有可能自行解开。 进程1可以使用资源，但它很礼貌，让其他线程先使用资源，进程2也可以使用资源，但它很绅士，也让其他线程先使用资源。这样你让我，我让你，最后两个线程都无法使用资源。 活锁没有进程阻塞。 7.4饥饿指如果线程T1占用了资源R，线程T2又请求封锁R，于是T2等待。T3也请求资源R，当T1释放了R上的封锁后，系统首先批准了T3的请求，T2仍然等待。然后T4又请求封锁R，当T3释放了R上的封锁之后，系统又批准了T4的请求……，T2可能永远等待。饥饿可以通过先来先服务资源分配策略来避免。","link":"/2019/06/20/《现代操作系统》学习笔记-6-死锁/"},{"title":"多态原理探究","text":"多态原理探究1、多态的实现原理 当类中声明虚函数时，编译器会在类中生成一个虚函数表 虚函数表是一个存储类成员函数指针的数据结构 虚函数表是由编译器自动生成与维护的 virtual成员函数会被编译器放入虚函数表中 存在虚函数时，每个对象中都有一个指向虚函数表的指针(vptr指针) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Parent{public: Parent(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } virtual void print(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"Parent..\"&lt;&lt;endl; }private: int a;};class Child:public Parent{public: Child(int a=0,int b=0):Parent(a),b(b){ cout&lt;&lt;\"儿子的构造函数\"&lt;&lt;endl; } virtual void print(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"Child..\"&lt;&lt;endl; }private: int b;};//证明vptr指针的存在class Parent2{public: Parent2(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } void print1(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 cout&lt;&lt;\"我是爹。。。\"&lt;&lt;endl; }private: int a;};class Parent3{public: Parent3(int a=0):a(a){ cout&lt;&lt;\"父亲的构造函数\"&lt;&lt;endl; } virtual void print2(){ //动手脚1 写virtual关键字 会特殊处理 //虚函数表 // cout&lt;&lt;\"Child..\"&lt;&lt;endl; }private: int a;};void Objplay(Parent *base){ base-&gt;print(); //有多态发生 //动手脚2 //效果：传来子类时 执行子类的print函数 传来父类时执行父类的print函数 //C++编译器根本不需要区分是子类对象还是父类对象 //父类对象和子类对象分别有vptr指针，==&gt;虚函数表===&gt;函数的入口地址 //迟绑定（运行的时候，C++编译器才去判断)}int main(){ Parent p1; //动手脚3 用类定义对象的时候 C++编译其会在对象中添加一个vptr指针 Child c1; //子类里面也有一个vptr指针 Objplay(&amp;p1); Objplay(&amp;c1); //证明vptr指针的存在 cout&lt;&lt;\"证明vptr指针的存在\"&lt;&lt;endl; cout&lt;&lt;endl; cout&lt;&lt;\"sizeof(Parent2): \"&lt;&lt;sizeof(Parent2)&lt;&lt;endl; cout&lt;&lt;\"sizeof(Parent3): \"&lt;&lt;sizeof(Parent3)&lt;&lt;endl; std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;}/* 说明1： 通过虚函数表指针VPTR调用重写函数是在程序运行时进行的，因此需要通过寻址操作才能确定真正应该调用的函数。 而普通成员函数是在编译时就确定了调用的函数。在效率上，虚函数的效率要低很多。 说明2： 出于效率考虑，没必要将所有成员函数都声明为虚函数*/ 2、vptr指针的分步初始化vptr指针的初始化是分步的，当对象在创建的时候，由编译器对vptr指针进行初始化。只有当对象的构造完全结束后vptr的指向才最终确定。父类对象的vptr指向父类虚函数表，子类对象的vptr指向子类虚函数表。有如下代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std;class Parent{public: Parent(int a=0):a(a){ cout&lt;&lt;\"Parent..\"&lt;&lt;endl; print();//是调用父类print函数 } virtual void print(){ cout&lt;&lt;\"Parent::print()\"&lt;&lt;endl; }private: int a;};class Child:public Parent{public: Child(int a=0,int b=0):Parent(a),b(b){ cout&lt;&lt;\"Child..\"&lt;&lt;endl; } virtual void print(){ cout&lt;&lt;\"Child::print()\"&lt;&lt;endl; }private: int b;};int main(){ /* 1、要初始化c1.vptr，初始化是分步 2、当执行父类的构造函数时，c1.vptr指向父类的虚函数表 当父类的构造函数运行完毕后，会指向子类的虚函数表 3、结论：子类的c1.vptr指针分步完成 ===&gt;子类对象构造时，在父类的构造函数调用虚函数，产生不了多态。 在子类的父类构造期间，对象的类型是父类而不是子类 */ Child c1; std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;} 在上述代码中，定义c1对象时，执行的是父类的print函数，这是因为在子类对象构造时，要先调用父类的构造函数，此时c1.vptr指向父类的虚函数表，所以在父类的构造函数中调用print函数执行的是父类的print函数，并不会产生多态。因此， 不要在构造函数和析构函数中调用虚函数 。","link":"/2019/03/18/多态原理探究/"},{"title":"计算机网络--http协议","text":"HTTP1、HTTP概况Web的应用协议是超文本传输协议（HyperText Transfer Protocol，HTTP），是Web的核心。HTTP由两个程序实现：一个客户程序和一个服务器程序。 Web页面：由对象组成 对象：一个文件，诸如HTML文件、一个JPEG图形。。可通过URL地址寻址 URL：统一资源定位符，互联网上的资源的地址，一般形式如下： &lt;协议&gt;://&lt;主机&gt;:&lt;端口&gt;/&lt;路径&gt; Web服务器：HTTP的服务端，用于存储Web对象，每个对象由URL寻址 HTTP使用TCP作为它的支撑运输协议（而不是UDP上运行）。 TCP为HTTP提供可靠数据传输服务。意味着HTTP请求能完整到达服务器，HTTP响应能完整到达客户。这里可以看出分层的最大优点：HTTP协议不用担心数据丢失，也不关注TCP从网络的数据丢失和乱取故障中恢复的细节。 HTTP服务器并不保存关于客户的任何信息，所以HTTP是一个无状态协议。 2、非持续连接和持续连接 非持续连接：每个请求/响应对是经一个单独的TCP连接发送。 持续连接：所有的请求及其响应经过相同的TCP连接发送。 2.1采用非持续连接的HTTP 以上步骤中，每个TCP连接在服务器发送一个对象后关闭，即该连接并不为其他的对象而持续下来。每个TCP连接只传输一个请求报文和一个响应报文。 往返时间（Round-Trip Time，RTT）：该时间是指一个短分组从客户到服务器然后再返回客户所话费的时间。RTT包括分组传播时延、分组在中间路由器和交换机上的排队时延。 粗略地讲，总的响应时间就是两个RTT加上服务器传输HTML文件的时间。 非持续连接的缺点： 必须为每一个请求的对象建立和维护一个全新的连接。给Web服务器带来了严重的负担。 每一个对象经受两倍RTT的交付时延，即一个RTT用于创建TCP连接，另一个RTT用于请求和接收一个对象。 2.2采用持续连接的HTTPHTTP 1.1 持续连接：服务器在发送响应后保持该TCP连接打开。在相同的客户与服务器之间，后续的请求和响应报文能够通过相同的连接进行传送。 持续连接分为非流水线方式和流水线方式。 非流水线方式：客户在收到前一个响应后才能发出下一个请求。每访问一个对象就要消耗一个RTT，浪费服务器资源。 流水线方式：客户在收到HTTP的响应报文之前就能够接着发送新的请求报文。访问所有对象只花费一个RTT，使TCP连接中的空闲时间减少。（默认使用） 如果一条连接经过一定时间间隔（一个可配置的超时间隔）仍未被使用，HTTP服务器就关闭该连接。 2.3HTTP报文格式 (1)请求报文 上图为一个请求报文示例。 报文第一行叫做请求行，其后继的行叫做首部行，行后有回车和换行。 请求行：包括三个字段：方法字段、URL字段和HTTP版本字段。 方法字段：GET、POST、HEAD、PUT和DELETE。 HEAD方法不反回请求对象，常用来调试跟踪。 URL字段：强求对象的位置。 HTTP版本字段 首部行 Host：指明对象所在的主机，该首部行提供的信息是Web代理高速缓存所要求的。 Connection：告诉服务器发送玩请求对象后是否关闭连接。 User-agent：指明用户代理，即向服务器发送请求的浏览器类型。 Accept-language：用户所希望得到的对象的语言版本。 最后还有一个实体主体（请求报文中一般不用），在使用POST方法时才使用实体体。当用户提交表单时，常常使用POST方法，例如提供搜索关键词。如果方法字段的值为POST时，在实体体中包含的就是用户在表单字段中的输入值。 用表单生成的请求报文不是必须使用POST方法，HTML表单经常使用GET方法，并在（表单字段中）所请求的URL中包括输入的数据。例如，www.xxx.com/animalserch?monkeys&amp;bananas。 (2)响应报文 响应报文包括：状态行、首部行和实体体。 状态行：3个字段：协议版本字段、状态码和相应状态信息 常见状态码和相关短语如下： 首部行： Connection：告诉客户发送完报文后该TCP连接是否关闭。 Date：指示服务器产生并发送该响应报文的时间和日期。 Server：指示服务器类型。 Last-Modified：指示对象创建或者修改的日期和时间。 Content-Length：指示了被发送对象中的字节数。 Content-Type：指示了实体体中的对象类型。 2.4用户与服务器的交互：cookie为了识别用户，HTTP使用了cookie，它允许站点对用户进行跟踪。 如上图，cookie技术有4个组件： 1）在HTTP响应报文中的一个cookie首部行 2）在HTTP请求报文中的一个cookie首部行 3）在用户端系统中保留一个cookie文件，并由用户的浏览器管理 4）位于Web站点的一个后端数据库 如上图，服务器响应报文中有Set-cookie：1678首部行，浏览器收到响应后，在它管理的特定cookie文件中添加一行，该行包含服务器的主机名和Set-cookie：首部中的识别码。当再次访问amazon站点时，浏览器查询该cookie文件并抽取识别码，放到HTTP请求报文中。这样，服务器就识别了客户。 2.5Web缓存Web缓存器（Web cache）也叫代理服务器（proxy server），它是能够代表初始Web服务器来满足HTTP请求的网络实体。 Web缓存器有自己的磁盘存储空间，并在存储空间中保存最近请求过的对象的副本。 Web缓存器既是服务器又是客户。 在因特网上部署Web缓存器有两个原因： Web缓存器可以大大减少对客户请求的响应时间，特别是当客户与初始服务器之间的瓶颈带宽远低于客户与Web缓存器之间的瓶颈带宽时更是如此。 Web缓存器能够大大减少一个机构的接入链路到因特网的通信量。 2.6条件GET方法高速缓存也引入一个问题，存放在缓存器中的对象副本可能是陈旧的。 缓存器向Web服务器请求一个对象，Web服务器向缓存器发送的响应报文中包含Last-Modified：首部行，缓存器在存储该对象时也存储了最后修改日期。 一段时间后，当用户向缓存器请求该对象，缓存器向服务器发送一个条件GET执行最新检查。 该条件GET告诉服务器，仅当自指定日期之后该对象被修改过，才发送该对象。从上面两图中看，没有被修改过，所以Web服务器向缓存器发回如下响应： 响应中没有包含所请求的对象。 3、例子浏览器键入URL，按下回车后发生什么。 以下为在浏览器键入http://www.itcast.cn/course/c.shtml 后的抓包结果。 1、DNS解析域名www.itcast.cn所对应的IP地址。 DNS请求 DNS响应 2、解析处IP地址后，三次握手建立TCP连接 3、浏览器向服务器发送HTTP请求，服务器发送ACK确认收到 上图HTTP请求报文中包含cookie首部行，因为我之前打开过该网站。 4、服务器返回HTTP响应 5、释放TCP连接 6、浏览器显示网页","link":"/2019/06/25/计算机网络-http协议/"},{"title":"重载与重写","text":"重载与重写之前一直没考虑过重载与重写这两个概念，脑子了只有重载这个概念。 1、函数重载： （1）必须在同一类中进行； （2）子类无法继承父类的函数，父类同名函数将被名称覆盖； （3）重载是在编译期间根据参数类型和个数决定函数调用。 2、函数重写 （1）必须发生与父类与子类之间； （2）父类与子类中的函数必须有完全相同的原型； （3）使用virtual声明后能够产生多态（如果不使用virtual，那叫重定义），多态是在运行期间根据具体对象的类型决定函数的调用。 下面是示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566 #include &lt;iostream&gt;using namespace std;class Parent{public: //同一类中为函数重载，这几个函数为重载 virtual void fun(){ cout&lt;&lt;\"fun of Parent\"&lt;&lt;endl; } virtual void fun(int i){ cout&lt;&lt;\"fun2 \"&lt;&lt;i&lt;&lt;endl; } virtual void abc(){ cout&lt;&lt;\"abc\"&lt;&lt;endl; }protected:private:};class Child:public Parent{public: void abc(){ cout&lt;&lt;\"abc\"&lt;&lt;endl; } void abc(int i){ cout&lt;&lt;i&lt;&lt;endl;}//如果没有下面的代码，那么可以通过子类对象调用父类函数，c1.fun();//如果有下面代码，子类无法重载父类函数，父类同名函数将被覆盖，所以c1.fun()无法调用//如果和父类原型相同，那么可以是重写。 void fun(int i,int j){ cout&lt;&lt;\"fun1 of Child \"&lt;&lt;i&lt;&lt;\" \"&lt;&lt;j&lt;&lt;endl; } void fun(int i,int j,int n){ cout&lt;&lt;\"fun2 of Child\"&lt;&lt;endl; }};int main(){ Child c1; c1.Parent::fun(); /* 1、c++编译器 看到fun名字，在子类中已经存在了（名称覆盖），所以C++编译器不会去找父类的4个参 数的func函数 2、c++编译器只会在子类中查找fun函数，找到了两个fun，一个是2个参数的，一个是3个参数的 3、c++编译器开始报错。。。 4、若想调用，只能加作用域运算符 */ c1.fun(1);//报错，没有匹配的函数，无法调用父类的fun(int i); //c1.fun(); //fun函数的名字，在子类中发生了名称覆盖，子类的函数的名字，占用了父类的函数的名字的位置 //因为子类中已经有了fun函数重载形式。。。。 //编译器开始在子类中找fun函数。。。但是没有0个参数的fun函数 std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl; return 0;}","link":"/2019/03/17/重载与重写/"}],"tags":[{"name":"网络编程","slug":"网络编程","link":"/tags/网络编程/"},{"name":"操作系统","slug":"操作系统","link":"/tags/操作系统/"},{"name":"http","slug":"http","link":"/tags/http/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"C++心得","slug":"C-心得","link":"/categories/C-心得/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/计算机网络/"}]}